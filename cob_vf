#################################################
# CHAR ANALYSIS PARA EL MODELO DE COBRANZA BPE
#################################################


#actualizado a la fecha 05-02-2018



#Instalación de paquetes

rm(list = ls())

library(rJava)
library(foreign)
library(readxl)
library(ggplot2)
library(dplyr)
library(gdata)
library(xlsx)
library(lubridate)
library(caret)
library(data.table)
library(reshape2)
library(plyr)


ruta <- "D:/edwin/Modelo scoring cobranza/PERFORMANCE"

variables <- read_excel(paste0(ruta,"/COBRANZA_VARIABLES_201506_201709_F.xlsx"))

target <- read_excel(paste0(ruta,"/COBRANZA_Y_MORA_201509_201709_F.xlsx"))


#creare una variable que idenfique ambos modelos

variables <- mutate(variables, ID_uni = paste0(as.numeric(CIF_KEY),as.numeric(CodCredito),as.numeric(CodSolicitud)))
target <- mutate(target, ID_uni = paste0(codunico,codcredito,codsolicitud))


#Filtraré y solo me quedaré con las variables que me interesan

target$CODMES_2 <- as.numeric(target$CODMES) - 1 

target$CODMES_2 <- as.character(ifelse(target$CODMES_2 == 201600, 201512, 
                                       ifelse(target$CODMES_2 == 201700, 201612, target$CODMES_2)))


variables$CODMES_h <- variables$CODMES

variables <- variables[,-13]


#nos quedamos solo con algunas columnas

variables <- variables[,c(1,7,12,13,16:45,47:53)]
target <- target[,c(36,37,21,23:35)]

# Juntamos ambas bases

data <- merge(target, variables, by.x = c("ID_uni","CODMES"), by.y = c("ID_uni","CODMES_h"), all.x = T)

df <- group_by(data, as.factor(ID_uni), CODMES) 
df <- dplyr::summarise(df, n = n())


data <- data[order(data$CntE1MSF),]

data <- mutate(data, unico = paste0(ID_uni,CODMES))

data <- data[!duplicated(data$unico),]

resumen <- group_by(data, CODMES) 
resumen <- dplyr::summarise(resumen,n = n())



###########################################################################
#Solo tomamos los periodos 201509 - 201608 (12 ventanas seguidas)
###########################################################################


data_desarrollo <- filter(data, as.numeric(CODMES)<201609 & flag_pago != 2)

resumen <- group_by(data_desarrollo, CODMES)
resumen <- dplyr::summarise(resumen,n = n())

data_oot <- filter(data, as.numeric(CODMES)>=201609 & flag_pago != 2)

resumen_oot <- group_by(data_oot, CODMES)
resumen_oot <- dplyr::summarise(resumen_oot,n = n())


#Ordenamiento de las variables


data_desarrollo <- data_desarrollo[,c(13:20,2:8,10,9,21:55,12)]

data_desarrollo <- data_desarrollo[,c(53,1:17,18,20,22,19,21,23,24,26,28,30,32,34,36,38,40,42,44,46,25,27,29,31,33,35,37,39,41,43,45,47,48:52)]


data_desarrollo <- filter(data_desarrollo, is.na(CntE1MSF)==F)


NA_data <- as.data.frame(colSums(is.na(data_desarrollo)))
colnames(NA_data) <- "Cantidad_NA"


data_desarrollo$flag_pago <- factor(data_desarrollo$flag_pago, levels= c(0,1))



#Observaremos la cantidad de % que hay de pago o no pago
round(prop.table(table(data_desarrollo$flag_pago))*100,2)
#0-> 36.53% y  1 -> 63.47%

#########################################################
# 2. ANÁLISIS BIVARIADO
#########################################################

# A) CANTIDAD DE ENTIDADES REPORTADAS EN EL SF (ya no es promedio, solo será el último)


ggplot() + geom_bar(aes(y = ..count..,x =as.factor(CntE1MSF),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


ggplot() + geom_bar(aes(y = ..count..,x =as.factor(CntE2MSF),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(CntE3MSF),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

featurePlot(x=data_desarrollo[,19:21],y=data_desarrollo$flag_pago,plot="box",auto.key = list(columns = 3))



#Posibles valores outliers >= 8 

#Se obtienen los mismos gráficos, así que da lo mismo
#Me quedaré con el último mes como variable 


# B) Promedio % Normal en el SF - t-3 (ya no es promedio, solo será el último)


data_desarrollo %>%
  ggplot(aes(PNor1MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 10) +
  labs(x = "% Normal en el SF - t-1")




data_desarrollo %>%
  ggplot(aes(PNor2MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 10) +
  labs(x = "% Normal en el SF - t-2")

data_desarrollo %>%
  ggplot(aes(PNor3MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 10) +
  labs(x = "% Normal en el SF - t-3")


featurePlot(x=data_desarrollo[,22:24],y=data_desarrollo$flag_pago,plot="box",auto.key = list(columns = 3))



#data_desarrollo$PNor1MSF_mean= apply(data_desarrollo[,21:23],1,FUN=function(x) mean(x))

data_desarrollo$PNor1MSF_f <- data_desarrollo$PNor1MSF


# no se corre
data_desarrollo %>%
  ggplot(aes(PNor1MSF_mean, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 10) +
  labs(x = "Promedio % Normal en el SF - 3U")


data_desarrollo %>%
  ggplot(aes(PNor1MSF_f, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 10) +
  labs(x = "Promedio % Normal en el SF - 1")



#Primera observación: Solo podré tomar un solo mes, porque me sale igual al promedio y cada mes es lo mismo


# C) Días de atraso en el sistema financiero (he sacado un ponderado de los últimos 12 meses)


data_desarrollo %>%
  ggplot(aes(ddatr1MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-1")




data_desarrollo %>%
  ggplot(aes(ddatr2MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-2")


data_desarrollo %>%
  ggplot(aes(ddatr3MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-3")




data_desarrollo %>%
  ggplot(aes(ddatr4MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-4")

data_desarrollo %>%
  ggplot(aes(ddatr5MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-5")




data_desarrollo %>%
  ggplot(aes(ddatr6MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-6")


data_desarrollo %>%
  ggplot(aes(ddatr7MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-7")




data_desarrollo %>%
  ggplot(aes(ddatr8MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-8")


data_desarrollo %>%
  ggplot(aes(ddatr9MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-9")




data_desarrollo %>%
  ggplot(aes(ddatr10MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-10")


data_desarrollo %>%
  ggplot(aes(ddatr11MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-11")




data_desarrollo %>%
  ggplot(aes(ddatr12MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - t-12")



featurePlot(x=data_desarrollo[,25:36],y=data_desarrollo$flag_pago,plot="strip", jitter = TRUE,auto.key = list(columns = 3))



featurePlot(x=data_desarrollo[,25:36],y=data_desarrollo$flag_pago,plot="box",auto.key = list(columns = 3))


m <- group_by(data_desarrollo, ddatr1MSF) %>% dplyr::summarise(n =sum(as.numeric(flag_pago)))


m <- group_by(data_desarrollo, ddatr1MSF) %>% dplyr::summarise ( buenos = sum(as.numeric(as.character(flag_pago))),   n=n(), malos = n - buenos) %>% filter(n>50)

colSums(is.na(data_desarrollo))


library(corrplot)

corrplot(cor(data_desarrollo[,19:27],))





#PONDERADO

data_desarrollo <- mutate(data_desarrollo, NAs_ddatr_3 = rowSums(is.na(data_desarrollo[,25:27])==T))
data_desarrollo <- mutate(data_desarrollo, NAs_ddatr_2 = rowSums(is.na(data_desarrollo[,26:27])==T))
data_desarrollo <- mutate(data_desarrollo, NAs_ddatr_1 = ifelse(is.na(data_desarrollo$ddatr3MSF)==T,1,0))


data_desarrollo <- mutate(data_desarrollo, prod_ddatr_3M = ifelse(NAs_ddatr_3 == 0,ifelse(NAs_ddatr_3 == 0,ifelse(is.na(ddatr1MSF)==F,ddatr1MSF,0),0)*3, 
                                      ifelse(NAs_ddatr_3 == 1,ifelse(NAs_ddatr_3 == 1,ifelse(is.na(ddatr1MSF)==F,ddatr1MSF,0),0)*2,
                                             ifelse(NAs_ddatr_3 == 2,ifelse(NAs_ddatr_3 == 2,ifelse(is.na(ddatr1MSF)==F,ddatr1MSF,0),0)*1,0))),
              prod_ddatr_2M = ifelse(NAs_ddatr_2 == 0,ifelse(NAs_ddatr_2 == 0,ifelse(is.na(ddatr2MSF)==F,ddatr2MSF,0),0)*2, 
                                      ifelse(NAs_ddatr_2 == 1,ifelse(NAs_ddatr_2 == 1,ifelse(is.na(ddatr2MSF)==F,ddatr2MSF,0),0)*1,0)),
             prod_ddatr_1M = ifelse(NAs_ddatr_1 == 0,ifelse(NAs_ddatr_1 == 0,ifelse(is.na(ddatr3MSF)==F,ddatr3MSF,0),0)*1, 0),
            ddatr3MSF_ponderado = (prod_ddatr_3M+prod_ddatr_2M+prod_ddatr_1M)/ifelse(NAs_ddatr_3 == 0,6,ifelse(NAs_ddatr_3 == 1,3,1)))


                                
data_desarrollo <- data_desarrollo[,-c(55,56,57,58,59,60)]

#Max 3 MESES

my.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)

data_desarrollo$ddatr3MSF_Max= as.numeric(apply(data_desarrollo[,25:27],1,FUN=function(x) my.max(x) ))

#Max 6 MESES


data_desarrollo$ddatr6MSF_Max= as.numeric(apply(data_desarrollo[,25:30],1,FUN=function(x) my.max(x) ))

#Max 12 MESES


data_desarrollo$ddatr12MSF_Max= as.numeric(apply(data_desarrollo[,25:36],1,FUN=function(x) my.max(x) ))


# MESES CON DÍAS DE ATRAZOS MAYOR A 10 DÍAS

group_by(data_desarrollo, ddatr1MSF) %>% dplyr::summarise(prop = mean( as.numeric(flag_pago)-1)) %>% filter(ddatr1MSF <100 | is.na(ddatr1MSF) == T) %>%
  ggplot() + geom_col( aes(x = reorder(ddatr1MSF, prop), y = prop)) + coord_flip()


group_by(data_desarrollo, ddatr2MSF) %>% dplyr::summarise(prop = mean( as.numeric(flag_pago)-1)) %>% filter(ddatr2MSF <100 | is.na(ddatr2MSF) == T) %>%
  ggplot() + geom_col( aes(x = reorder(ddatr2MSF, prop), y = prop)) + coord_flip()

group_by(data_desarrollo, ddatr3MSF) %>% dplyr::summarise(prop = mean( as.numeric(flag_pago)-1)) %>% filter(ddatr3MSF <100 | is.na(ddatr3MSF) == T) %>%
  ggplot() + geom_col( aes(x = reorder(ddatr3MSF, prop), y = prop)) + coord_flip()


group_by(data_desarrollo, ddatr4MSF) %>% dplyr::summarise(prop = mean( as.numeric(flag_pago)-1)) %>% filter(ddatr4MSF <100 | is.na(ddatr4MSF) == T) %>%
  ggplot() + geom_col( aes(x = reorder(ddatr4MSF, prop), y = prop)) + coord_flip()



# MESES CON DÍAS DE ATRAZOS MAYOR A 10 DÍAS  - 3 meses
data_desarrollo$M_ddatr3MSF_10d= apply(data_desarrollo[,25:27],1,FUN=function(x) length(which(x>10)))

# MESES CON DÍAS DE ATRAZOS MAYOR A 10 DÍAS - 6 meses
data_desarrollo$M_ddatr6MSF_10d= apply(data_desarrollo[,25:30],1,FUN=function(x) length(which(x>10)))

# MESES CON DÍAS DE ATRAZOS MAYOR A 10 DÍAS - 12 meses
data_desarrollo$M_ddatr12MSF_10d= apply(data_desarrollo[,25:36],1,FUN=function(x) length(which(x>10)))



data_desarrollo %>%
  ggplot(aes(ddatr3MSF_ponderado, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - promedio ponderado 3 meses t-3")


data_desarrollo %>%
  ggplot(aes(M_ddatr12MSF_10d, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Cantidad de Meses con Días de atraso mayor a 10 en el sistema financiero t-12")


sum(is.na(data_desarrollo$ddatr3MSF_ponderado)==T)


# D) Deuda directa vencida en el SF (ultimos 3 meses) - ponderada

data_desarrollo %>%
  ggplot(aes(ddvnr1MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Deuda directa vencida en el SF - t-1")


data_desarrollo %>%
  ggplot(aes(ddvnr2MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Deuda directa vencida en el SF - t-2")

data_desarrollo %>%
  ggplot(aes(ddvnr3MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Deuda directa vencida en el SF - t-3")


data_desarrollo %>%
  ggplot(aes(ddvnr4MSF, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Deuda directa vencida en el SF - t-4")



featurePlot(x=data_desarrollo[,37:48],y=data_desarrollo$flag_pago,plot="box",auto.key = list(columns = 3))

#PONDERADO

data_desarrollo <- mutate(data_desarrollo, NAs_ddvnr_3 = rowSums(is.na(data_desarrollo[,37:39])==T))
data_desarrollo <- mutate(data_desarrollo, NAs_ddvnr_2 = rowSums(is.na(data_desarrollo[,38:39])==T))
data_desarrollo <- mutate(data_desarrollo, NAs_ddvnr_1 = ifelse(is.na(data_desarrollo$ddvnr3MSF)==T,1,0))


data_desarrollo <- mutate(data_desarrollo, prod_ddvnr_3M = ifelse(NAs_ddvnr_3 == 0,ifelse(NAs_ddvnr_3 == 0,ifelse(is.na(ddvnr1MSF)==F,ddvnr1MSF,0),0)*3, 
                                                                  ifelse(NAs_ddvnr_3 == 1,ifelse(NAs_ddvnr_3 == 1,ifelse(is.na(ddvnr1MSF)==F,ddvnr1MSF,0),0)*2,
                                                                         ifelse(NAs_ddvnr_3 == 2,ifelse(NAs_ddvnr_3 == 2,ifelse(is.na(ddvnr1MSF)==F,ddvnr1MSF,0),0)*1,0))),
                          prod_ddvnr_2M = ifelse(NAs_ddvnr_2 == 0,ifelse(NAs_ddvnr_2 == 0,ifelse(is.na(ddvnr2MSF)==F,ddvnr2MSF,0),0)*2, 
                                                 ifelse(NAs_ddvnr_2 == 1,ifelse(NAs_ddvnr_2 == 1,ifelse(is.na(ddvnr2MSF)==F,ddvnr2MSF,0),0)*1,0)),
                          prod_ddvnr_1M = ifelse(NAs_ddvnr_1 == 0,ifelse(NAs_ddvnr_1 == 0,ifelse(is.na(ddvnr3MSF)==F,ddvnr3MSF,0),0)*1, 0),
                          ddvnr3MSF_ponderado = (prod_ddvnr_3M+prod_ddvnr_2M+prod_ddvnr_1M)/ifelse(NAs_ddvnr_3 == 0,6,ifelse(NAs_ddvnr_3 == 1,3,1)))



data_desarrollo <- data_desarrollo[,-c(62,63,64,65,66,67)]

#Max 3 MESES

my.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)

data_desarrollo$ddvnr3MSF_Max= as.numeric(apply(data_desarrollo[,37:39],1,FUN=function(x) my.max(x) ))

#Max 6 MESES


data_desarrollo$ddvnr6MSF_Max= as.numeric(apply(data_desarrollo[,37:42],1,FUN=function(x) my.max(x) ))

#Max 12 MESES


data_desarrollo$ddvnr12MSF_Max= as.numeric(apply(data_desarrollo[,37:48],1,FUN=function(x) my.max(x) ))




data_desarrollo %>%
  ggplot(aes(ddvnr3MSF_ponderado, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Deuda directa vencida en el SF (ultimos 3 meses) - ponderada")



sum(is.na(data_desarrollo$ddvnr3MSF_ponderado)==T)

# E) Posee Garantía

#data_desarrollo$Gtia_Soles_cat <- ifelse(data_desarrollo$Gtia_Soles > 0, "Si", "No")



data_desarrollo %>%
  ggplot(aes(Gtia_Soles, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Posee Garantía")

hist(data_desarrollo$Gtia_Soles)

#ha sido agregado para la prueba
data_desarrollo$Gtia_Soles_cat <- ifelse(data_desarrollo$Gtia_Soles > 100000, 1, 0)

data_desarrollo$Gtia_Soles_cat <- ifelse(data_desarrollo$Gtia_Soles > 0, 1, 0)


summary(data_desarrollo$Gtia_Soles)

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(Gtia_Soles_cat),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


# F) table(data_desarrollo$MaxDmB) - Máximo día de mora mayor a 15 días o no

hist(data_desarrollo$MaxDmB)


ggplot() + geom_bar(aes(y = ..count..,x =as.factor(MaxDmB),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


#data_desarrollo$MaxDmB_15 <- ifelse(data_desarrollo$MaxDmB >= 15, "Si", "No")

data_desarrollo$MaxDmB_15 <- ifelse(data_desarrollo$MaxDmB >= 15, 1, 0)

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(MaxDmB_15),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


# G) Ubicación del Local - por ahora no lo toco

data_desarrollo$DesUbicacionLocal <- as.factor(data_desarrollo$DesUbicacionLocal) 

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(DesUbicacionLocal),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))



# H) Sector Económico - tampoco lo toco

table(data_desarrollo$Sect_Apli)

data_desarrollo$Sect_Apli <- ifelse(data_desarrollo$Sect_Apli == "COMERCIO", "Comercio", 
                                    ifelse(data_desarrollo$Sect_Apli == "PRODUCCION", "Produccion", 
                                           ifelse(data_desarrollo$Sect_Apli == "SERVICIO","Servicio", data_desarrollo$Sect_Apli)))


ggplot() + geom_bar(aes(y = ..count..,x =as.factor(Sect_Apli),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))



list(colSums(is.na(data_desarrollo)))
list(colSums(is.na(data_oot)))


#) I)  Patrimonio - tampoco lo toco 




########################################################
# antes de categorizar quiero analizar sus correlaciones
##########################################################

corrplot(cor(data_desarrollo[,c(19,54:65,49,50,53)]))


correlaciones <- as.data.frame(cor(data_desarrollo[,c(19,54:65,49,50,53)]))

#para variables diferences, no hay correlaciones fuertes

names(data_desarrollo[,c(19,54:65,49,50,53)])






#####################################################################
# 3. IMPORTANCIA DE VARIABLES
#####################################################################



df_variables<- data.frame(ID_var=1:18,
                          variable= c("CntE1MSF", "PNor1MSF_f", "ddatr3MSF_ponderado", "ddatr3MSF_Max", "ddatr6MSF_Max",
                                      "ddatr12MSF_Max", "M_ddatr3MSF_10d", "M_ddatr6MSF_10d", "M_ddatr12MSF_10d", "ddvnr3MSF_ponderado",
                                      "ddvnr3MSF_Max", "ddvnr6MSF_Max", "ddvnr12MSF_Max", "Gtia_Soles", "Gtia_Soles_cat", "MaxDmB",
                                      "MaxDmB_15","Patrimonio"), stringsAsFactors = F)



data_prueba <- data.table(data_desarrollo)

n <- nrow(data_desarrollo)
n_var <- nrow(df_variables)
#i=18
for (i in 1:n_var) {
  
  dt_temp2 <- data_prueba[,list(codunico,x=get(df_variables$variable[ID_var=i]), flag_pago)]
  dt_temp2 <- dt_temp2[!is.na(x),]
  
  v1<- quantile(dt_temp2$x, probs = seq(0,1,.10),na.rm = T, names = F)[2:11]
  dt_temp2$cat<-NA
  for (j in 1:length(v1)) {
    dt_temp2 <- within(dt_temp2,{
      cat[is.na(cat) & x<=v1[j]]=j})
  }
  
  df_table0 <- dcast(dt_temp2, cat~flag_pago, fun.aggregate = function(x) sum(!is.na(x)), value.var = "codunico")
  df_table0 <- cbind(df_table0, Recov_rel=round(100*cumsum(prop.table(df_table0$'0')),1),
                     NoRecov_rel=round(100*cumsum(prop.table(df_table0$'1')),1)  ,
                     Diff=abs(round(100*(cumsum(prop.table(df_table0$'0'))-cumsum(prop.table(df_table0$'1'))),1)))
  df_table1 <- data.frame(codunico=i,variable=df_variables$variable[i])
  df_table1$KS <- max(df_table0$Diff)
  
  if (i==1) {df_varfinal <- df_table1}
  else {df_varfinal <- rbind(df_varfinal,df_table1)}
  
}


#EL CAMPO "variable" a formato char
df_varfinal$variable <- as.character(df_varfinal$variable)
#str(df_varfinal)
#ORDENANDO A LAs VARIABLE POR KS
df_varfinal<-df_varfinal[order(-df_varfinal$KS),]

var_numerica <- df_varfinal

#######
# PARA VARIABLES CATEGORICAS 


df_variables<- data.frame(ID_var=1:2,
                          variable= c("Sect_Apli","DesUbicacionLocal"), stringsAsFactors = F)



data_prueba <- data.table(data_desarrollo)

n <- nrow(data_desarrollo)
n_var <- nrow(df_variables)
#i=1
for (i in 1:n_var) {
  
  dt_temp2 <- data_prueba[,list(codunico,x=get(df_variables$variable[ID_var=i]), flag_pago)]
  dt_temp2 <- dt_temp2[!is.na(x),]
  
  v1 <- unique(dt_temp2$x)
  
  dt_temp2$cat<-NA
  for (j in 1:length(v1)) {
    dt_temp2 <- within(dt_temp2,{
      cat[is.na(cat) & x==v1[j]]=j})
  }
  
  df_table0 <- dcast(dt_temp2, cat~flag_pago, fun.aggregate = function(x) sum(!is.na(x)), value.var = "codunico")
  df_table0 <- cbind(df_table0, Recov_rel=round(100*cumsum(prop.table(df_table0$'0')),1),
                     NoRecov_rel=round(100*cumsum(prop.table(df_table0$'1')),1)  ,
                     Diff=abs(round(100*(cumsum(prop.table(df_table0$'0'))-cumsum(prop.table(df_table0$'1'))),1)))
  df_table1 <- data.frame(codunico=i,variable=df_variables$variable[i])
  df_table1$KS <- max(df_table0$Diff)
  
  if (i==1) {df_varfinal <- df_table1}
  else {df_varfinal <- rbind(df_varfinal,df_table1)}
  
}

#EL CAMPO "variable" a formato char
df_varfinal$variable <- as.character(df_varfinal$variable)

df_varfinal<-df_varfinal[order(-df_varfinal$KS),]

var_categorica <- df_varfinal

#I.6 TABLA DE VARIABLES CON BUEN PERFORMANCE INDIVIDUAL

df_varfinal <- rbind(var_numerica,var_categorica)

df_varfinal<-df_varfinal[order(-df_varfinal$KS),]

df_temp1<-data.frame(data_prueba[, 
                                 mget(df_varfinal$variable[df_varfinal$KS>=0])])

t<-length(colnames(df_temp1))

df_varperf<- data.frame(id_var=1:t,var=colnames(df_temp1))



#-------------------------------------------------------------------------
# me quedé acá


#No hay correlación fuerte

#TABLA CON VARIABLES FINALES:  

df_model <- data.frame(data_prueba[,list(codunico,flag_pago,MaxDmB,PNor1MSF_f, ddatr3MSF_ponderado,
                                         M_ddatr3MSF_10d,M_ddatr6MSF_10d,ddatr12MSF_Max,
                                         ddvnr6MSF_Max,ddvnr3MSF_ponderado,
                                         Patrimonio, Gtia_Soles,Gtia_Soles_cat, Sect_Apli)])


df_model$Sect_Apli <- as.factor(df_model$Sect_Apli)

corrplot(cor(df_model[,3:12]))

#cor(data_prueba[complete.cases(data_prueba[,52:55]),][,52:55])


#nos quedamos solo con los numericos y categoricos seleccionados

df_varperf_num <- df_varperf[c(1,3,4,6,8,9,11,13,15,16,17),]


df_varperf_cat <- df_varperf[20,]



##I.9 CATEGORIZACION OPTIMA DE LAS VARIABLES:

library(partykit)

df_varperf_<-df_varperf_num

df_varperf_$var <- as.character(df_varperf_$var)


dt_model<-data.table(df_model)
n <- nrow(dt_model)
n_var <- nrow(df_varperf_)

#i=10

for (i in 1:n_var) {
  
  dt_temp2 <- dt_model[,list(codunico,x=get(df_varperf_$var[id_var=i]), flag_pago)]
  dt_temp2 <- dt_temp2[!is.na(x),]
  
  
  tree1 <- partykit::ctree(flag_pago ~ x, dt_temp2,
                           control=ctree_control(mincriterion=.90, minsplit=.05*n,
                                                 minbucket=.1*n,maxdepth=2)
  )
  dt_temp2$xcat <-predict(tree1,dt_temp2,type="node")
  
  t<-dt_temp2[,.(Minimo=round(min(x),2), Maximo=round(max(x),2)), keyby="xcat"]
  
  t<-cbind(data.table(Variable=rep(df_varperf_$var[id=i],nrow(t))),
           t)
  
  if (i==1) {dt_varscat <- t} else {dt_varscat<- rbind(t,dt_varscat)}
  
}


cat_var_num <- dt_varscat


df_varperf_<-df_varperf_cat

df_varperf_$var <- as.character(df_varperf_$var)


dt_model<-data.table(df_model)
n <- nrow(dt_model)
n_var <- nrow(df_varperf_)
#i=1
for (i in 1:n_var) {
  
  dt_temp2 <- dt_model[,list(codunico,x=get(df_varperf_$var[id_var=i]), flag_pago)]
  dt_temp2 <- dt_temp2[!is.na(x),]
  
  
  tree1 <- partykit::ctree(flag_pago ~ x, dt_temp2,
                           control=ctree_control(mincriterion=.90, minsplit=0.05*n,
                                                 minbucket=.1*n,maxdepth=2)
  )
  dt_temp2$xcat <-predict(tree1,dt_temp2,type="node")
  
  t<-dt_temp2[,.(Minimo=unique(x), Maximo=unique(x)), keyby="xcat"]
  
  t<-cbind(data.table(Variable=rep(df_varperf_$var[id=i],nrow(t))),
           t)
  
  if (i==1) {dt_varscat <- t} else {dt_varscat<- rbind(t,dt_varscat)}
  
}

cat_var_cat <- dt_varscat
cat_var_num


df_varperf_f <- rbind(df_varperf_num,df_varperf_cat,c("21","Gtia_Soles"))

df_varperf_f$var <- as.character(df_varperf_f$var)



#df_varperf$var <- as.character(df_varperf$var)



df_train <- data_desarrollo[,names(data_desarrollo) %in% list(c("CIF_KEY","CODMES","flag_pago",as.vector(t(list(df_varperf_f$var)[[1]])[1,]) ))[[1]]]


summary(df_train)

hist(df_train$Patrimonio)
boxplot(df_train$Patrimonio)

hist(log(df_train$Patrimonio+1))

boxplot(log(df_train$Patrimonio+1))

norm <- function(x){
  v <- (x - min(x))/(max(x) - min(x))
  return(v)
}

hist(norm(df_train$Patrimonio))

summary(norm(df_train$Patrimonio))


boxplot(norm(df_train$Patrimonio))

hist(norm(df_train$Gtia_Soles))

summary(norm(df_train$Gtia_Soles))


hist(norm(df_train$ddatr3MSF_ponderado))

hist(log(df_train$ddatr3MSF_ponderado+1))


summary(log(df_train$ddatr3MSF_ponderado+1))



#IDEA: DEBIDO A QUE HAY OUTLIER, PUEDO INTENTAR LIMPIANDO LA DATA Y ME QUEDO SOLO CON ALGUNOS


#############################################################################
#--------------- 2DO INTENTO FEBRERO 2018 --------------------------------#
#############################################################################

##############################################
### CATEGORIZACION
##############################################

#1) var numerica

cat_var_num

colSums(is.na(df_train)==T)


df_train$Gtia_Soles_cat_f <- factor(ifelse(df_train$Gtia_Soles_cat == 0,1,2),levels = c(1,2),labels =  c("No","Si"))



#recaregorizado
df_train$Gtia_Soles_cat_f <- factor(ifelse(df_train$Gtia_Soles <= 654459,1,2),levels = c(1,2),labels =  c("<=644459",">264459"))





df_train$Patrimonio_cat <- factor(ifelse(df_train$Patrimonio < 410419,1,2),levels = c(1,2),labels =  c("<417419.00",">= 417639.00"))


df_train$ddvnr3MSF_ponderado_cat <- factor(ifelse(df_train$ddvnr3MSF_ponderado <= 172.4,1, 2),levels = c(1,2),labels =  c("<172.4",">=172.4"))


df_train$dddvnr6MSF_Max_cat <- factor(ifelse(df_train$ddvnr6MSF_Max < 337.5,1,
                                                 ifelse(df_train$ddvnr6MSF_Max < 7788.2,2, 3)),levels = c(1,2,3),labels =  c("<337.5" ,"[337.5 - 7788.2>",">=7788.2"))


#recategorizacion
df_train$dddvnr6MSF_Max_cat <- factor(ifelse(df_train$ddvnr6MSF_Max < 337.5,1,2),levels = c(1,2),labels =  c("<337.5" ,">=337.5"))



df_train$ddatr12MSF_Max_cat <- factor(ifelse(df_train$ddatr12MSF_Max < 8,1,
                                             ifelse(df_train$ddatr12MSF_Max < 12,2, 3)),levels = c(1,2,3),labels =  c("<8" ,"[8 - 12>",">=12"))
                                                                            
#recategorizado
df_train$ddatr12MSF_Max_cat <- factor(ifelse(df_train$ddatr12MSF_Max < 8,1,2),levels = c(1,2),labels =  c("<8" ,">=8"))



df_train$M_ddatr6MSF_10d_cat <- factor(ifelse(df_train$M_ddatr6MSF_10d <= 0,1,
                                          ifelse(df_train$M_ddatr6MSF_10d < 4,2, 3)),levels = c(1,2,3),labels =  c("0Meses" ,"1a4Meses","Masde4Meses"))
                                                                            
                                                                            
df_train$M_ddatr3MSF_10d_cat <- factor(ifelse(df_train$M_ddatr3MSF_10d <= 0,1,
                                         ifelse(df_train$M_ddatr3MSF_10d <= 1,2, 3)),levels = c(1,2,3),labels =  c("0Meses" ,"1Mes","Masde1Mes"))
                                                                              
                                                                              
df_train$ddatr3MSF_ponderadocat <- factor(ifelse(df_train$ddatr3MSF_ponderado < 6,1,
                                             ifelse(df_train$ddatr3MSF_ponderado < 8,2, 3)),levels = c(1,2,3),labels =  c("<6días" ,"<8días",">=8días"))
                                                                              
                                       
                                       

df_train$PNor1MSF_f_cat <- factor(ifelse(df_train$PNor1MSF_f <= 78,1,
                                            ifelse(df_train$PNor1MSF_f <= 97,2,3)),levels = c(1,2,3),labels =  c("[0-78]","<78-97]",">97"))

#recategorizado
df_train$PNor1MSF_f_cat <- factor(ifelse(df_train$PNor1MSF_f <= 78,1,2),levels = c(1,2),labels =  c("[0-78]",">78"))


#recategorizado
df_train$PNor1MSF_f_cat <- factor(ifelse(df_train$PNor1MSF_f <= 97,1,2),levels = c(1,2),labels =  c("[0-97]",">97"))



df_train$MaxDmB_cat <- factor(ifelse(df_train$MaxDmB <= 6,1,
                                                    ifelse(df_train$MaxDmB <= 9,2,
                                                           ifelse(df_train$MaxDmB <= 18,3,4))),levels = c(1,2,3,4),labels =  c("<=6dias" ,"<=9dias","<=18dias",">=19días"))
                                       

#Recategorizado
df_train$MaxDmB_cat <- factor(ifelse(df_train$MaxDmB <= 6,1,
                                     ifelse(df_train$MaxDmB <= 9,2,
                                            ifelse(df_train$MaxDmB <= 14,3,4))),levels = c(1,2,3,4),labels =  c("<=6dias" ,"<=9dias","<=14dias",">=15días"))


#Recategorizado
df_train$MaxDmB_cat <- factor(ifelse(df_train$MaxDmB <= 6,1,
                                     ifelse(df_train$MaxDmB <= 9,2,
                                            ifelse(df_train$MaxDmB <= 14,3,4))),levels = c(1,2,3,4),labels =  c("<=6dias" ,"<=9dias","<=14dias",">=15días"))

#Recategorizado
df_train$MaxDmB_cat <- factor(ifelse(df_train$MaxDmB <= 0,1,
                                     ifelse(df_train$MaxDmB <= 4,2,3)),levels = c(1,2,3),labels =  c("<=1dias" ,"<=4dias",">=6días"))

#2) categoricas



#RANG_INGRESO 
#9417 
#FLAG_LIMA_PROVINCIA 
#3386 
#EDAD 
#5326 
#ANTIGUEDAD 
#1762 



#quitamos ubicaion del local
df_train <- df_train[,c(1:3,16:25,6)]

table(df_train$Sect_Apli)

df_train$Sect_Apli <- factor(ifelse(df_train$Sect_Apli == "Comercio",1,
                                     ifelse(df_train$Sect_Apli == "Servicio",2,3)),levels = c(1,2,3),labels =  c("Comercio" ,"Servicio","Produccion"))


#recategorizado
df_train$Sect_Apli <- factor(ifelse(df_train$Sect_Apli == "Comercio",1,2),levels = c(1,2),labels =  c("Comercio" ,"Servicio/Produccion"))



###################################################
#  DATA OOT
###################################################



data_oot <- data_oot[,c(13:20,2:8,10,9,21:55,12)]

data_oot <- data_oot[,c(53,1:17,18,20,22,19,21,23,24,26,28,30,32,34,36,38,40,42,44,46,25,27,29,31,33,35,37,39,41,43,45,47,48:52)]


data_oot <- filter(data_oot, is.na(CntE1MSF)==F)


NA_data <- as.data.frame(colSums(is.na(data_oot)))
colnames(NA_data) <- "Cantidad_NA"


data_oot$flag_pago <- factor(data_oot$flag_pago, levels= c(0,1))



#Observaremos la cantidad de % que hay de pago o no pago
round(prop.table(table(data_oot$flag_pago))*100,2)
#0-> 41.23% y  1 -> 58.77%

#########################################################
# 2. ANÁLISIS BIVARIADO
#########################################################

# A) CANTIDAD DE ENTIDADES REPORTADAS EN EL SF


ggplot() + geom_bar(aes(y = ..count..,x =as.factor(CntE1MSF),fill = flag_pago),data=data_oot, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


ggplot() + geom_bar(aes(y = ..count..,x =as.factor(CntE2MSF),fill = flag_pago),data=data_oot, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(CntE3MSF),fill = flag_pago),data=data_oot, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

featurePlot(x=data_oot[,19:21],y=data_oot$flag_pago,plot="box",auto.key = list(columns = 3))

#Posibles valores outliers >= 8 

#Se obtienen los mismos gráficos, así que da lo mismo
#Me quedaré con el último mes como variable 


# B) % Normal en el SF - t-1


data_oot$PNor1MSF_f <- data_oot$PNor1MSF

data_oot %>%
  ggplot(aes(PNor1MSF_f, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 10) +
  labs(x = "% Normal en el SF - 1")


# C) Días de atraso en el sistema financiero (he sacado un ponderado de los últimos 12 meses)

m <- group_by(data_oot, ddatr1MSF) %>% dplyr::summarise(n =sum(as.numeric(flag_pago)))


m <- group_by(data_oot, ddatr1MSF) %>% dplyr::summarise ( buenos = sum(as.numeric(as.character(flag_pago))),   n=n(), malos = n - buenos) %>% filter(n>50)

colSums(is.na(data_oot))


library(corrplot)

corrplot(cor(data_oot[,19:27],))



#PONDERADO

data_oot <- mutate(data_oot, NAs_ddatr_3 = rowSums(is.na(data_oot[,25:27])==T))
data_oot <- mutate(data_oot, NAs_ddatr_2 = rowSums(is.na(data_oot[,26:27])==T))
data_oot <- mutate(data_oot, NAs_ddatr_1 = ifelse(is.na(data_oot$ddatr3MSF)==T,1,0))


data_oot <- mutate(data_oot, prod_ddatr_3M = ifelse(NAs_ddatr_3 == 0,ifelse(NAs_ddatr_3 == 0,ifelse(is.na(ddatr1MSF)==F,ddatr1MSF,0),0)*3, 
                                                                  ifelse(NAs_ddatr_3 == 1,ifelse(NAs_ddatr_3 == 1,ifelse(is.na(ddatr1MSF)==F,ddatr1MSF,0),0)*2,
                                                                         ifelse(NAs_ddatr_3 == 2,ifelse(NAs_ddatr_3 == 2,ifelse(is.na(ddatr1MSF)==F,ddatr1MSF,0),0)*1,0))),
                          prod_ddatr_2M = ifelse(NAs_ddatr_2 == 0,ifelse(NAs_ddatr_2 == 0,ifelse(is.na(ddatr2MSF)==F,ddatr2MSF,0),0)*2, 
                                                 ifelse(NAs_ddatr_2 == 1,ifelse(NAs_ddatr_2 == 1,ifelse(is.na(ddatr2MSF)==F,ddatr2MSF,0),0)*1,0)),
                          prod_ddatr_1M = ifelse(NAs_ddatr_1 == 0,ifelse(NAs_ddatr_1 == 0,ifelse(is.na(ddatr3MSF)==F,ddatr3MSF,0),0)*1, 0),
                          ddatr3MSF_ponderado = (prod_ddatr_3M+prod_ddatr_2M+prod_ddatr_1M)/ifelse(NAs_ddatr_3 == 0,6,ifelse(NAs_ddatr_3 == 1,3,1)))



data_oot <- data_oot[,-c(55,56,57,58,59,60)]

#Max 3 MESES

my.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)

data_oot$ddatr3MSF_Max= as.numeric(apply(data_oot[,25:27],1,FUN=function(x) my.max(x) ))

#Max 6 MESES


data_oot$ddatr6MSF_Max= as.numeric(apply(data_oot[,25:30],1,FUN=function(x) my.max(x) ))

#Max 12 MESES


data_oot$ddatr12MSF_Max= as.numeric(apply(data_oot[,25:36],1,FUN=function(x) my.max(x) ))


# MESES CON DÍAS DE ATRAZOS MAYOR A 10 DÍAS

group_by(data_oot, ddatr1MSF) %>% dplyr::summarise(prop = mean( as.numeric(flag_pago)-1)) %>% filter(ddatr1MSF <100 | is.na(ddatr1MSF) == T) %>%
  ggplot() + geom_col( aes(x = reorder(ddatr1MSF, prop), y = prop)) + coord_flip()


group_by(data_oot, ddatr2MSF) %>% dplyr::summarise(prop = mean( as.numeric(flag_pago)-1)) %>% filter(ddatr2MSF <100 | is.na(ddatr2MSF) == T) %>%
  ggplot() + geom_col( aes(x = reorder(ddatr2MSF, prop), y = prop)) + coord_flip()

group_by(data_oot, ddatr3MSF) %>% dplyr::summarise(prop = mean( as.numeric(flag_pago)-1)) %>% filter(ddatr3MSF <100 | is.na(ddatr3MSF) == T) %>%
  ggplot() + geom_col( aes(x = reorder(ddatr3MSF, prop), y = prop)) + coord_flip()


group_by(data_oot, ddatr4MSF) %>% dplyr::summarise(prop = mean( as.numeric(flag_pago)-1)) %>% filter(ddatr4MSF <100 | is.na(ddatr4MSF) == T) %>%
  ggplot() + geom_col( aes(x = reorder(ddatr4MSF, prop), y = prop)) + coord_flip()



# MESES CON DÍAS DE ATRAZOS MAYOR A 10 DÍAS  - 3 meses
data_oot$M_ddatr3MSF_10d= apply(data_oot[,25:27],1,FUN=function(x) length(which(x>10)))

# MESES CON DÍAS DE ATRAZOS MAYOR A 10 DÍAS - 6 meses
data_oot$M_ddatr6MSF_10d= apply(data_oot[,25:30],1,FUN=function(x) length(which(x>10)))

# MESES CON DÍAS DE ATRAZOS MAYOR A 10 DÍAS - 12 meses
data_oot$M_ddatr12MSF_10d= apply(data_oot[,25:36],1,FUN=function(x) length(which(x>10)))



data_oot %>%
  ggplot(aes(ddatr3MSF_ponderado, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Días de atraso en el sistema financiero - promedio ponderado 3 meses t-3")


data_oot %>%
  ggplot(aes(M_ddatr12MSF_10d, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Cantidad de Meses con Días de atraso mayor a 10 en el sistema financiero t-12")


sum(is.na(data_oot$ddatr3MSF_ponderado)==T)




# D) Deuda directa vencida en el SF (ultimos 3 meses) - ponderada


#PONDERADO

data_oot <- mutate(data_oot, NAs_ddvnr_3 = rowSums(is.na(data_oot[,37:39])==T))
data_oot <- mutate(data_oot, NAs_ddvnr_2 = rowSums(is.na(data_oot[,38:39])==T))
data_oot <- mutate(data_oot, NAs_ddvnr_1 = ifelse(is.na(data_oot$ddvnr3MSF)==T,1,0))


data_oot <- mutate(data_oot, prod_ddvnr_3M = ifelse(NAs_ddvnr_3 == 0,ifelse(NAs_ddvnr_3 == 0,ifelse(is.na(ddvnr1MSF)==F,ddvnr1MSF,0),0)*3, 
                                                                  ifelse(NAs_ddvnr_3 == 1,ifelse(NAs_ddvnr_3 == 1,ifelse(is.na(ddvnr1MSF)==F,ddvnr1MSF,0),0)*2,
                                                                         ifelse(NAs_ddvnr_3 == 2,ifelse(NAs_ddvnr_3 == 2,ifelse(is.na(ddvnr1MSF)==F,ddvnr1MSF,0),0)*1,0))),
                          prod_ddvnr_2M = ifelse(NAs_ddvnr_2 == 0,ifelse(NAs_ddvnr_2 == 0,ifelse(is.na(ddvnr2MSF)==F,ddvnr2MSF,0),0)*2, 
                                                 ifelse(NAs_ddvnr_2 == 1,ifelse(NAs_ddvnr_2 == 1,ifelse(is.na(ddvnr2MSF)==F,ddvnr2MSF,0),0)*1,0)),
                          prod_ddvnr_1M = ifelse(NAs_ddvnr_1 == 0,ifelse(NAs_ddvnr_1 == 0,ifelse(is.na(ddvnr3MSF)==F,ddvnr3MSF,0),0)*1, 0),
                          ddvnr3MSF_ponderado = (prod_ddvnr_3M+prod_ddvnr_2M+prod_ddvnr_1M)/ifelse(NAs_ddvnr_3 == 0,6,ifelse(NAs_ddvnr_3 == 1,3,1)))



data_oot <- data_oot[,-c(62,63,64,65,66,67)]

#Max 3 MESES

my.max <- function(x) ifelse( !all(is.na(x)), max(x, na.rm=T), NA)

data_oot$ddvnr3MSF_Max= as.numeric(apply(data_oot[,37:39],1,FUN=function(x) my.max(x) ))

#Max 6 MESES


data_oot$ddvnr6MSF_Max= as.numeric(apply(data_oot[,37:42],1,FUN=function(x) my.max(x) ))

#Max 12 MESES


data_oot$ddvnr12MSF_Max= as.numeric(apply(data_oot[,37:48],1,FUN=function(x) my.max(x) ))




data_oot %>%
  ggplot(aes(ddvnr3MSF_ponderado, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Deuda directa vencida en el SF (ultimos 3 meses) - ponderada")



sum(is.na(data_oot$ddvnr3MSF_ponderado)==T)




# E) Posee Garantía

#data_desarrollo$Gtia_Soles_cat <- ifelse(data_desarrollo$Gtia_Soles > 0, "Si", "No")

data_oot %>%
  ggplot(aes(Gtia_Soles, fill = flag_pago)) +
  #  geom_density(alpha = 0.5, bw = 5e3) +
  geom_histogram(bins = 50) +
  labs(x = "Posee Garantía")

hist(data_oot$Gtia_Soles)

#ha sido agregado para la prueba
data_oot$Gtia_Soles_cat <- ifelse(data_oot$Gtia_Soles > 100000, 1, 0)

data_oot$Gtia_Soles_cat <- ifelse(data_oot$Gtia_Soles > 0, 1, 0)


summary(data_oot$Gtia_Soles)

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(Gtia_Soles_cat),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))




# F) table(data_desarrollo$MaxDmB) - Máximo día de mora mayor a 15 días o no

hist(data_oot$MaxDmB)

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(MaxDmB),fill = flag_pago),data=data_oot, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


data_oot$MaxDmB_15 <- ifelse(data_oot$MaxDmB >= 15, 1, 0)

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(MaxDmB_15),fill = flag_pago),data=data_desarrollo, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))



# G) Ubicación del Local - por ahora no lo toco

data_oot$DesUbicacionLocal <- as.factor(data_oot$DesUbicacionLocal) 

# H) Sector Económico - tampoco lo toco

table(data_oot$Sect_Apli)

data_oot$Sect_Apli <- ifelse(data_oot$Sect_Apli == "COMERCIO", "Comercio", 
                             ifelse(data_oot$Sect_Apli == "PRODUCCION", "Produccion", 
                                    ifelse(data_oot$Sect_Apli == "SERVICIO","Servicio", data_oot$Sect_Apli)))





df_oot_f<- data_oot[,names(data_oot) %in% list(c("CIF_KEY","CODMES","flag_pago",as.vector(t(list(df_varperf_f$var)[[1]])[1,]) ))[[1]]]

list(colSums(is.na(df_oot_f)))

##############################################
### CATEGORIZACION
##############################################

##############################################
### CATEGORIZACION
##############################################

#1) var numerica

cat_var_num

colSums(is.na(df_oot_f)==T)


df_oot_f$Gtia_Soles_cat_f <- factor(ifelse(df_oot_f$Gtia_Soles_cat == 0,1,2),levels = c(1,2),labels =  c("No","Si"))



df_oot_f$Patrimonio_cat <- factor(ifelse(df_oot_f$Patrimonio < 417419,1,2),levels = c(1,2),labels =  c("<417419.00",">= 417639.00"))


df_oot_f$ddvnr3MSF_ponderado_cat <- factor(ifelse(df_oot_f$ddvnr3MSF_ponderado <= 172.4,1, 2),levels = c(1,2),labels =  c("<172.4",">=172.4"))


df_oot_f$dddvnr6MSF_Max_cat <- factor(ifelse(df_oot_f$ddvnr6MSF_Max < 337.5,1,
                                             ifelse(df_oot_f$ddvnr6MSF_Max < 7788.2,2, 3)),levels = c(1,2,3),labels =  c("<337.5" ,"[337.5 - 7788.2>",">=7788.2"))


df_oot_f$ddatr12MSF_Max_cat <- factor(ifelse(df_oot_f$ddatr12MSF_Max < 8,1,
                                             ifelse(df_oot_f$ddatr12MSF_Max < 12,2, 3)),levels = c(1,2,3),labels =  c("<8" ,"[8 - 12>",">=12"))



df_oot_f$M_ddatr6MSF_10d_cat <- factor(ifelse(df_oot_f$M_ddatr6MSF_10d <= 0,1,
                                              ifelse(df_oot_f$M_ddatr6MSF_10d < 4,2, 3)),levels = c(1,2,3),labels =  c("0Meses" ,"1a4Meses","Masde4Meses"))


df_oot_f$M_ddatr3MSF_10d_cat <- factor(ifelse(df_oot_f$M_ddatr3MSF_10d <= 0,1,
                                              ifelse(df_oot_f$M_ddatr3MSF_10d <= 1,2, 3)),levels = c(1,2,3),labels =  c("0Meses" ,"1Mes","Masde1Mes"))


df_oot_f$ddatr3MSF_ponderadocat <- factor(ifelse(df_oot_f$ddatr3MSF_ponderado < 6,1,
                                                 ifelse(df_oot_f$ddatr3MSF_ponderado < 8,2, 3)),levels = c(1,2,3),labels =  c("<6días" ,"<8días",">=8días"))




df_oot_f$PNor1MSF_f_cat <- factor(ifelse(df_oot_f$PNor1MSF_f <= 78,1,
                                         ifelse(df_oot_f$PNor1MSF_f <= 97,2,3)),levels = c(1,2,3),labels =  c("[0-78]","<78-97]",">97"))

df_oot_f$MaxDmB_cat <- factor(ifelse(df_oot_f$MaxDmB <= 6,1,
                                     ifelse(df_oot_f$MaxDmB <= 9,2,
                                            ifelse(df_oot_f$MaxDmB <= 18,3,4))),levels = c(1,2,3,4),labels =  c("<=6dias" ,"<=9dias","<=18dias",">=19días"))


#2) categoricas



#RANG_INGRESO 
#9417 
#FLAG_LIMA_PROVINCIA 
#3386 
#EDAD 
#5326 
#ANTIGUEDAD 
#1762 



#quitamos ubicaion del local
df_oot_f <- df_oot_f[,c(1:3,16:25,6)]

table(df_oot_f$Sect_Apli)

df_oot_f$Sect_Apli <- factor(ifelse(df_oot_f$Sect_Apli == "Comercio",1,
                                    ifelse(df_oot_f$Sect_Apli == "Servicio",2,3)),levels = c(1,2,3),labels =  c("Comercio" ,"Servicio","Produccion"))






#############################################################################
#--------------- FIN 2DO INTENTO FEBRERO 2018 --------------------------------#
#############################################################################













#####################################################################################################
#----------------------------------------------------------------------------------------------------
#                                         MODELADO
#----------------------------------------------------------------------------------------------------
#####################################################################################################


#NOTA: data de desarrollo: df_train
#adta oot: df_oot_f


library(xlsx)
library(lubridate)

library(Information) # Data Exploration with Information Theory (Weight-of-Evidence and Information Value)
library(ROCR)        # Model Performance and ROC curve
library(caret)       # Classification and Regression Training -  for any machine learning algorithms
library(knitr)
library(DT)
#install.packages("devtools")
library(devtools)
#install.packages("woe")
library(woe)
#library(Information)
library(rpart)
library(rpart.plot)


base <- df_train


oot <- filter(df_oot_f, as.numeric(CODMES) >= 201701 & as.numeric(CODMES) <= 201704)

#Cargamos la data oot 201706 - 201709
ruta <- "D:/edwin/Modelo scoring cobranza/data_prueba"

b201706 <- filter(df_oot_f, as.numeric(CODMES) == 201706)
b201707 <- filter(df_oot_f, as.numeric(CODMES) >= 201707)
b201708 <- filter(df_oot_f, as.numeric(CODMES) >= 201708)
b201709 <- filter(df_oot_f, as.numeric(CODMES) >= 201709)


base$flag_pago_val <- as.numeric(as.character(base$flag_pago))
oot$flag_pago_val <- as.numeric(as.character(oot$flag_pago))
b201706$flag_pago_val <- as.numeric(as.character(b201706$flag_pago))
b201707$flag_pago_val <- as.numeric(as.character(b201707$flag_pago))
b201708$flag_pago_val <- as.numeric(as.character(b201708$flag_pago))
b201709$flag_pago_val <- as.numeric(as.character(b201709$flag_pago))



#base$flag_pago <- ifelse(base$flag_pago == 1, "X1_Bueno", "X0_Malo")



#oot$flag_pago <-  (ifelse(oot$flag_pago == 1, "X1_Bueno", "X0_Malo"))

#b201706$flag_pago <-  ifelse(b201706$flag_pago == 1, "X1_Bueno", "X0_Malo")

#b201707$flag_pago <-  ifelse(b201707$flag_pago == 1, "X1_Bueno", "X0_Malo")

#b201708$flag_pago <-  ifelse(b201708$flag_pago == 1, "X1_Bueno", "X0_Malo")

#b201709$flag_pago <-  ifelse(b201709$flag_pago == 1, "X1_Bueno", "X0_Malo")



base$flag_pago <- factor(base$flag_pago, levels = c(0,1), labels = c("X0_Malo","X1_Bueno"))


oot$flag_pago <-  factor(oot$flag_pago, levels = c(0,1), labels = c("X0_Malo","X1_Bueno"))

b201706$flag_pago <-  factor(b201706$flag_pago, levels = c(0,1), labels = c("X0_Malo","X1_Bueno"))

b201707$flag_pago <-  factor(b201707$flag_pago, levels = c(0,1), labels = c("X0_Malo","X1_Bueno"))

b201708$flag_pago <-  factor(b201708$flag_pago, levels = c(0,1), labels = c("X0_Malo","X1_Bueno"))

b201709$flag_pago <-  factor(b201709$flag_pago, levels = c(0,1), labels = c("X0_Malo","X1_Bueno"))



base_tr <- base

oot_tr <- oot


##############################################################################
################ FUNCIONES IMPORTANTES #######################################
##############################################################################


# Funcion 1: Crear un función para calcular la distribución porcentual de los factores
pct <- function(x){
  tbl <- table(x)
  tbl_pct <- cbind(tbl,round(prop.table(tbl)*100,2))
  colnames(tbl_pct) <- c('Count','Percentage')
  kable(tbl_pct)
}




# Funcion 2: Función propia para calcular IV, WOE y Eficiencia
gbpct <- function(x, y=base_tr$flag_pago){
  mt <- as.matrix(table(as.factor(x), as.factor(y))) # x -> independent variable(vector), y->dependent variable(vector)
  Total <- mt[,1] + mt[,2]                          # Total observations
  Total_Pct <- round(Total/sum(mt)*100, 2)          # Total PCT
  Bad_pct <- round((mt[,1]/sum(mt[,1]))*100, 2)     # PCT of BAd or event or response
  Good_pct <- round((mt[,2]/sum(mt[,2]))*100, 2)   # PCT of Good or non-event
  Bad_Rate <- round((mt[,1]/(mt[,1]+mt[,2]))*100, 2) # Bad rate or response rate
  grp_score <- round((Good_pct/(Good_pct + Bad_pct))*10, 2) # score for each group
  WOE <- round(log(Good_pct/Bad_pct)*10, 2)      # Weight of Evidence for each group
  g_b_comp <- ifelse(mt[,1] == mt[,2], 0, 1)
  IV <- ifelse(g_b_comp == 0, 0, (Good_pct - Bad_pct)*(WOE/10)) # Information value for each group
  Efficiency <- abs(Good_pct - Bad_pct)/2                       # Efficiency for each group
  otb<-as.data.frame(cbind(mt, Good_pct,  Bad_pct,  Total, 
                           Total_Pct,  Bad_Rate, grp_score, 
                           WOE, IV, Efficiency ))
  otb$Names <- rownames(otb)
  rownames(otb) <- NULL
  otb[,c(12,2,1,3:11)] # return IV table
}


# Funcion 3: Normalización usando rango
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

#table(oot$ddatr1MSF,oot$ddatr1MSF_f)
#table(oot$ddatr1MSF_f)

#############################################################################
#############################################################################

#flag pago -> 1 buenos
# flag pago -> 0 Malos



#1) ddatrMSF_ponderada_cat
# Máximo Atraso en el SF - t-1
#table(base_tr$ddatr1MSF_f)
#table(oot$ddatr1MSF,oot$ddatr1MSF_f)

# NA              1
# [0-1]        2
# [1-4]     3
# [4-7]     4
# >7           5 

names(base_tr)

A1 <- gbpct(base_tr$ddatrMSF_ponderada_cat)

#op1<-par(mfrow=c(1,1), new=TRUE)
plot(base_tr$ddatrMSF_ponderada_cat, base_tr$flag_pago, 
     ylab="Malos-Buenos", xlab="Categoría", 
     main="Atraso ponderado de los 6U meses en el SF ~ Malo-Buenos ")

barplot(A1$WOE, col="brown", names.arg=c(A1$Levels), 
        main="Score:Atraso ponderado de los 6U meses en el SF ~ Malo-Buenos",
        xlab="Categoría",
        ylab="WOE"
)

#par(op1)

kable(A1, caption = 'Atraso ponderado de los 6U meses en el SF ~ Malo-Buenos')


#2) Var41_f
#N° de meses con atrasos máximos hasta 15 días en el SF - t-12

#table(base_tr$Var41_f)
#table(oot$Var41_f)
#table(oot$Var41_f,oot$Var41)

#[0-9]    -> 1
#<9-11]   -> 2
#12       -> 3

A2 <- gbpct(base_tr$MaxDmB_15_cat)

#op1<-par(mfrow=c(1,1), new=TRUE)
plot(base_tr$MaxDmB_15_cat, base_tr$flag_pago, 
     ylab="Malos-Buenos", xlab="Categoría", 
     main="Tiene  meses con atrasos máximos hasta 15 días en el SF - NO/SI ~ Malo-Buenos ")

barplot(A2$WOE, col="brown", names.arg=c(A2$Levels), 
        main="Score:Tiene  meses con atrasos máximos hasta 15 días en el SF - NO/SI ~ Malo-Buenos",
        xlab="Categoría",
        ylab="WOE"
)

#par(op1)

kable(A2, caption = 'Score:Tiene  meses con atrasos máximos hasta 15 días en el SF - NO/SI ~ Malo-Buenos')





#3) ddvnrMSF_ponderada_cat
#Deuda directa vencida en el SF - t-1

#table(base_tr$ddvnr1MSF_f)
#table(oot$ddvnr1MSF_f)
#table(oot$ddvnr1MSF_f,oot$ddvnr1MSF)

#<=276  -> 1
#>276-> 2



A3 <- gbpct(base_tr$ddvnrMSF_ponderada_cat)

#op1<-par(mfrow=c(1,1), new=TRUE)
plot(base_tr$ddvnrMSF_ponderada_cat, base_tr$flag_pago, 
     ylab="Malos-Buenos", xlab="Categoría", 
     main="Deuda directa vencida ponderada 6U meses en el SF - Malo-Buenos ")

barplot(A3$WOE, col="brown", names.arg=c(A3$Levels), 
        main="Deuda directa vencida ponderada 6U meses en el SF - Malo-Bueno",
        xlab="Categoría",
        ylab="WOE"
)

#par(op1)

kable(A3, caption = 'Deuda directa vencida ponderada 6U meses en el SF - Malo-Bueno ~ Malo-Buenos')

#4) PNor1MSF_mean_cat
#Promedio % Normal en el SF - t-3

#table(base_tr$PrNo3MSF_f)
#table(oot$PrNo3MSF_f)
#table(oot$PrNo3MSF_f,oot$PNor3MSF)

#[0-78]    -> 1
#<78-97]  -> 2
#>97  -> 3


A4 <- gbpct(base_tr$PNor1MSF_mean_cat)

#op1<-par(mfrow=c(1,1), new=TRUE)
plot(base_tr$PNor1MSF_mean_cat, base_tr$flag_pago, 
     ylab="Malos-Buenos", xlab="Categoría", 
     main="Promedio % Normal en el SF - t-1 ~ Malo-Buenos ")

barplot(A4$WOE, col="brown", names.arg=c(A4$Levels), 
        main="Promedio % Normal en el SF - t-1 ~ Malo-Buenos",
        xlab="Categoría",
        ylab="WOE"
)

#par(op1)

kable(A4, caption = 'Promedio % Normal en el SF - t-1 ~ Malo-Buenos')


#5) Gtia_Soles_cat_f
#Posee Garantía

#table(base_tr$Gtia2_f)
#table(oot$Gtia2_f)
#table(oot$Gtia2_f,oot$Gtia_Soles)

#NO -> 1
#SI    -> 2

A5 <- gbpct(base_tr$Gtia_Soles_cat_f)

#op1<-par(mfrow=c(1,1), new=TRUE)
plot(base_tr$Gtia_Soles_cat_f, base_tr$flag_pago, 
     ylab="Malos-Buenos", xlab="Categoría", 
     main="Posee Garantía NO/SI ~ Malo-Buenos ")

barplot(A5$WOE, col="brown", names.arg=c(A5$Levels), 
        main="Posee Garantía NO/SI ~ Malo-Buenos",
        xlab="Categoría",
        ylab="WOE"
)

#par(op1)

kable(A5, caption = 'Posee Garantía NO/SI ~ Malo-Buenos')

#names(base_tr)

#6) DesUbicacionLocal_cat
#Sector Económico

#table(base_tr$SecEco_f)
#table(oot$SecEco_f)
#table(oot$SecEco_f,oot$Sect_Apli)


#NA   -> 1
#Vivienda - Campo Ferial   -> 2
#Otros -> 3



#esto no #


#A6 <- gbpct(base_tr$DesUbicacionLocal_cat)

#plot(base_tr$DesUbicacionLocal_cat, base_tr$flag_pago, 
#    ylab="Malos-Buenos", xlab="Categoría", 
#   main="Ubicación del local ~ Malo-Buenos ")

#barplot(A6$WOE, col="brown", names.arg=c(A6$Levels), 
#       main="Ubicación del local ~ Malo-Buenos",
#      xlab="Categoría",
#     ylab="WOE"
#)



#kable(A6, caption = 'Ubicación del local ~ Malo-Buenos')

#fin

#names(base_tr)

#7) Patrimonio_cat
#Promedio % Normal en el SF - t-3

#table(base_tr$Patrimo_f)
#table(oot$Patrimo_f)
#table(oot$Patrimo_f,oot$Patrimonio)

#<417639.00          -> 1
# >= 417639.00       -> 2



A7 <- gbpct(base_tr$Patrimonio_cat)

#op1<-par(mfrow=c(1,1), new=TRUE)
plot(base_tr$Patrimonio_cat, base_tr$flag_pago, 
     ylab="Malos-Buenos", xlab="Categoría", 
     main="Patrimonio ~ Malo-Buenos ")

barplot(A7$WOE, col="brown", names.arg=c(A7$Levels), 
        main="Patrimonio ~ Malo-Buenos",
        xlab="Categoría",
        ylab="WOE"
)

#par(op1)

kable(A7, caption = 'Patrimonio ~ Malo-Buenos')

#names(base_tr)



###################################
# WOE E IV
##################################


IV <- Information::create_infotables(data=base_tr[4:15], NULL, y="flag_pago_val", 10)
IV$Summary$IV <- round(IV$Summary$IV*100,2)

IV$Tables

kable(IV$Summary)




#########################################################################################
############# observación: no cumple la regla de woe la variable patrimonio #############
#########################################################################################


# require library(Information) 

#correr esto me genera conflicto con el paquete caret

#prueb <- base_tr
#prueb$good_bad_21<-as.numeric(ifelse(prueb$flag_pago == "X1_Bueno", 1, 0))
#IV <- Information::create_infotables(data=prueb, NULL, y="good_bad_21", 10)
#IV$Summary$IV <- round(IV$Summary$IV*100,2)

#IV$Tables

#kable(IV$Summary)


##################################################################################
####################### parte 2 ##################################
##################################################################3####333

############################################################################
#######  MUESTREO ##############################################################
################################################################################
set.seed(1992)

##########################################
#a) muestreo aleatorio


div_part <- sort(sample(nrow(base_tr), nrow(base_tr)*.7))

#Muestra train 70%
train<-base_tr[div_part,] # 70% 
pct(train$flag_pago)

# Muestra test 30%
test<-base_tr[-div_part,] # 30%
pct(test$flag_pago)


var = flag_pago~Patrimonio_cat+MaxDmB_15_cat+ddatrMSF_ponderada_cat+ddvnrMSF_ponderada_cat+PNor1MSF_mean_cat


var = flag_pago~PNor1MSF_f_cat+ddatr3MSF_ponderadocat+M_ddatr3MSF_10d_cat+dddvnr6MSF_Max_cat+Patrimonio_cat+Gtia_Soles_cat_f+Sect_Apli

var = flag_pago~MaxDmB_cat +ddatr3MSF_ponderadocat+ PNor1MSF_f_cat + M_ddatr6MSF_10d_cat + dddvnr6MSF_Max_cat + Patrimonio_cat + Gtia_Soles_cat_f + Sect_Apli


  
#############################################
#
################################################3
#otra manera de dividir la data
#################################################

#Remuestreo

set.seed(1992)
inTrain <- createDataPartition(y = base_tr$flag_pago, p=0.7, list=FALSE)
training <- base_tr[inTrain,]
pct(training$flag_pago)
testing <- base_tr[-inTrain,]
pct(testing$flag_pago)

#########################################################
## FALTA LA OTRA MANERA DE DIVIDIR LA DATA CON LO DE LA PAG DE SCORE


###################################################################
####################################################################
#################### PARTE 3 ########################################
####################################################################

########3 MODELOS ##############################################
#a partir de acá vienen los modelos

#############################################################
#                        REGRESION LOGÍSTICA 
#######################################################3


#------------------------------------------------------------
#1) Regresión Logística - corte normal
# ------------------------------------------------------------

inicio_todo <- Sys.time()

ini <- Sys.time()

names(train)

model_rl_1 <- glm(var,
                  data = train, family = "binomial")

summary(model_rl_1)

predic_model_rl_1 <- predict(model_rl_1, newdata = test,type = "response")

fin <- Sys.time()

time_m1 <- fin - ini

summary(predic_model_rl_1)

tb1 <- table(test$flag_pago,ifelse(predic_model_rl_1>0.5,"X1_Bueno","X0_Malo"))
tb1
table(test$flag_pago)
m1_accuracy <- (665 + 1367)/nrow(test)
m1_accuracy
#0.7303493

#score test data set
test$m1_score <- predict(model_rl_1,type='response',test)
m1_pred <- prediction(test$m1_score, test$flag_pago)
m1_perf <- performance(m1_pred,"tpr","fpr")

#ROC
plot(m1_perf, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision <- performance(m1_pred, measure = "prec", x.measure = "rec")
plot(m1_perf_precision, main="m1 Logistic:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc <- performance(m1_pred, measure = "acc")
plot(m1_perf_acc, main="m1 Logistic:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS <- round(max(attr(m1_perf,'y.values')[[1]]-attr(m1_perf,'x.values')[[1]])*100, 2)
m1_AUROC <- round(performance(m1_pred, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini <- (2*m1_AUROC - 100)

###########################################################################
cat("AUROC: ",m1_AUROC,"\tKS: ", m1_KS, "\tGini:", m1_Gini, "\tAccuracy:", m1_accuracy,"\n")


#------------------------------------------------------------------------
# TUNING REG LOGISTICO - con muestreo - cut of optimo
#------------------------------------------------------------------------
#install.packages("InformationValue")
library(InformationValue)
optCutOff <- optimalCutoff(test$flag_pago_val, predic_model_rl_1)[1] 
optCutOff



tb2 <- table(test$flag_pago,ifelse(predic_model_rl_1>optCutOff,"X1_Bueno","X0_Malo"))
tb2
m2_accuracy <-  ( 621+1517)/nrow(test)
m2_accuracy
#0.7316906

#score test data set
test$m2_score <- predict(model_rl_1,type='response',test)
m2_pred <- prediction(ifelse(test$m1_score>optCutOff,1,0), test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf <- performance(m2_pred,"tpr","fpr")

#ROC
plot(m2_perf, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision <- performance(m2_pred, measure = "prec", x.measure = "rec")
plot(m2_perf_precision, main="m1 Logistic:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc <- performance(m2_pred, measure = "acc")
plot(m2_perf_acc, main="m1 Logistic:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS <- round(max(attr(m2_perf,'y.values')[[1]]-attr(m2_perf,'x.values')[[1]])*100, 2)
m2_AUROC <- round(performance(m2_pred, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini <- (2*m2_AUROC - 100)

###########################################################################
cat("AUROC: ",m2_AUROC,"\tKS: ", m2_KS, "\tGini:", m2_Gini, "\tAccuracy:", m2_accuracy,"\n")

#------------------------------------------------------------------------
# 3.- REGRESION LOGISTICA - TUNEO DE PARAMETRO CON CARET - MAS - con train
#------------------------------------------------------------------------


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)

set.seed(1992)
model_rl_3_tun <- train(var,  data=train, method="glm", 
                        family="binomial", trControl = trControl, metric = 'ROC',tuneLength = 10)

fin <- Sys.time()

time_m3 <- fin - ini

summary(model_rl_3_tun)
summary(model_rl_1)
model_rl_3_tun
#model_rl_2_tun$finalModel$fitted.values
pred3 <- predict(model_rl_3_tun,newdata = test, type = "prob")


#score test data set
test$m3_score <- predict(model_rl_3_tun,type='prob',newdata=test)[2]
m3_pred <- prediction(test$m3_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3 <- table(ifelse(test$m3_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
tb3

m3_accuracy <- (592+1517)/nrow(test)
m3_accuracy

m3_perf <- performance(m3_pred,"tpr","fpr")

#ROC
plot(m3_perf, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision <- performance(m3_pred, measure = "prec", x.measure = "rec")
plot(m3_perf_precision, main="m1 Logistic:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc <- performance(m3_pred, measure = "acc")
plot(m3_perf_acc, main="m1 Logistic:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS <- round(max(attr(m3_perf,'y.values')[[1]]-attr(m3_perf,'x.values')[[1]])*100, 2)
m3_AUROC <- round(performance(m3_pred, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini <- (2*m3_AUROC - 100)

###########################################################################
cat("AUROC: ",m3_AUROC,"\tKS: ", m3_KS, "\tGini:", m3_Gini,"\tAccuracy:", m3_accuracy, "\n")


#----------------------------------------------------------------------
#con La otra base training
#----------------------------------------------------------------------



#------------------------------------------------------------
#4) Regresión Logística - corte normal
#------------------------------------------------------------

ini <- Sys.time()

model_rl_4 <- glm(var,
                  data = training, family = "binomial")


fin <- Sys.time()

time_m4 <- fin - ini

summary(model_rl_4)

predic_model_rl_4 <- predict(model_rl_4, newdata = testing,type = "response")

summary(predic_model_rl_4)


tb4 <- table(testing$flag_pago,ifelse(predic_model_rl_4>0.5,"X1_Bueno","X0_Malo"))
tb4

m4_accuracy <- (576 + 1509)/nrow(testing)
#0.7527312

#score test data set
testing$m4_score <- predict(model_rl_4,type='response',testing)
m4_pred <- prediction(testing$m4_score, testing$flag_pago)
m4_perf <- performance(m4_pred,"tpr","fpr")

#ROC
plot(m4_perf, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision <- performance(m4_pred, measure = "prec", x.measure = "rec")
plot(m4_perf_precision, main="m1 Logistic:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc <- performance(m4_pred, measure = "acc")
plot(m4_perf_acc, main="m1 Logistic:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS <- round(max(attr(m4_perf,'y.values')[[1]]-attr(m4_perf,'x.values')[[1]])*100, 2)
m4_AUROC <- round(performance(m4_pred, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini <- (2*m4_AUROC - 100)

###########################################################################
cat("AUROC: ",m4_AUROC,"\tKS: ", m4_KS, "\tGini:", m4_Gini,"\tAccuracy:", m4_accuracy, "\n")


#------------------------------------------------------------------------
# TUNING REG LOGISTICO - con muestreo - cut of optimo
#------------------------------------------------------------------------
#install.packages("InformationValue")
#library(InformationValue)
optCutOff2 <- optimalCutoff(testing$flag_pago_val, predic_model_rl_4)[1] 
optCutOff2


#------------------------------------------------------------------------
# 5.- REGRESION LOGISTICA - TUNEO DE PARAMETRO CON CARET - MAS - con training
#------------------------------------------------------------------------

#plot(model_rl_3_tun, scales = list(x = list(log = 10)))


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)

set.seed(1992)
model_rl_5_tun <- train(var,  data=training, method="glm", 
                        family="binomial", trControl = trControl, metric = 'ROC',tuneLength = 10)

fin <- Sys.time()

time_m5 <- fin - ini

summary(model_rl_5_tun)

model_rl_5_tun$finalModel
summary(model_rl_1)
model_rl_5_tun
model_rl_3_tun


#model_rl_2_tun$finalModel$fitted.values
pred5 <- predict(model_rl_5_tun,newdata = testing, type = "prob")


#score test data set
testing$m5_score <- predict(model_rl_5_tun,type='prob',newdata=testing)[2]
m5_pred <- prediction(testing$m5_score, testing$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(testing$m5_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
m5_accuracy <- (576+1509)/nrow(testing)
m5_accuracy
#0.7328767

m5_perf <- performance(m5_pred,"tpr","fpr")

#ROC
plot(m5_perf, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision <- performance(m5_pred, measure = "prec", x.measure = "rec")
plot(m5_perf_precision, main="m1 Logistic:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc <- performance(m5_pred, measure = "acc")
plot(m5_perf_acc, main="m1 Logistic:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS <- round(max(attr(m5_perf,'y.values')[[1]]-attr(m5_perf,'x.values')[[1]])*100, 2)
m5_AUROC <- round(performance(m5_pred, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini <- (2*m5_AUROC - 100)

###########################################################################
cat("AUROC: ",m5_AUROC,"\tKS: ", m5_KS, "\tGini:", m5_Gini, "\tAccuracy:", m5_accuracy,"\n")
cat("AUROC: ",m4_AUROC,"\tKS: ", m4_KS, "\tGini:", m4_Gini, "\tAccuracy:", m4_accuracy,"\n")
cat("AUROC: ",m3_AUROC,"\tKS: ", m3_KS, "\tGini:", m3_Gini, "\tAccuracy:", m3_accuracy,"\n")
cat("AUROC: ",m2_AUROC,"\tKS: ", m2_KS, "\tGini:", m2_Gini, "\tAccuracy:", m2_accuracy,"\n")
cat("AUROC: ",m1_AUROC,"\tKS: ", m1_KS, "\tGini:", m1_Gini, "\tAccuracy:", m1_accuracy,"\n")


#########################################################################################
#---------------------------------------------------------------------------------------
#                             ARBOL DE DECISION
#---------------------------------------------------------------------------------------
########################################################################################

#------------------------------------------------------------
#6) Arbol - data train  y test
# ------------------------------------------------------------

#se usará el paquete rpart

ini <- Sys.time()

names(train)
set.seed(1992)
model_6_ad_1 <- rpart(var,data = train, control = rpart.control(cp=0.0001, minsplit = 25))


fin <- Sys.time()

time_m6 <- fin - ini

#model_6_ad_1 <- rpart(var,data = train)

summary(model_6_ad_1)
printcp(model_6_ad_1)


############################
######## Poda #############
############################

printcp(model_6_ad_1)
bestcp <- model_6_ad_1$cptable[which.min(model_6_ad_1$cptable[,"xerror"]),"CP"]

# Prune the tree using the best cp.
model_6_ad_1 <- prune(model_6_ad_1, cp = bestcp)

prp(model_6_ad_1, faclen = 0, cex = 0.5, extra = 1)
prp(model_6_ad_1, type = 2, cex = 0.5,extra = 1)
#para ver el gráfico rpart.plot
rpart.plot(model_6_ad_1)
#summary(model_6_ad_1)

predic_model_6_ad_1 <- predict(model_6_ad_1, newdata = test,type = "prob")[,2]

summary(predic_model_6_ad_1)

tb6 <- table(test$flag_pago,ifelse(predic_model_6_ad_1>0.5,"X1_Bueno","X0_Malo"))
tb6

m6_accuracy <- (622 + 1493)/nrow(test)
m6_accuracy
#0.7696507

#score test data set
test$m6_ad_score <- predict(model_6_ad_1,type='prob',test)[,2]
m6_pred <- prediction(test$m6_ad_score, test$flag_pago)
m6_perf <- performance(m6_pred,"tpr","fpr")

#ROC
plot(m6_perf, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision, main="m6 Árbol de decisión:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc <- performance(m6_pred, measure = "acc")
plot(m6_perf_acc, main="m6 Árbol de decisión:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS <- round(max(attr(m6_perf,'y.values')[[1]]-attr(m6_perf,'x.values')[[1]])*100, 2)
m6_AUROC <- round(performance(m6_pred, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini <- (2*m6_AUROC - 100)

###########################################################################
cat("AUROC: ",m6_AUROC,"\tKS: ", m6_KS, "\tGini:", m6_Gini, "\tAccuracy:", m6_accuracy,"\n")

#------------------------------------------------------------
#7) Arbol - data train  y test - con caret
# ------------------------------------------------------------


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)



set.seed(1992)
model_7_ad_tun_1 <- train(var,  data=train, method="rpart", 
                          trControl = trControl, metric = 'ROC',tuneLength = 13)

fin <- Sys.time()

time_m7 <- fin - ini

prp(model_7_ad_tun_1$finalModel, faclen = 0, cex = 0.5, extra = 1)
#print(model_7_ad_tun_1)
#summary(model_7_ad_tun_1)

plot(model_7_ad_tun_1$finalModel)
text(model_7_ad_tun_1$finalModel)

plot(model_7_ad_tun_1)

#library(rattle)
fancyRpartPlot(model_7_ad_tun_1$finalModel, cex = 0.3)

model_7_ad_tun_1$bestTune


model_7_ad_tun_1



#model_rl_2_tun$finalModel$fitted.values
pred7 <- predict(model_7_ad_tun_1,newdata = test, type = "prob")


#score test data set
test$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=test)[2]
m7_pred <- prediction(test$m7_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb7 <- table(ifelse(test$m7_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
tb7

m7_accuracy <- (630+ 1467)/nrow(test)
m7_accuracy
#0.7631004

m7_perf <- performance(m7_pred,"tpr","fpr")

#ROC
plot(m7_perf, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision <- performance(m7_pred, measure = "prec", x.measure = "rec")
plot(m7_perf_precision, main="m7 Arbol de decision:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc <- performance(m7_pred, measure = "acc")
plot(m7_perf_acc, main="m7 Arbol tuneado:Accuracy as function of threshold")

#KS, Gini & AUC m1
m7_KS <- round(max(attr(m7_perf,'y.values')[[1]]-attr(m7_perf,'x.values')[[1]])*100, 2)
m7_AUROC <- round(performance(m7_pred, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini <- (2*m7_AUROC - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")

#------------------------------------------------------------
# 8)Arbol de decision - CART - data training
#------------------------------------------------------------

ini <- Sys.time()

set.seed(1992)
model_8_ad_2 <- rpart(var, data = training,control = rpart.control(cp=0.00033, minsplit = 60, minbucket = 20))

fin <- Sys.time()

time_m8 <- fin - ini

printcp(model_8_ad_2)


############################
######## Poda #############
############################

printcp(model_8_ad_2)
bestcp <- model_8_ad_2$cptable[which.min(model_8_ad_2$cptable[,"xerror"]),"CP"]

# Prune the tree using the best cp.
model_8_ad_2 <- prune(model_8_ad_2, cp = bestcp)

prp(model_8_ad_2, faclen = 0, cex = 0.5, extra = 1)
prp(model_8_ad_2, type = 2, cex = 0.5,extra = 1)
#para ver el gráfico rpart.plot
rpart.plot(model_8_ad_2,cex = 0.5)
#summary(model_6_ad_1)



predic_model_8_ad_2 <- predict(model_8_ad_2, newdata = testing,type = "prob")[,2]

summary(predic_model_8_ad_2)


tb8 <- table(testing$flag_pago,ifelse(predic_model_8_ad_2>0.5,"X1_Bueno","X0_Malo"))
tb8

m8_accuracy <- (637 + 1443)/nrow(testing)
m8_accuracy
#0.7304795

#score test data set
testing$m8_score <- predict(model_8_ad_2,type='prob',testing)[,2]
m8_pred <- prediction(testing$m8_score, testing$flag_pago)
m8_perf <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision <- performance(m8_pred, measure = "prec", x.measure = "rec")
plot(m8_perf_precision, main="m8 Arbol de decision - rpart:Precision/recall curve")


# Plot accuracy as function of threshold
m8_perf_acc <- performance(m8_pred, measure = "acc")
plot(m8_perf_acc, main="m8 Arbol de decision - rpart:Accuracy as function of threshold")

#KS, Gini & AUC m1
m8_KS <- round(max(attr(m8_perf,'y.values')[[1]]-attr(m8_perf,'x.values')[[1]])*100, 2)
m8_AUROC <- round(performance(m8_pred, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini <- (2*m8_AUROC - 100)

###########################################################################
cat("AUROC: ",m8_AUROC,"\tKS: ", m8_KS, "\tGini:", m8_Gini,"\tAccuracy:", m8_accuracy, "\n")



#------------------------------------------------------------------------
# 9.- ARBOL DE DECISION RPART - TUNEO DE PARAMETRO CON CARET - MAE - con training
#------------------------------------------------------------------------

ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)

grid_ad <- data.frame(cp = seq(0.0003, 0.0005, by = 0.00001))

set.seed(1992)
#model_9_ad_tun_2 <- train(var,  data=training, method="rpart", 
#                        trControl = trControl, metric = 'ROC',tuneLength = 15)

model_9_ad_tun_2 <- train(var,  data=training, method="rpart", 
                          trControl = trControl, metric = 'ROC', tuneGrid = grid_ad,
                          control = rpart.control(minsplit = 60, minbucket = 20))

fin <- Sys.time()

time_m9 <- fin - ini

prp(model_9_ad_tun_2$finalModel, faclen = 0, cex = 0.5, extra = 1)


#print(model_7_ad_tun_1)
#summary(model_7_ad_tun_1)

plot(model_9_ad_tun_2$finalModel)
text(model_9_ad_tun_2$finalModel)

plot(model_9_ad_tun_2)

#library(rattle)
fancyRpartPlot(model_9_ad_tun_2$finalModel, cex = 0.5)

model_9_ad_tun_2$bestTune


model_9_ad_tun_2

model_9_ad_tun_2$finalModel



#model_rl_2_tun$finalModel$fitted.values
pred9 <- predict(model_9_ad_tun_2,newdata = testing, type = "prob")[,2]


#score test data set
testing$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=testing)[,2]
m9_pred <- prediction(testing$m9_score, testing$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb9 <- table(ifelse(testing$m9_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
tb9

m9_accuracy <- (648+1426)/nrow(testing)
m9_accuracy
#0.7552804

m9_perf <- performance(m9_pred,"tpr","fpr")

#ROC
plot(m9_perf, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision <- performance(m9_pred, measure = "prec", x.measure = "rec")
plot(m9_perf_precision, main="m9 Arbol de decision - caret:Precision/recall curve")


# Plot accuracy as function of threshold
m9_perf_acc <- performance(m9_pred, measure = "acc")
plot(m9_perf_acc, main="m9 Arbol de decision - caret:Accuracy as function of threshold")

#KS, Gini & AUC m1
m9_KS <- round(max(attr(m9_perf,'y.values')[[1]]-attr(m9_perf,'x.values')[[1]])*100, 2)
m9_AUROC <- round(performance(m9_pred, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini <- (2*m9_AUROC - 100)

###########################################################################
cat("AUROC: ",m9_AUROC,"\tKS: ", m9_KS, "\tGini:", m9_Gini, "\tAccuracy:", m9_accuracy,"\n")

##############################################################################
#ARBOLES CARET
#
cat("AUROC: ",m6_AUROC,"\tKS: ", m6_KS, "\tGini:", m6_Gini, "\tAccuracy:", m6_accuracy,"\n")
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")
cat("AUROC: ",m8_AUROC,"\tKS: ", m8_KS, "\tGini:", m8_Gini,"\tAccuracy:", m8_accuracy, "\n")
cat("AUROC: ",m9_AUROC,"\tKS: ", m9_KS, "\tGini:", m9_Gini, "\tAccuracy:", m9_accuracy,"\n")

#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE II           ARBOL DE DECISION        CTREE
#---------------------------------------------------------------------------------------
########################################################################################


#------------------------------------------------------------
# 10. ARBOL DE DECISION CTREE - TRAIN
# ------------------------------------------------------------

#se usará el paquete rpart

library(partykit)
library(party)

names(train)

#model_10_ad_1 <- ctree(var,data = train, control = rpart.control(cp=0.0001, minsplit = 25))

ini <- Sys.time()

set.seed(1992)
model_10_ad_1 <- ctree(var,data = train, controls = ctree_control(mincriterion = 0.5, maxdepth = 6))

fin <- Sys.time()

time_m10 <- fin - ini

#model_6_ad_1 <- rpart(var,data = train)

plot(model_10_ad_1)



#predic_model_10_ad_1 <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = test,type = "prob")))))[,2]

#otra manera de sacar las probilidades del target
#dt_test$predProb = sapply(predict(model, newdata=dt_test,type="prob"),'[[',2)  # obtain probability of class 1 (second element from the lists)
predic_model_10_ad_1 <- sapply(predict(model_10_ad_1, newdata = test,type = "prob"),'[[',2)

summary(predic_model_10_ad_1)

tb10 <- table(test$flag_pago,ifelse(predic_model_10_ad_1>0.5,"X1_Bueno","X0_Malo"))
tb10

m10_accuracy <- (709 + 1410)/nrow(test)
m10_accuracy
#0.7711063

#score test data set
test$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = test,type = "prob")))))[,2]

m10_pred <- prediction(test$m10_ad_score, test$flag_pago)
m10_perf <- performance(m10_pred,"tpr","fpr")

#ROC
plot(m10_perf, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision <- performance(m10_pred, measure = "prec", x.measure = "rec")
plot(m10_perf_precision, main="m6 Árbol de decisión - CTREE:Precision/recall curve")


# Plot accuracy as function of threshold
m10_perf_acc <- performance(m10_pred, measure = "acc")
plot(m10_perf_acc, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold")

#KS, Gini & AUC m1
m10_KS <- round(max(attr(m10_perf,'y.values')[[1]]-attr(m10_perf,'x.values')[[1]])*100, 2)
m10_AUROC <- round(performance(m10_pred, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini <- (2*m10_AUROC - 100)

###########################################################################
cat("AUROC: ",m10_AUROC,"\tKS: ", m10_KS, "\tGini:", m10_Gini, "\tAccuracy:", m10_accuracy,"\n")


#------------------------------------------------------------
# 11. ARBOL DE DECISION CTREE - TRAIN - TUNEADO CON CTREE2
# ------------------------------------------------------------

ini <- Sys.time()


trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)



set.seed(1992)
model_11_ad_tun_1 <- train(var,  data=train, method="ctree2", 
                           trControl = trControl, metric = 'ROC',tuneLength = 13)


fin <- Sys.time()

time_m11 <- fin - ini

plot(model_11_ad_tun_1$finalModel)

plot(model_11_ad_tun_1$finalModel, gp = gpar(fontsize = 0.5),     # font size changed to 6
     inner_panel=node_inner,
     ip_args=list(
       abbreviate = TRUE, 
       id = FALSE)
)


model_11_ad_tun_1$bestTune


model_11_ad_tun_1


#model_rl_2_tun$finalModel$fitted.values

pred11 <-predict(model_11_ad_tun_1, newdata = test,type = "prob")[,2]


#score test data set
test$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=test)[2]
m11_pred <- prediction(test$m11_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb11 <- table(ifelse(test$m11_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
tb11

m11_accuracy <- (619+1498)/nrow(test)
m11_accuracy
#0.7703785

m11_perf <- performance(m11_pred,"tpr","fpr")

#ROC
plot(m11_perf, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision <- performance(m11_pred, measure = "prec", x.measure = "rec")
plot(m11_perf_precision, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve")


# Plot accuracy as function of threshold
m11_perf_acc <- performance(m11_pred, measure = "acc")
plot(m11_perf_acc, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold")

#KS, Gini & AUC m1
m11_KS <- round(max(attr(m11_perf,'y.values')[[1]]-attr(m11_perf,'x.values')[[1]])*100, 2)
m11_AUROC <- round(performance(m11_pred, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini <- (2*m11_AUROC - 100)

###########################################################################
cat("AUROC: ",m11_AUROC,"\tKS: ", m11_KS, "\tGini:", m11_Gini, "\tAccuracy:", m11_accuracy,"\n")


#------------------------------------------------------------
# 12.ARBOL DE DECISION CTREE - rpart - TRAINING
#------------------------------------------------------------

ini <- Sys.time()

set.seed(1992)
model_12_ad_2 <- ctree(var,data = training)

fin <- Sys.time()

time_m12 <- fin - ini


predic_model_12_ad_2 <- sapply(predict(model_12_ad_2, newdata = testing,type = "prob"),'[[',2)

summary(predic_model_12_ad_2)


tb12 <- table(testing$flag_pago,ifelse(predic_model_12_ad_2>0.5,"X1_Bueno","X0_Malo"))
tb12

m12_accuracy <- (677 + 1397)/nrow(testing)
m12_accuracy
#0.7552804

#score test data set
testing$m12_score <- sapply(predict(model_12_ad_2, newdata = testing,type = "prob"),'[[',2)
m12_pred <- prediction(testing$m12_score, testing$flag_pago)
m12_perf <- performance(m12_pred,"tpr","fpr")

#ROC
plot(m12_perf, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision <- performance(m12_pred, measure = "prec", x.measure = "rec")
plot(m12_perf_precision, main="m12 Arbol de decision - CTREE :Precision/recall curve")


# Plot accuracy as function of threshold
m12_perf_acc <- performance(m12_pred, measure = "acc")
plot(m12_perf_acc, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold")

#KS, Gini & AUC m1
m12_KS <- round(max(attr(m12_perf,'y.values')[[1]]-attr(m12_perf,'x.values')[[1]])*100, 2)
m12_AUROC <- round(performance(m12_pred, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini <- (2*m12_AUROC - 100)

###########################################################################
cat("AUROC: ",m12_AUROC,"\tKS: ", m12_KS, "\tGini:", m12_Gini,"\tAccuracy:", m12_accuracy, "\n")



#------------------------------------------------------------------------
# 13. ARBOL DE DECISION CTREE - TUNEO DE PARAMETRO CON CARET - MAE - con training
#------------------------------------------------------------------------

ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)


set.seed(1992)
model_13_ad_tun_1 <- train(var,  data=training, method="ctree2", 
                           trControl = trControl, metric = 'ROC',tuneLength = 13)


fin <- Sys.time()

time_m13 <- fin - ini

model_13_ad_tun_1

model_13_ad_tun_1$bestTune


model_13_ad_tun_1$finalModel



#model_rl_2_tun$finalModel$fitted.values
pred13 <- predict(model_13_ad_tun_1,newdata = testing, type = "prob")[,2]


#score test data set
testing$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=testing)[,2]
m13_pred <- prediction(testing$m13_score, testing$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(testing$m13_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
m13_accuracy <- (571+1501)/nrow(testing)
m13_accuracy
#0.7545521

m13_perf <- performance(m13_pred,"tpr","fpr")

#ROC
plot(m13_perf, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision <- performance(m13_pred, measure = "prec", x.measure = "rec")
plot(m13_perf_precision, main="m13 Arbol de decision - CTREE:Precision/recall curve")


# Plot accuracy as function of threshold
m13_perf_acc <- performance(m13_pred, measure = "acc")
plot(m13_perf_acc, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold")

#KS, Gini & AUC m1
m13_KS <- round(max(attr(m13_perf,'y.values')[[1]]-attr(m13_perf,'x.values')[[1]])*100, 2)
m13_AUROC <- round(performance(m13_pred, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini <- (2*m13_AUROC - 100)

###########################################################################
cat("AUROC: ",m13_AUROC,"\tKS: ", m13_KS, "\tGini:", m13_Gini, "\tAccuracy:", m13_accuracy,"\n")


##############################################################################
#ARBOLES CTREE
#
cat("AUROC: ",m10_AUROC,"\tKS: ", m10_KS, "\tGini:", m10_Gini, "\tAccuracy:", m10_accuracy,"\n")
cat("AUROC: ",m11_AUROC,"\tKS: ", m11_KS, "\tGini:", m11_Gini, "\tAccuracy:", m11_accuracy,"\n")
cat("AUROC: ",m12_AUROC,"\tKS: ", m12_KS, "\tGini:", m12_Gini,"\tAccuracy:", m12_accuracy, "\n")
cat("AUROC: ",m13_AUROC,"\tKS: ", m13_KS, "\tGini:", m13_Gini, "\tAccuracy:", m13_accuracy,"\n")


#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE III          MODELOS ENSAMBLADOS
#---------------------------------------------------------------------------------------
########################################################################################


#------------------------------------------------------------
# 14. RANDOMFOREST - TRAIN
# ------------------------------------------------------------

#se usará el paquete randomForest

library(randomForest)

ini <- Sys.time()

#probar con mtry = 2 y con minsplit = p^0.5
set.seed(1992)
model_14_rf_1 <- randomForest(var,data = train, ntree = 1000, mtry = 2)

fin <- Sys.time()

time_m14 <- fin - ini

#tune.rf <- tuneRF(train[,-c(1:9,17:19)],train[,17], stepFactor=0.5)

plot(model_14_rf_1)
importance(model_14_rf_1)
predic_model_14_rf_1 <- predict(model_14_rf_1, newdata = test,type = "prob")[,2]

summary(predic_model_14_rf_1)

tb14 <- table(test$flag_pago,ifelse(predic_model_14_rf_1>0.5,"X1_Bueno","X0_Malo"))
tb14

m14_accuracy <- (638 + 1502)/nrow(test)
m14_accuracy
#0.7323751

#score test data set
test$m14_rf_score <- predict(model_14_rf_1, newdata = test,type = "prob")[,2]

m14_pred <- prediction(test$m14_rf_score, test$flag_pago)
m14_perf <- performance(m14_pred,"tpr","fpr")

#ROC
plot(m14_perf, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision <- performance(m14_pred, measure = "prec", x.measure = "rec")
plot(m14_perf_precision, main="m14 Random Forest:Precision/recall curve")


# Plot accuracy as function of threshold
m14_perf_acc <- performance(m14_pred, measure = "acc")
plot(m14_perf_acc, main="m14 Random Forest:Accuracy as function of threshold")

#KS, Gini & AUC m1
m14_KS <- round(max(attr(m14_perf,'y.values')[[1]]-attr(m14_perf,'x.values')[[1]])*100, 2)
m14_AUROC <- round(performance(m14_pred, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini <- (2*m14_AUROC - 100)

###########################################################################
cat("AUROC: ",m14_AUROC,"\tKS: ", m14_KS, "\tGini:", m14_Gini, "\tAccuracy:", m14_accuracy,"\n")


#------------------------------------------------------------
# 15. RANDOM FOREST - TRAIN - TUNEADO
# ------------------------------------------------------------


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)


library(ranger)

set.seed(1992)
model_15_RF_tun_1 <- train(var,  data=train, method="ranger", 
                           trControl = trControl, metric = 'ROC',tuneLength = 13)


fin <- Sys.time()

time_m15 <- fin - ini

plot(model_15_RF_tun_1)



model_15_RF_tun_1$bestTune


model_15_RF_tun_1


#model_rl_2_tun$finalModel$fitted.values

pred15 <-predict(model_15_RF_tun_1, newdata = test,type = "prob")[,2]


#score test data set
test$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=test)[2]
m15_pred <- prediction(test$m15_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(test$m15_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
m15_accuracy <- (641+1501)/nrow(test)
m15_accuracy
#0.7330595

m15_perf <- performance(m15_pred,"tpr","fpr")

#ROC
plot(m15_perf, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision <- performance(m15_pred, measure = "prec", x.measure = "rec")
plot(m15_perf_precision, main="m15 RandomForest:Precision/recall curve")


# Plot accuracy as function of threshold
m15_perf_acc <- performance(m15_pred, measure = "acc")
plot(m15_perf_acc, main="m15 RandomForest:Accuracy as function of threshold")

#KS, Gini & AUC m1
m15_KS <- round(max(attr(m15_perf,'y.values')[[1]]-attr(m15_perf,'x.values')[[1]])*100, 2)
m15_AUROC <- round(performance(m15_pred, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini <- (2*m15_AUROC - 100)

###########################################################################
cat("AUROC: ",m15_AUROC,"\tKS: ", m15_KS, "\tGini:", m15_Gini, "\tAccuracy:", m15_accuracy,"\n")


#------------------------------------------------------------
# 16.RANDOM FOREST - RANGER -  TRAINING
#------------------------------------------------------------

ini <- Sys.time()

set.seed(1992)
model_16_rf_2 <- randomForest(var,data = training, ntree = 2500, mtry = 3)


plot(model_16_rf_2)

predic_model_16_rf_2 <- predict(model_16_rf_2, newdata = testing,type = "prob")[,2]

fin <- Sys.time()

time_m16 <- fin - ini

summary(predic_model_16_rf_2)

tb16 <- table(testing$flag_pago,ifelse(predic_model_16_rf_2>0.5,"X1_Bueno","X0_Malo"))
tb16


m16_accuracy <- (640 + 1553)/nrow(testing)
m16_accuracy
#0.7510274

#score test data set

testing$m16_rf_2_score <- predict(model_16_rf_2, newdata = testing,type = "prob")[,2]

m16_pred <- prediction(testing$m16_rf_2_score, testing$flag_pago)
m16_perf <- performance(m16_pred,"tpr","fpr")

#ROC
plot(m16_perf, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision <- performance(m16_pred, measure = "prec", x.measure = "rec")
plot(m16_perf_precision, main="m16 RandomForest :Precision/recall curve")


# Plot accuracy as function of threshold
m16_perf_acc <- performance(m16_pred, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS <- round(max(attr(m16_perf,'y.values')[[1]]-attr(m16_perf,'x.values')[[1]])*100, 2)
m16_AUROC <- round(performance(m16_pred, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini <- (2*m16_AUROC - 100)

###########################################################################
cat("AUROC: ",m16_AUROC,"\tKS: ", m16_KS, "\tGini:", m16_Gini,"\tAccuracy:", m16_accuracy, "\n")


#------------------------------------------------------------------------
# 17. RANDOMFOREST - TUNEO DE PARAMETRO CON CARET - MAE - con training
#------------------------------------------------------------------------

ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)


set.seed(1992)
model_17_rf_tun_2 <- train(var,  data=training, method="ranger", 
                           trControl = trControl, metric = 'ROC',tuneLength = 13)


fin <- Sys.time()

time_m17 <- fin - ini

model_17_rf_tun_2

model_17_rf_tun_2$bestTune


model_17_rf_tun_2$finalModel



pred17 <- predict(model_17_rf_tun_2,newdata = testing, type = "prob")[,2]


#score test data set
testing$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=testing)[,2]
m17_pred <- prediction(testing$m17_score, testing$flag_pago)

table(ifelse(testing$m17_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
m17_accuracy <- (639+1515)/nrow(testing)
m17_accuracy
#0.7376712

m17_perf <- performance(m17_pred,"tpr","fpr")

#ROC
plot(m17_perf, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision <- performance(m17_pred, measure = "prec", x.measure = "rec")
plot(m17_perf_precision, main="m17 RandomForest:Precision/recall curve")


# Plot accuracy as function of threshold
m17_perf_acc <- performance(m17_pred, measure = "acc")
plot(m17_perf_acc, main="m17 RandomForest:Accuracy as function of threshold")

#KS, Gini & AUC m1
m17_KS <- round(max(attr(m17_perf,'y.values')[[1]]-attr(m17_perf,'x.values')[[1]])*100, 2)
m17_AUROC <- round(performance(m17_pred, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini <- (2*m17_AUROC - 100)

###########################################################################
cat("AUROC: ",m17_AUROC,"\tKS: ", m17_KS, "\tGini:", m17_Gini, "\tAccuracy:", m17_accuracy,"\n")


##############################################################################
#RandomForest
#
cat("AUROC: ",m14_AUROC,"\tKS: ", m14_KS, "\tGini:", m14_Gini, "\tAccuracy:", m14_accuracy,"\n")
cat("AUROC: ",m15_AUROC,"\tKS: ", m15_KS, "\tGini:", m15_Gini, "\tAccuracy:", m15_accuracy,"\n")
cat("AUROC: ",m16_AUROC,"\tKS: ", m16_KS, "\tGini:", m16_Gini,"\tAccuracy:", m16_accuracy, "\n")
cat("AUROC: ",m17_AUROC,"\tKS: ", m17_KS, "\tGini:", m17_Gini, "\tAccuracy:", m17_accuracy,"\n")


#Precaución

#Para datos que incluyen variables categóricas con diferente número de niveles, 
#los bosques aleatorios están sesgados a favor de aquellos atributos con más niveles. 
#Se pueden usar métodos tales como árboles de inferencia condicional para resolver el problema.

#------------------------------------------------------------
# 18. GBM (GRADIENTE BOOSTING MODEL) - TRAIN
# ------------------------------------------------------------

#nota_ Realizaré dos algoritmos boosting, GBM y Xgboost

library(gbm)


set.seed(1992)


ini <- Sys.time()

var_2 = flag_pago_val~Gtia_Soles_cat_f + Patrimonio_cat + ddatrMSF_ponderada_cat +  ddvnrMSF_ponderada_cat + PNor1MSF_mean_cat + MaxDmB_15_cat

#Target tiene que ser character o numerico, no trabaja con factor
set.seed(1992)

model_18_gbm_1 <- gbm(var,data = train,shrinkage=0.01, distribution = 'bernoulli',  n.trees=10000, verbose=F, interaction.depth = 3)

fin <- Sys.time()

time_m18 <- fin  - ini

#model_18_gbm_1 <- gbm(var_2,data = train,shrinkage=0.01, distribution = 'bernoulli',  n.trees=10000, verbose=F, interaction.depth = 3, train.fraction = round(nrow(train)*0.8) )

#importancia de variables
summary.gbm(model_18_gbm_1, plotit=TRUE)

summary.gbm(model_18_gbm_1)

gbm.perf(model_18_gbm_1)


for (i in 1:length(model_18_gbm_1$var.names)) {
  plot(model_18_gbm_1, i.var = i,
       ntrees = gbm.perf(model_18_gbm_1, plot.it = FALSE),
       type = "response")
}


predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18 <- table(test$flag_pago,ifelse(predic_model_18_gbm_1>0.5,"X1_Bueno","X0_Malo"))
tb18

m18_accuracy <- (622+1518)/nrow(test)
m18_accuracy

#0.7323751

#score test data set
test$m18_GBM_score <- predict(model_18_gbm_1, newdata = test,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred <- prediction(test$m18_GBM_score, test$flag_pago)
m18_perf <- performance(m18_pred,"tpr","fpr")

#ROC
plot(m18_perf, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision <- performance(m18_pred, measure = "prec", x.measure = "rec")
plot(m18_perf_precision, main="m18 GBM:Precision/recall curve")


# Plot accuracy as function of threshold
m18_perf_acc <- performance(m18_pred, measure = "acc")
plot(m18_perf_acc, main="m18 GBM:Accuracy as function of threshold")

#KS, Gini & AUC m1
m18_KS <- round(max(attr(m18_perf,'y.values')[[1]]-attr(m18_perf,'x.values')[[1]])*100, 2)
m18_AUROC <- round(performance(m18_pred, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini <- (2*m18_AUROC - 100)

###########################################################################
cat("AUROC: ",m18_AUROC,"\tKS: ", m18_KS, "\tGini:", m18_Gini, "\tAccuracy:", m18_accuracy,"\n")




#------------------------------------------------------------
# 19. GBM (GRADIENTE BOOSTING MODEL) - TRAIN - TUNEADO
# ------------------------------------------------------------

ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)


set.seed(1992)
model_19_GBM_tun_1 <- train(var,  data=train, method="gbm", 
                            trControl = trControl, metric = 'ROC',tuneLength = 13, verbose = F)

fin <- Sys.time()


(time_m19 <- fin - ini)


plot(model_19_GBM_tun_1)



model_19_GBM_tun_1$bestTune


model_19_GBM_tun_1


#model_rl_2_tun$finalModel$fitted.values

pred19 <-predict(model_19_GBM_tun_1, newdata = test,type = "prob")[,2]


#score test data set
test$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=test)[2]
m19_pred <- prediction(test$m19_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(test$m19_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
m19_accuracy <- (630+1508)/nrow(test)
m19_accuracy
#0.7316906

m19_perf <- performance(m19_pred,"tpr","fpr")

#ROC
plot(m19_perf, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision <- performance(m19_pred, measure = "prec", x.measure = "rec")
plot(m19_perf_precision, main="m19 GBM:Precision/recall curve")


# Plot accuracy as function of threshold
m19_perf_acc <- performance(m19_pred, measure = "acc")
plot(m19_perf_acc, main="m19 GBM:Accuracy as function of threshold")

#KS, Gini & AUC m1
m19_KS <- round(max(attr(m19_perf,'y.values')[[1]]-attr(m19_perf,'x.values')[[1]])*100, 2)
m19_AUROC <- round(performance(m19_pred, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini <- (2*m19_AUROC - 100)

###########################################################################
cat("AUROC: ",m19_AUROC,"\tKS: ", m19_KS, "\tGini:", m19_Gini, "\tAccuracy:", m19_accuracy,"\n")


#------------------------------------------------------------
# 20.GBM -  TRAINING
#------------------------------------------------------------



set.seed(1992)

var_2 = flag_pago_val~Gtia_Soles_cat_f + Patrimonio_cat + ddatrMSF_ponderada_cat +  ddvnrMSF_ponderada_cat + PNor1MSF_mean_cat + MaxDmB_15_cat



ini <- Sys.time()

model_20_gbm_2 <- gbm(var_2,data = training,shrinkage=0.01, distribution = 'adaboost',  n.trees=10000, verbose=F, interaction.depth = 8)

fin <- Sys.time()

time_m20 <- fin - ini


#importancia de variables
summary.gbm(model_20_gbm_2, plotit=TRUE)

summary.gbm(model_20_gbm_2)

gbm.perf(model_20_gbm_2)


for (i in 1:length(model_20_gbm_2$var.names)) {
  plot(model_20_gbm_2, i.var = i,
       ntrees = gbm.perf(model_20_gbm_2, plot.it = FALSE),
       type = "response")
}



predic_model_20_gbm_2 <- predict(model_20_gbm_2, newdata = testing,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2, n=30)


summary(predic_model_20_gbm_2)

tb20 <- table(testing$flag_pago,ifelse(predic_model_20_gbm_2>0.5,"X1_Bueno","X0_Malo"))
tb20


m20_accuracy <- (636 + 1514)/nrow(testing)
m20_accuracy
#0.7363014

#score test data set


testing$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = testing,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred <- prediction(testing$m20_GBM_2_score, testing$flag_pago)
m20_perf <- performance(m20_pred,"tpr","fpr")

#ROC
plot(m20_perf, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision <- performance(m20_pred, measure = "prec", x.measure = "rec")
plot(m20_perf_precision, main="m20 GBM :Precision/recall curve")


# Plot accuracy as function of threshold
m20_perf_acc <- performance(m20_pred, measure = "acc")
plot(m20_perf_acc, main="m20 GBM :Accuracy as function of threshold")

#KS, Gini & AUC m1
m20_KS <- round(max(attr(m20_perf,'y.values')[[1]]-attr(m20_perf,'x.values')[[1]])*100, 2)
m20_AUROC <- round(performance(m20_pred, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini <- (2*m20_AUROC - 100)

###########################################################################
cat("AUROC: ",m20_AUROC,"\tKS: ", m20_KS, "\tGini:", m20_Gini,"\tAccuracy:", m20_accuracy, "\n")


#------------------------------------------------------------------------
# 21. GBM - TUNEO DE PARAMETRO CON CARET - MAE - con training
#------------------------------------------------------------------------


trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)

ini <- Sys.time()

set.seed(1992)
model_21_rf_tun_2 <- train(var,  data=training, method="gbm", 
                           trControl = trControl, metric = 'ROC',tuneLength = 13, verbose = F)


fin <- Sys.time()

(time_m21 <- fin - ini)

model_21_rf_tun_2

model_21_rf_tun_2$bestTune


model_21_rf_tun_2$finalModel



pred21 <- predict(model_21_rf_tun_2,newdata = testing, type = "prob")[,2]


#score test data set
testing$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=testing)[,2]
m21_pred <- prediction(testing$m21_score, testing$flag_pago)

table(ifelse(testing$m21_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
m21_accuracy <- (638+1513)/nrow(testing)
m21_accuracy
#0.7366438

m21_perf <- performance(m21_pred,"tpr","fpr")

#ROC
plot(m21_perf, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision <- performance(m21_pred, measure = "prec", x.measure = "rec")
plot(m21_perf_precision, main="m21 GBM:Precision/recall curve")


# Plot accuracy as function of threshold
m21_perf_acc <- performance(m21_pred, measure = "acc")
plot(m21_perf_acc, main="m21 RandomForest:Accuracy as function of threshold")

#KS, Gini & AUC m1
m21_KS <- round(max(attr(m21_perf,'y.values')[[1]]-attr(m21_perf,'x.values')[[1]])*100, 2)
m21_AUROC <- round(performance(m21_pred, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini <- (2*m21_AUROC - 100)

###########################################################################
cat("AUROC: ",m21_AUROC,"\tKS: ", m21_KS, "\tGini:", m21_Gini, "\tAccuracy:", m21_accuracy,"\n")


##########################################################################
# GBM
##########################################################################


cat("AUROC: ",m18_AUROC,"\tKS: ", m18_KS, "\tGini:", m18_Gini, "\tAccuracy:", m18_accuracy,"\n")
cat("AUROC: ",m19_AUROC,"\tKS: ", m19_KS, "\tGini:", m19_Gini, "\tAccuracy:", m19_accuracy,"\n")
cat("AUROC: ",m20_AUROC,"\tKS: ", m20_KS, "\tGini:", m20_Gini,"\tAccuracy:", m20_accuracy, "\n")
cat("AUROC: ",m21_AUROC,"\tKS: ", m21_KS, "\tGini:", m21_Gini, "\tAccuracy:", m21_accuracy,"\n")


fin <- Sys.time()

timepo_parteuno <- fin - ini

#FIN PARTE uno


##########################################################################
# 22. Xgboost
##########################################################################

library(xgboost)
library(methods)
## Loading required package: methods
library(data.table)
## Loading required package: data.table
library(magrittr)
library(vcd)
library(Matrix)

nameLastCol <- names(train)[ncol(train)]
#para este tipo de modelo se van a tener que hacer algunas transformaciones en la data


#la data debe estar en una matriz y todoas las variables categoricas se convertir+an en dummies
#y debe estar en matriz


trainx <- data.table(train[,c(4:10)])


trainx <- sparse.model.matrix(flag_pago_val~., data = trainx)

trainy = train[,10] == "1"

#head(trainy)

testx <- data.table(test[,c(4:10)])


testx <- sparse.model.matrix(flag_pago_val~., data = testx)

testy = test[,10] == "1"

#head(testy)


#for(i in 1:ncol(trainx))

# {trainx[[i]] <- as.numeric(trainx[[i]])
#}

#trainx <- as.matrix(trainx)

#testx <- data.table(test[,10:16])


#for(i in 1:ncol(testx))

#{testx[[i]] <- as.numeric(testx[[i]])
#}

#testx <- as.matrix(testx)

#set.seed(1992)

#names(train)

#var_2 = flag_pago_val~ddatr1MSF_f+Var41_f+ddvnr1MSF_f+PrNo3MSF_f+Gtia2_f+SecEco_f+Patrimo_f





########
#se hace un cv

#para elegir el correcto cv.nround

ini <- Sys.time()

set.seed(1992)
param <- list("objective" = "binary:logistic",
              "eval_metric" = "auc")

cv.nround <- 40
cv.nfold <- 10

bst.cv <- xgb.cv(param=param, data = trainx, label = trainy, 
                 nfold = cv.nfold, nrounds = cv.nround)


set.seed(1992)
model_22_xgboost_1 <- xgboost(data = trainx, label = trainy, max_depth = 2, eta = 1, nthread = 2, nrounds = 20, objective = "binary:logistic", eval_metric = "auc")


fin <- Sys.time()

time_m22 <- fin - ini

#Impotancia de variables
importance_matrix <- xgb.importance(dimnames(trainx)[[2]],model = model_22_xgboost_1)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)


#ver como se compartan las reglas del arbol que he usado
xgb.dump(model_22_xgboost_1, with_stats = T)

#Gráfico del árbol
xgb.plot.tree(model = model_22_xgboost_1)


predic_model_22_xgboost_1 <- predict(model_22_xgboost_1, newdata = testx, type = "response")

head(predic_model_22_xgboost_1, n=30)


tb22 <- table(test$flag_pago,ifelse(predic_model_22_xgboost_1>0.5,"X1_Bueno","X0_Malo"))
tb22

m22_accuracy <- (637+1499)/nrow(test)
m22_accuracy

#0.7310062

#score test data set
test$m22_Xgboost_score <- predict(model_22_xgboost_1, newdata = testx, type = "response")

m22_pred <- prediction(test$m22_Xgboost_score, test$flag_pago)
m22_perf <- performance(m22_pred,"tpr","fpr")

#ROC
plot(m22_perf, lwd=2, colorize=TRUE, main="ROC m22: Xgboost Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m22_perf_precision <- performance(m22_pred, measure = "prec", x.measure = "rec")
plot(m22_perf_precision, main="m22 Xgboost:Precision/recall curve")


# Plot accuracy as function of threshold
m22_perf_acc <- performance(m22_pred, measure = "acc")
plot(m22_perf_acc, main="m22 Xgboost:Accuracy as function of threshold")

#KS, Gini & AUC m1
m22_KS <- round(max(attr(m22_perf,'y.values')[[1]]-attr(m22_perf,'x.values')[[1]])*100, 2)
m22_AUROC <- round(performance(m22_pred, measure = "auc")@y.values[[1]]*100, 2)
m22_Gini <- (2*m22_AUROC - 100)

###########################################################################
cat("AUROC: ",m22_AUROC,"\tKS: ", m22_KS, "\tGini:", m22_Gini, "\tAccuracy:", m22_accuracy,"\n")




#------------------------------------------------------------
# 23. Xgboost - TRAIN - TUNEADO
# ------------------------------------------------------------


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)



set.seed(1992)
model_23_Xgboost_tun_1 <- train(var,data = train, method="xgbTree", 
                                trControl = trControl, metric = 'ROC',tuneLength = 13, verbose = F)


fin <- Sys.time()


(time_m23 <- fin - ini)


model_23_Xgboost_tun_1

pred23 <-predict(model_23_Xgboost_tun_1, newdata = test,type = "prob")[,2]


#score test data set
test$m23_score <- predict(model_23_Xgboost_tun_1,type='prob',newdata=test)[2]
m23_pred <- prediction(test$m23_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(test$m23_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
m23_accuracy <- (626+1503)/nrow(test)
m23_accuracy
#0.7316906

m23_perf <- performance(m23_pred,"tpr","fpr")

#ROC
plot(m23_perf, lwd=2, colorize=TRUE, main="ROC m23: Xgboost - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m23_perf_precision <- performance(m23_pred, measure = "prec", x.measure = "rec")
plot(m23_perf_precision, main="m23 Xgboost:Precision/recall curve")


# Plot accuracy as function of threshold
m23_perf_acc <- performance(m23_pred, measure = "acc")
plot(m23_perf_acc, main="m23 Xgboost:Accuracy as function of threshold")

#KS, Gini & AUC m1
m23_KS <- round(max(attr(m23_perf,'y.values')[[1]]-attr(m23_perf,'x.values')[[1]])*100, 2)
m23_AUROC <- round(performance(m23_pred, measure = "auc")@y.values[[1]]*100, 2)
m23_Gini <- (2*m23_AUROC - 100)

###########################################################################
cat("AUROC: ",m23_AUROC,"\tKS: ", m23_KS, "\tGini:", m23_Gini, "\tAccuracy:", m23_accuracy,"\n")


#------------------------------------------------------------
# 24.Xgboost -  TRAINING
#------------------------------------------------------------


trainingx <- data.table(training[,c(4:10)])


trainingx <- sparse.model.matrix(flag_pago_val~., data = trainingx)

trainingy = training[,10] == "1"

head(trainingy)

testingx <- data.table(testing[,c(4:10)])


testingx <- sparse.model.matrix(flag_pago_val~., data = testingx)

testingy = testing[,10] == "1"

head(testingy)


ini <- Sys.time()

set.seed(1992)
model_24_xgboost_2 <- xgboost(data = trainingx, label = trainingy, max_depth = 2, eta = 1, nthread = 2, nrounds = 1000, objective = "binary:logistic", eval_metric = "auc")


fin <- Sys.time()

time_m24 <- fin - ini

#Impotancia de variables
importance_matrix <- xgb.importance(dimnames(trainx)[[2]],model = model_24_xgboost_2)
print(importance_matrix)
xgb.plot.importance(importance_matrix = importance_matrix)


#ver como se compartan las reglas del arbol que he usado
xgb.dump(model_24_xgboost_2, with_stats = T)

#Gráfico del árbol
xgb.plot.tree(model = model_24_xgboost_2)


predic_model_24_xgboost_2 <- predict(model_24_xgboost_2, newdata = testingx, type = "response")

head(predic_model_24_xgboost_2, n=30)


summary(predic_model_24_xgboost_2)

tb24 <- table(testing$flag_pago,ifelse(predic_model_24_xgboost_2>0.5,"X1_Bueno","X0_Malo"))
tb24


m24_accuracy <- (634 + 1501)/nrow(testing)
m24_accuracy
#0.7311644

#score test data set


testing$m24_Xgboost_2_score <- predict(model_24_xgboost_2, newdata = testingx, type = "response")



m24_pred <- prediction(testing$m24_Xgboost_2_score, testing$flag_pago)
m24_perf <- performance(m24_pred,"tpr","fpr")

#ROC
plot(m24_perf, lwd=2, colorize=TRUE, main="ROC m24: Xgboost - data testing")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m24_perf_precision <- performance(m24_pred, measure = "prec", x.measure = "rec")
plot(m24_perf_precision, main="m24 Xgboost :Precision/recall curve")


# Plot accuracy as function of threshold
m24_perf_acc <- performance(m24_pred, measure = "acc")
plot(m24_perf_acc, main="m24 Xgboost :Accuracy as function of threshold")

#KS, Gini & AUC m1
m24_KS <- round(max(attr(m24_perf,'y.values')[[1]]-attr(m24_perf,'x.values')[[1]])*100, 2)
m24_AUROC <- round(performance(m24_pred, measure = "auc")@y.values[[1]]*100, 2)
m24_Gini <- (2*m24_AUROC - 100)

###########################################################################
cat("AUROC: ",m24_AUROC,"\tKS: ", m24_KS, "\tGini:", m24_Gini,"\tAccuracy:", m24_accuracy, "\n")


#------------------------------------------------------------------------
# 25. Xgboost - TUNEO DE PARAMETRO - MAE - con training
#------------------------------------------------------------------------


trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)

ini <- Sys.time()

set.seed(1992)
model_25_xgboost_tun_2 <- train(var,  data=training, method="xgbTree", 
                                trControl = trControl, metric = 'ROC',tuneLength = 13, verbose = F)


fin <- Sys.time()

(time_m25 <- fin - ini)

model_25_xgboost_tun_2

model_25_xgboost_tun_2$bestTune


model_25_xgboost_tun_2$finalModel



pred25 <- predict(model_25_xgboost_tun_2,newdata = testing, type = "prob")[,2]


#score test data set
testing$m25_score <- predict(model_25_xgboost_tun_2,type='prob',newdata=testing)[,2]
m25_pred <- prediction(testing$m25_score, testing$flag_pago)

table(ifelse(testing$m25_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
m25_accuracy <- (615+1530)/nrow(testing)
m25_accuracy
#0.734589

m25_perf <- performance(m25_pred,"tpr","fpr")

#ROC
plot(m25_perf, lwd=2, colorize=TRUE, main="ROC m25: Xgboost Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m25_perf_precision <- performance(m25_pred, measure = "prec", x.measure = "rec")
plot(m25_perf_precision, main="m25 Xgboost:Precision/recall curve")


# Plot accuracy as function of threshold
m25_perf_acc <- performance(m25_pred, measure = "acc")
plot(m25_perf_acc, main="m25 Xgboost:Accuracy as function of threshold")

#KS, Gini & AUC m1
m25_KS <- round(max(attr(m25_perf,'y.values')[[1]]-attr(m25_perf,'x.values')[[1]])*100, 2)
m25_AUROC <- round(performance(m25_pred, measure = "auc")@y.values[[1]]*100, 2)
m25_Gini <- (2*m25_AUROC - 100)

###########################################################################
cat("AUROC: ",m25_AUROC,"\tKS: ", m25_KS, "\tGini:", m25_Gini, "\tAccuracy:", m25_accuracy,"\n")


##########################################################################
# Xgboost
##########################################################################


cat("AUROC: ",m22_AUROC,"\tKS: ", m22_KS, "\tGini:", m22_Gini, "\tAccuracy:", m22_accuracy,"\n")
cat("AUROC: ",m23_AUROC,"\tKS: ", m23_KS, "\tGini:", m23_Gini, "\tAccuracy:", m23_accuracy,"\n")
cat("AUROC: ",m24_AUROC,"\tKS: ", m24_KS, "\tGini:", m24_Gini,"\tAccuracy:", m24_accuracy, "\n")
cat("AUROC: ",m25_AUROC,"\tKS: ", m25_KS, "\tGini:", m25_Gini, "\tAccuracy:", m25_accuracy,"\n")




#------------------------------------------------------------
# 26. RANDOM FOREST - TRAIN - TUNEADO -  ctree
# ------------------------------------------------------------

library(party)

ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)


library(ranger)

set.seed(1992)
model_26_RF_ctree_tun_1 <- train(var,  data=train, method="cforest", 
                                 trControl = trControl, metric = 'ROC',tuneLength = 13)

fin <- Sys.time()

(time_m26 <- fin - ini)


model_26_RF_ctree_tun_1$bestTune


model_26_RF_ctree_tun_1



pred26 <-predict(model_26_RF_ctree_tun_1, newdata = test,type = "prob")[,2]


#score test data set
test$m26_score <- predict(model_26_RF_ctree_tun_1,type='prob',newdata=test)[2]
m26_pred <- prediction(test$m26_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(test$m26_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
m26_accuracy <- (639+ 1488)/nrow(test)
m26_accuracy
#0.7279261

m26_perf <- performance(m26_pred,"tpr","fpr")

#ROC
plot(m26_perf, lwd=2, colorize=TRUE, main="ROC m26: RandomForest - ctree - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m26_perf_precision <- performance(m26_pred, measure = "prec", x.measure = "rec")
plot(m26_perf_precision, main="m26 RandomForest - ctree:Precision/recall curve")


# Plot accuracy as function of threshold
m26_perf_acc <- performance(m26_pred, measure = "acc")
plot(m26_perf_acc, main="m26 RandomForest - ctree:Accuracy as function of threshold")

#KS, Gini & AUC m1
m26_KS <- round(max(attr(m26_perf,'y.values')[[1]]-attr(m26_perf,'x.values')[[1]])*100, 2)
m26_AUROC <- round(performance(m26_pred, measure = "auc")@y.values[[1]]*100, 2)
m26_Gini <- (2*m26_AUROC - 100)

###########################################################################
cat("AUROC: ",m26_AUROC,"\tKS: ", m26_KS, "\tGini:", m26_Gini, "\tAccuracy:", m26_accuracy,"\n")



#------------------------------------------------------------------------
# 27. RANDOMFOREST - TUNEO DE PARAMETRO CON CARET - MAE - con training
#------------------------------------------------------------------------


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)


set.seed(1992)
model_27_rf_ctree_tun_2 <- train(var,  data=training, method="cforest", 
                                 trControl = trControl, metric = 'ROC',tuneLength = 13)

fin <- Sys.time()

time_m27 <- fin - ini

model_27_rf_ctree_tun_2

model_27_rf_ctree_tun_2$bestTune


model_27_rf_ctree_tun_2$finalModel



pred27 <- predict(model_27_rf_ctree_tun_2,newdata = testing, type = "prob")[,2]


#score test data set
testing$m27_score <- predict(model_27_rf_ctree_tun_2,type='prob',newdata=testing)[,2]
m27_pred <- prediction(testing$m27_score, testing$flag_pago)

table(ifelse(testing$m27_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
m27_accuracy <- (395+1677)/nrow(testing)
m27_accuracy
#0.709589

m27_perf <- performance(m27_pred,"tpr","fpr")

#ROC
plot(m27_perf, lwd=2, colorize=TRUE, main="ROC m27: RandomForest Ctree Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m27_perf_precision <- performance(m27_pred, measure = "prec", x.measure = "rec")
plot(m27_perf_precision, main="m27 RandomForest ctree :Precision/recall curve")


# Plot accuracy as function of threshold
m27_perf_acc <- performance(m27_pred, measure = "acc")
plot(m27_perf_acc, main="m27 RandomForest ctree :Accuracy as function of threshold")

#KS, Gini & AUC m1
m27_KS <- round(max(attr(m27_perf,'y.values')[[1]]-attr(m27_perf,'x.values')[[1]])*100, 2)
m27_AUROC <- round(performance(m27_pred, measure = "auc")@y.values[[1]]*100, 2)
m27_Gini <- (2*m27_AUROC - 100)

###########################################################################
cat("AUROC: ",m27_AUROC,"\tKS: ", m27_KS, "\tGini:", m27_Gini, "\tAccuracy:", m27_accuracy,"\n")


##############################################################################
#RandomForest - ctree
#
cat("AUROC: ",m26_AUROC,"\tKS: ", m26_KS, "\tGini:", m26_Gini, "\tAccuracy:", m26_accuracy,"\n")
cat("AUROC: ",m27_AUROC,"\tKS: ", m27_KS, "\tGini:", m27_Gini, "\tAccuracy:", m27_accuracy,"\n")


#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE IV          SVM
#---------------------------------------------------------------------------------------
########################################################################################


#------------------------------------------------------------
# 28. SVM - lineal - TRAIN
# ------------------------------------------------------------

#se usará el paquete randomForest

#no permite tener valores missing

library(e1071)

train.svm <- train[,3:9]
test.svm <- test[,3:9]


set.seed(1992)
model_28_svm_l_1 <- svm(var,data = train.svm, kernel="linear",probability=TRUE)

ini <- Sys.time()

set.seed(1992)
#model_28_svm_l_1 <- tune.svm(var, data = data.svm, gamma = 10^(-6:-1), cost = 10^(1:2))

fin <- Sys.time()

time_m28 <- fin - ini


#poner los parámetros correctos

#model_28_svm_l_1 <- svm(var,data = data.svm, kernel="linear",probability=TRUE)


summary(model_28_svm_l_1)

predic_model_28_svm_l_1 <- predict(model_28_svm_l_1, newdata = test.svm,probability=TRUE)

predic_model_28_svm_l_1_pr <- attr(predic_model_28_svm_l_1, "probabilities")[,1]

summary(predic_model_28_svm_l_1)



tb28 <- table(test$flag_pago,predic_model_28_svm_l_1)
tb28

m28_accuracy <- (729 + 1316)/nrow(test)
m28_accuracy
#0.6998631

#score test data set


test$m28_svm_l_score <- attr(predict(model_28_svm_l_1, newdata = test.svm,probability=TRUE),"probabilities")[,1]





m28_pred <- prediction(test$m28_svm_l_score, test$flag_pago)
m28_perf <- performance(m28_pred,"tpr","fpr")

#ROC
plot(m28_perf, lwd=2, colorize=TRUE, main="ROC m28: SVM - Lineal Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m28_perf_precision <- performance(m28_pred, measure = "prec", x.measure = "rec")
plot(m28_perf_precision, main="m28 SVm lineal :Precision/recall curve")


# Plot accuracy as function of threshold
m28_perf_acc <- performance(m28_pred, measure = "acc")
plot(m28_perf_acc, main="m28 SVM lineal :Accuracy as function of threshold")

#KS, Gini & AUC m1
m28_KS <- round(max(attr(m28_perf,'y.values')[[1]]-attr(m28_perf,'x.values')[[1]])*100, 2)
m28_AUROC <- round(performance(m28_pred, measure = "auc")@y.values[[1]]*100, 2)
m28_Gini <- (2*m28_AUROC - 100)

###########################################################################
cat("AUROC: ",m28_AUROC,"\tKS: ", m28_KS, "\tGini:", m28_Gini, "\tAccuracy:", m28_accuracy,"\n")


#------------------------------------------------------------
# 29. SVM - Lineal - TRAIN - TUNEADO
# ------------------------------------------------------------

library(kernlab)


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)



set.seed(1992)
model_29_svm_l_tun_1 <- train(var,  data=train, method="svmLinear", 
                              trControl = trControl, metric = 'ROC',tuneLength = 13)


fin <- Sys.time()

time_m29 <- fin - ini

model_29_svm_l_tun_1$bestTune


model_29_svm_l_tun_1


#model_rl_2_tun$finalModel$fitted.values

pred29 <-predict(model_29_svm_l_tun_1, newdata = test,type = "prob")[,2]


#score test data set
test$m29_score <- predict(model_29_svm_l_tun_1,type='prob',newdata=test)[2]
m29_pred <- prediction(test$m29_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(test$m29_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
m29_accuracy <- (729+1316)/nrow(test)
m29_accuracy
#0.6998631

m29_perf <- performance(m29_pred,"tpr","fpr")

#ROC
plot(m29_perf, lwd=2, colorize=TRUE, main="ROC m29: SVM - Lineal - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m29_perf_precision <- performance(m29_pred, measure = "prec", x.measure = "rec")
plot(m29_perf_precision, main="m29 SVM Lineal:Precision/recall curve")


# Plot accuracy as function of threshold
m29_perf_acc <- performance(m29_pred, measure = "acc")
plot(m29_perf_acc, main="m29 SVM - Lineal:Accuracy as function of threshold")

#KS, Gini & AUC m1
m29_KS <- round(max(attr(m29_perf,'y.values')[[1]]-attr(m29_perf,'x.values')[[1]])*100, 2)
m29_AUROC <- round(performance(m29_pred, measure = "auc")@y.values[[1]]*100, 2)
m29_Gini <- (2*m29_AUROC - 100)

###########################################################################
cat("AUROC: ",m29_AUROC,"\tKS: ", m29_KS, "\tGini:", m29_Gini, "\tAccuracy:", m29_accuracy,"\n")


#------------------------------------------------------------
# 30.SVM - LINEAL -  TRAINING
#------------------------------------------------------------

set.seed(1992)

training.svm <- training[,3:9]
testing.svm <- testing[,3:9]

ini <- Sys.time()

set.seed(1992)
model_30_svm_l_2 <- svm(var,data = training.svm, kernel="linear",probability=TRUE)



#set.seed(1992)
#model_28_svm_l_1 <- tune.svm(var, data = data.svm, gamma = 10^(-6:-1), cost = 10^(1:2))

fin <- Sys.time()

time_m30 <- fin - ini

summary(model_30_svm_l_2)

predic_model_30_svm_l_1 <- predict(model_30_svm_l_2, newdata = testing.svm,probability=TRUE)

predic_model_30_svm_l_1_pr <- attr(predic_model_30_svm_l_1, "probabilities")[,1]

summary(predic_model_30_svm_l_1)



tb30 <- table(testing$flag_pago,predic_model_30_svm_l_1)
tb30

m30_accuracy <- (725 + 1332)/nrow(test)
m30_accuracy
#0.7039699



#score test data set


testing$m30_svm_l_score <- attr(predict(model_30_svm_l_2, newdata = testing.svm,probability=TRUE),"probabilities")[,1]


m30_pred <- prediction(testing$m30_svm_l_score, testing$flag_pago)
m30_perf <- performance(m30_pred,"tpr","fpr")

#ROC
plot(m30_perf, lwd=2, colorize=TRUE, main="ROC m30: SVM - Lineal - data testing")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m30_perf_precision <- performance(m30_pred, measure = "prec", x.measure = "rec")
plot(m30_perf_precision, main="m30 SVM - Lineal :Precision/recall curve")


# Plot accuracy as function of threshold
m30_perf_acc <- performance(m30_pred, measure = "acc")
plot(m30_perf_acc, main="m30 SVM - Lineal :Accuracy as function of threshold")

#KS, Gini & AUC m1
m30_KS <- round(max(attr(m30_perf,'y.values')[[1]]-attr(m30_perf,'x.values')[[1]])*100, 2)
m30_AUROC <- round(performance(m30_pred, measure = "auc")@y.values[[1]]*100, 2)
m30_Gini <- (2*m30_AUROC - 100)

###########################################################################
cat("AUROC: ",m30_AUROC,"\tKS: ", m30_KS, "\tGini:", m30_Gini,"\tAccuracy:", m30_accuracy, "\n")


#------------------------------------------------------------------------
# 31. SVM - LINEAL - MAE - con training
#------------------------------------------------------------------------


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)


set.seed(1992)
model_31_svm_l_tun_2 <- train(var,  data=training, method="svmLinear", 
                              trControl = trControl, metric = 'ROC',tuneLength = 13)


fin <- Sys.time()

(time_m31 <- fin - ini)

model_31_svm_l_tun_2

model_31_svm_l_tun_2$bestTune


model_31_svm_l_tun_2$finalModel



pred31 <- predict(model_31_svm_l_tun_2,newdata = testing, type = "prob")[,2]


#score test data set
testing$m31_score <- predict(model_31_svm_l_tun_2,type='prob',newdata=testing)[,2]
m31_pred <- prediction(testing$m31_score, testing$flag_pago)

table(ifelse(testing$m31_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
m31_accuracy <- (725+1332)/nrow(testing)
m31_accuracy
#0.7044521

m31_perf <- performance(m31_pred,"tpr","fpr")

#ROC
plot(m31_perf, lwd=2, colorize=TRUE, main="ROC m31: SVM - Lineal - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m31_perf_precision <- performance(m31_pred, measure = "prec", x.measure = "rec")
plot(m31_perf_precision, main="m31 SVM - Lineal:Precision/recall curve")


# Plot accuracy as function of threshold
m31_perf_acc <- performance(m31_pred, measure = "acc")
plot(m31_perf_acc, main="m31 SVM - Lineal:Accuracy as function of threshold")

#KS, Gini & AUC m1
m31_KS <- round(max(attr(m31_perf,'y.values')[[1]]-attr(m31_perf,'x.values')[[1]])*100, 2)
m31_AUROC <- round(performance(m31_pred, measure = "auc")@y.values[[1]]*100, 2)
m31_Gini <- (2*m31_AUROC - 100)

###########################################################################
cat("AUROC: ",m31_AUROC,"\tKS: ", m31_KS, "\tGini:", m31_Gini, "\tAccuracy:", m31_accuracy,"\n")


##############################################################################
#SVM - Lineal
#
cat("AUROC: ",m28_AUROC,"\tKS: ", m28_KS, "\tGini:", m28_Gini, "\tAccuracy:", m28_accuracy,"\n")
cat("AUROC: ",m29_AUROC,"\tKS: ", m29_KS, "\tGini:", m29_Gini, "\tAccuracy:", m29_accuracy,"\n")
cat("AUROC: ",m30_AUROC,"\tKS: ", m30_KS, "\tGini:", m30_Gini,"\tAccuracy:", m30_accuracy, "\n")
cat("AUROC: ",m31_AUROC,"\tKS: ", m31_KS, "\tGini:", m31_Gini, "\tAccuracy:", m31_accuracy,"\n")




#------------------------------------------------------------
# 32. SVM - Polinomial - TRAIN
# ------------------------------------------------------------

#se usará el paquete randomForest

#no permite tener valores missing



train.svm <- train[,3:9]
test.svm <- test[,3:9]



ini <- Sys.time()

set.seed(1992)
model_32_svm_p_1 <- svm(var,data = train.svm, kernel="polynomial",probability=TRUE)

#model_28_svm_l_1 <- tune.svm(var, data = data.svm, gamma = 10^(-6:-1), cost = 10^(1:2))

fin <- Sys.time()

(time_m32 <- fin - ini)


#poner los parámetros correctos

#model_28_svm_l_1 <- svm(var,data = data.svm, kernel="linear",probability=TRUE)


summary(model_32_svm_p_1)

predic_model_32_svm_p_1 <- predict(model_32_svm_p_1, newdata = test.svm,probability=TRUE)

#predic_model_28_svm_l_1_pr <- attr(predic_model_28_svm_l_1, "probabilities")[,1]

summary(predic_model_32_svm_p_1)



tb32 <- table(test$flag_pago,predic_model_32_svm_p_1)
tb32

m32_accuracy <- (590 + 1520)/nrow(test)
m32_accuracy
#0.7221081

#score test data set


test$m32_svm_p_score <- attr(predict(model_32_svm_p_1, newdata = test.svm,probability=TRUE),"probabilities")[,1]



m32_pred <- prediction(test$m32_svm_p_score, test$flag_pago)
m32_perf <- performance(m32_pred,"tpr","fpr")

#ROC
plot(m32_perf, lwd=2, colorize=TRUE, main="ROC m32: SVM - Polinomial Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m32_perf_precision <- performance(m32_pred, measure = "prec", x.measure = "rec")
plot(m32_perf_precision, main="m32 SVm Polinomial :Precision/recall curve")


# Plot accuracy as function of threshold
m32_perf_acc <- performance(m32_pred, measure = "acc")
plot(m32_perf_acc, main="m32 SVM Polinomial :Accuracy as function of threshold")

#KS, Gini & AUC m1
m32_KS <- round(max(attr(m32_perf,'y.values')[[1]]-attr(m32_perf,'x.values')[[1]])*100, 2)
m32_AUROC <- round(performance(m32_pred, measure = "auc")@y.values[[1]]*100, 2)
m32_Gini <- (2*m32_AUROC - 100)

###########################################################################
cat("AUROC: ",m32_AUROC,"\tKS: ", m32_KS, "\tGini:", m32_Gini, "\tAccuracy:", m32_accuracy,"\n")


#------------------------------------------------------------
# 33. SVM - Polinomial - TRAIN - TUNEADO
# ------------------------------------------------------------

#library(kernlab)


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)



set.seed(1992)
model_33_svm_p_tun_1 <- train(var,  data=train, method="svmPoly", 
                              trControl = trControl, metric = 'ROC',tuneLength = 13)


fin <- Sys.time()

(time_m33 <- fin - ini)

model_33_svm_p_tun_1$bestTune


model_33_svm_p_tun_1


#model_rl_2_tun$finalModel$fitted.values

pred33 <-predict(model_33_svm_p_tun_1, newdata = test,type = "prob")[,2]


#score test data set
test$m33_score <- predict(model_33_svm_p_tun_1,type='prob',newdata=test)[2]
m33_pred <- prediction(test$m33_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(test$m33_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
m33_accuracy <- (516+1575)/nrow(test)
m33_accuracy
#0.7156057

m33_perf <- performance(m33_pred,"tpr","fpr")

#ROC
plot(m33_perf, lwd=2, colorize=TRUE, main="ROC m33: SVM - Polinomial - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m33_perf_precision <- performance(m33_pred, measure = "prec", x.measure = "rec")
plot(m33_perf_precision, main="m33 SVM Polinomial:Precision/recall curve")


# Plot accuracy as function of threshold
m33_perf_acc <- performance(m33_pred, measure = "acc")
plot(m33_perf_acc, main="m33 SVM - Polinomial:Accuracy as function of threshold")

#KS, Gini & AUC m1
m33_KS <- round(max(attr(m33_perf,'y.values')[[1]]-attr(m33_perf,'x.values')[[1]])*100, 2)
m33_AUROC <- round(performance(m33_pred, measure = "auc")@y.values[[1]]*100, 2)
m33_Gini <- (2*m33_AUROC - 100)

###########################################################################
cat("AUROC: ",m33_AUROC,"\tKS: ", m33_KS, "\tGini:", m33_Gini, "\tAccuracy:", m33_accuracy,"\n")


#------------------------------------------------------------
# 34.SVM - Polinomial -  TRAINING
#------------------------------------------------------------

set.seed(1992)

training.svm <- training[,3:9]
testing.svm <- testing[,3:9]

ini <- Sys.time()

set.seed(1992)
model_34_svm_l_2 <- svm(var,data = training.svm, kernel="polynomial",probability=TRUE)



#set.seed(1992)
#model_28_svm_l_1 <- tune.svm(var, data = data.svm, gamma = 10^(-6:-1), cost = 10^(1:2))

fin <- Sys.time()

(time_m34 <- fin - ini)

summary(model_34_svm_l_2)

predic_model_34_svm_p_1 <- predict(model_34_svm_l_2, newdata = testing.svm,probability=TRUE)

#predic_model_30_svm_l_1_pr <- attr(predic_model_30_svm_l_1, "probabilities")[,1]

summary(predic_model_34_svm_p_1)



tb34 <- table(testing$flag_pago,predic_model_34_svm_p_1)
tb34

m34_accuracy <- (610 + 1520)/nrow(test)
m34_accuracy
#0.7289528



#score test data set


testing$m34_svm_p_score <- attr(predict(model_34_svm_l_2, newdata = testing.svm,probability=TRUE),"probabilities")[,1]


m34_pred <- prediction(testing$m34_svm_p_score, testing$flag_pago)
m34_perf <- performance(m30_pred,"tpr","fpr")

#ROC
plot(m34_perf, lwd=2, colorize=TRUE, main="ROC m34: SVM - Polinomial - data testing")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m34_perf_precision <- performance(m34_pred, measure = "prec", x.measure = "rec")
plot(m34_perf_precision, main="m34 SVM - Polinomial :Precision/recall curve")


# Plot accuracy as function of threshold
m34_perf_acc <- performance(m34_pred, measure = "acc")
plot(m34_perf_acc, main="m34 SVM - Polinomial :Accuracy as function of threshold")

#KS, Gini & AUC m1
m34_KS <- round(max(attr(m34_perf,'y.values')[[1]]-attr(m34_perf,'x.values')[[1]])*100, 2)
m34_AUROC <- round(performance(m34_pred, measure = "auc")@y.values[[1]]*100, 2)
m34_Gini <- (2*m34_AUROC - 100)

###########################################################################
cat("AUROC: ",m34_AUROC,"\tKS: ", m34_KS, "\tGini:", m34_Gini,"\tAccuracy:", m34_accuracy, "\n")


#------------------------------------------------------------------------
# 35. SVM - Polinomial - MAE - con training
#------------------------------------------------------------------------


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)


set.seed(1992)
model_35_svm_p_tun_2 <- train(var,  data=training, method="svmPoly", 
                              trControl = trControl, metric = 'ROC',tuneLength = 13)


fin <- Sys.time()

(time_35 <- fin - ini)

model_35_svm_p_tun_2

model_35_svm_p_tun_2$bestTune


model_35_svm_p_tun_2$finalModel



pred35 <- predict(model_35_svm_p_tun_2,newdata = testing, type = "prob")[,2]


#score test data set
testing$m35_score <- predict(model_35_svm_p_tun_2,type='prob',newdata=testing)[,2]
m35_pred <- prediction(testing$m35_score, testing$flag_pago)

table(ifelse(testing$m35_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
m35_accuracy <- (725+1332)/nrow(testing)
m35_accuracy
#0.7044521

m35_perf <- performance(m35_pred,"tpr","fpr")

#ROC
plot(m35_perf, lwd=2, colorize=TRUE, main="ROC m35: SVM - Polinomial - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m35_perf_precision <- performance(m35_pred, measure = "prec", x.measure = "rec")
plot(m35_perf_precision, main="m35 SVM - Polinomial:Precision/recall curve")


# Plot accuracy as function of threshold
m35_perf_acc <- performance(m35_pred, measure = "acc")
plot(m35_perf_acc, main="m35 SVM - Polinomial:Accuracy as function of threshold")

#KS, Gini & AUC m1
m35_KS <- round(max(attr(m35_perf,'y.values')[[1]]-attr(m35_perf,'x.values')[[1]])*100, 2)
m35_AUROC <- round(performance(m35_pred, measure = "auc")@y.values[[1]]*100, 2)
m35_Gini <- (2*m35_AUROC - 100)

###########################################################################
cat("AUROC: ",m35_AUROC,"\tKS: ", m35_KS, "\tGini:", m35_Gini, "\tAccuracy:", m35_accuracy,"\n")


##############################################################################
#SVM - Polinomial
#
cat("AUROC: ",m32_AUROC,"\tKS: ", m32_KS, "\tGini:", m32_Gini, "\tAccuracy:", m32_accuracy,"\n")
cat("AUROC: ",m33_AUROC,"\tKS: ", m33_KS, "\tGini:", m33_Gini, "\tAccuracy:", m33_accuracy,"\n")
cat("AUROC: ",m34_AUROC,"\tKS: ", m34_KS, "\tGini:", m34_Gini,"\tAccuracy:", m34_accuracy, "\n")
cat("AUROC: ",m35_AUROC,"\tKS: ", m35_KS, "\tGini:", m35_Gini, "\tAccuracy:", m35_accuracy,"\n")




#------------------------------------------------------------
# 36. SVM - Radial - TRAIN
# ------------------------------------------------------------

#se usará el paquete randomForest

#no permite tener valores missing



train.svm <- train[,3:9]
test.svm <- test[,3:9]



ini <- Sys.time()

set.seed(1992)
model_36_svm_r_1 <- svm(var,data = train.svm, kernel="radial",probability=TRUE)

#model_28_svm_l_1 <- tune.svm(var, data = data.svm, gamma = 10^(-6:-1), cost = 10^(1:2))

fin <- Sys.time()

(time_m36 <- fin - ini)


#poner los parámetros correctos

#model_28_svm_l_1 <- svm(var,data = data.svm, kernel="linear",probability=TRUE)


summary(model_36_svm_r_1)

predic_model_36_svm_r_1 <- predict(model_36_svm_r_1, newdata = test.svm,probability=TRUE)

#predic_model_28_svm_l_1_pr <- attr(predic_model_28_svm_l_1, "probabilities")[,1]

summary(predic_model_36_svm_r_1)



tb36 <- table(test$flag_pago,predic_model_36_svm_r_1)
tb36

m36_accuracy <- (617 + 1511)/nrow(test)
m36_accuracy
#0.7282683

#score test data set


test$m36_svm_r_score <- attr(predict(model_36_svm_r_1, newdata = test.svm,probability=TRUE),"probabilities")[,1]



m36_pred <- prediction(test$m36_svm_r_score, test$flag_pago)
m36_perf <- performance(m36_pred,"tpr","fpr")

#ROC
plot(m36_perf, lwd=2, colorize=TRUE, main="ROC m36: SVM - Radial Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m36_perf_precision <- performance(m36_pred, measure = "prec", x.measure = "rec")
plot(m36_perf_precision, main="m36 SVm Radial :Precision/recall curve")


# Plot accuracy as function of threshold
m36_perf_acc <- performance(m36_pred, measure = "acc")
plot(m36_perf_acc, main="m36 SVM Radial :Accuracy as function of threshold")

#KS, Gini & AUC m1
m36_KS <- round(max(attr(m36_perf,'y.values')[[1]]-attr(m36_perf,'x.values')[[1]])*100, 2)
m36_AUROC <- round(performance(m36_pred, measure = "auc")@y.values[[1]]*100, 2)
m36_Gini <- (2*m36_AUROC - 100)

###########################################################################
cat("AUROC: ",m36_AUROC,"\tKS: ", m36_KS, "\tGini:", m36_Gini, "\tAccuracy:", m36_accuracy,"\n")


#------------------------------------------------------------
# 37. SVM - Lineal - TRAIN - TUNEADO
# ------------------------------------------------------------

#library(kernlab)


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 10, number = 10, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)



set.seed(1992)
model_37_svm_r_tun_1 <- train(var,  data=train, method="svmRadial", 
                              trControl = trControl, metric = 'ROC',tuneLength = 13)


fin <- Sys.time()

(time_m37 <- fin - ini)

model_37_svm_r_tun_1$bestTune


model_37_svm_r_tun_1


#model_rl_2_tun$finalModel$fitted.values

pred37 <-predict(model_37_svm_r_tun_1, newdata = test,type = "prob")[,2]


#score test data set
test$m37_score <- predict(model_37_svm_r_tun_1,type='prob',newdata=test)[2]
m37_pred <- prediction(test$m37_score, test$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(test$m37_score>0.5,"X1_Bueno","X0_Malo"), test$flag_pago)
m37_accuracy <- (547+1579)/nrow(test)
m37_accuracy
#0.7275838

m37_perf <- performance(m37_pred,"tpr","fpr")

#ROC
plot(m37_perf, lwd=2, colorize=TRUE, main="ROC m37: SVM - Radial - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m37_perf_precision <- performance(m37_pred, measure = "prec", x.measure = "rec")
plot(m37_perf_precision, main="m37 SVM Radial:Precision/recall curve")


# Plot accuracy as function of threshold
m37_perf_acc <- performance(m37_pred, measure = "acc")
plot(m37_perf_acc, main="m37 SVM - Radial:Accuracy as function of threshold")

#KS, Gini & AUC m1
m37_KS <- round(max(attr(m37_perf,'y.values')[[1]]-attr(m37_perf,'x.values')[[1]])*100, 2)
m37_AUROC <- round(performance(m37_pred, measure = "auc")@y.values[[1]]*100, 2)
m37_Gini <- (2*m37_AUROC - 100)

###########################################################################
cat("AUROC: ",m37_AUROC,"\tKS: ", m37_KS, "\tGini:", m37_Gini, "\tAccuracy:", m37_accuracy,"\n")


#------------------------------------------------------------
# 38.SVM - Radial -  TRAINING
#------------------------------------------------------------

set.seed(1992)

training.svm <- training[,3:9]
testing.svm <- testing[,3:9]

ini <- Sys.time()

set.seed(1992)
model_38_svm_r_2 <- svm(var,data = training.svm, kernel="radial",probability=TRUE)



#set.seed(1992)
#model_28_svm_l_1 <- tune.svm(var, data = data.svm, gamma = 10^(-6:-1), cost = 10^(1:2))

fin <- Sys.time()

(time_m38 <- fin - ini)

summary(model_38_svm_r_2)

predic_model_38_svm_r_2 <- predict(model_38_svm_r_2, newdata = testing.svm,probability=TRUE)

#predic_model_30_svm_l_1_pr <- attr(predic_model_30_svm_l_1, "probabilities")[,1]

summary(predic_model_38_svm_r_2)



tb38 <- table(testing$flag_pago,predic_model_38_svm_r_2)
tb38

m38_accuracy <- (623 + 1526)/nrow(test)
m38_accuracy
#0.7354552



#score test data set


testing$m38_svm_r_score <- attr(predict(model_38_svm_r_2, newdata = testing.svm,probability=TRUE),"probabilities")[,1]


m38_pred <- prediction(testing$m38_svm_r_score, testing$flag_pago)
m38_perf <- performance(m38_pred,"tpr","fpr")

#ROC
plot(m38_perf, lwd=2, colorize=TRUE, main="ROC m38: SVM - Radial - data testing")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m38_perf_precision <- performance(m38_pred, measure = "prec", x.measure = "rec")
plot(m38_perf_precision, main="m38 SVM - Radial :Precision/recall curve")


# Plot accuracy as function of threshold
m38_perf_acc <- performance(m38_pred, measure = "acc")
plot(m38_perf_acc, main="m38 SVM - Radial :Accuracy as function of threshold")

#KS, Gini & AUC m1
m38_KS <- round(max(attr(m38_perf,'y.values')[[1]]-attr(m38_perf,'x.values')[[1]])*100, 2)
m38_AUROC <- round(performance(m38_pred, measure = "auc")@y.values[[1]]*100, 2)
m38_Gini <- (2*m38_AUROC - 100)

###########################################################################
cat("AUROC: ",m38_AUROC,"\tKS: ", m38_KS, "\tGini:", m38_Gini,"\tAccuracy:", m38_accuracy, "\n")


#------------------------------------------------------------------------
# 39. SVM - Radial - MAE - con training
#------------------------------------------------------------------------


ini <- Sys.time()

trControl <- trainControl(method = "repeatedcv",  repeats = 2, number = 2, verboseIter = FALSE,
                          summaryFunction = twoClassSummary, classProbs = T)


set.seed(1992)
model_39_svm_r_tun_2 <- train(var,  data=training, method="svmRadial", 
                              trControl = trControl, metric = 'ROC',tuneLength = 2)


fin <- Sys.time()

(time_m39 <- fin - ini)

model_39_svm_r_tun_2

model_39_svm_r_tun_2$bestTune


model_39_svm_r_tun_2$finalModel



pred39 <- predict(model_39_svm_r_tun_2,newdata = testing, type = "prob")[,2]


#score test data set
testing$m39_score <- predict(model_39_svm_r_tun_2,type='prob',newdata=testing)[,2]
m39_pred <- prediction(testing$m39_score, testing$flag_pago)

table(ifelse(testing$m39_score>0.5,"X1_Bueno","X0_Malo"), testing$flag_pago)
m39_accuracy <- ( 523+1596)/nrow(testing)
m39_accuracy
#0.7256849

m39_perf <- performance(m39_pred,"tpr","fpr")

#ROC
plot(m39_perf, lwd=2, colorize=TRUE, main="ROC m39: SVM - Radial - Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m39_perf_precision <- performance(m39_pred, measure = "prec", x.measure = "rec")
plot(m39_perf_precision, main="m39 SVM - Radial:Precision/recall curve")


# Plot accuracy as function of threshold
m39_perf_acc <- performance(m39_pred, measure = "acc")
plot(m39_perf_acc, main="m39 SVM - Radial:Accuracy as function of threshold")

#KS, Gini & AUC m1
m39_KS <- round(max(attr(m39_perf,'y.values')[[1]]-attr(m39_perf,'x.values')[[1]])*100, 2)
m39_AUROC <- round(performance(m39_pred, measure = "auc")@y.values[[1]]*100, 2)
m39_Gini <- (2*m39_AUROC - 100)

###########################################################################
cat("AUROC: ",m39_AUROC,"\tKS: ", m39_KS, "\tGini:", m39_Gini, "\tAccuracy:", m39_accuracy,"\n")


##############################################################################
#SVM - Radial
#
cat("AUROC: ",m36_AUROC,"\tKS: ", m36_KS, "\tGini:", m36_Gini, "\tAccuracy:", m36_accuracy,"\n")
cat("AUROC: ",m37_AUROC,"\tKS: ", m37_KS, "\tGini:", m37_Gini, "\tAccuracy:", m37_accuracy,"\n")
cat("AUROC: ",m38_AUROC,"\tKS: ", m38_KS, "\tGini:", m38_Gini,"\tAccuracy:", m38_accuracy, "\n")
cat("AUROC: ",m39_AUROC,"\tKS: ", m39_KS, "\tGini:", m39_Gini, "\tAccuracy:", m39_accuracy,"\n")


final_todo <- Sys.time()

tiempo_todo <- final_todo - inicio_todo

#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE v       COMPARACIÓN DE MODELOS
#---------------------------------------------------------------------------------------
########################################################################################


# tABLA DE PERFORMANCE - OOS
models <- c('m1:Regresión Logística - MASR', 'm2:Regresión Logística corte optimo - MASR',
            'm3:Regresión Logística - MASR - caret','m4:Regresión Logística - MACR', 
            'm5:Regresión Logística - MACR - caret', "m6: AD - cart - MASR",
            "m7: AD - cart - MASR - caret", "m8: AD - cart - MACR",
            "m9: AD - cart - MACR - caret", "m10: AD - ctree - MAsR",
            "m11: AD - ctree - MASR - caret", "m12: AD - ctree - MACR",
            "m13: AD - ctree - MACR - caret", "m14: RF - cart - MAsR",
            "m15: RF - cart - MASR - caret", "m16: RF - cart - MACR",
            "m17: RF - cart - MACR - caret", "m18: GBM - MAsR",
            "m19: GBM - MASR - caret","m20: GBM - MACR", "m21: GBM - MACR - caret","m22: Xgboost - MAsR",
            "m23: Xgboost - MASR - caret","m24: Xgboost - MACR", "m25: Xgboost - MACR - caret",
            "m26: RF - ctree - MASR - caret", "m27: RF - ctree - MACR - caret",
            "m28: SVM - Lineal - MASR","m29: SVM - Lineal - MASR - caret","m30: SVM - Lineal  - MACR", "m31: SVM - Lineal  - MACR - caret",
            "m32: SVM - Polinomial - MASR","m33: SVM - Polinomial - MASR - caret","m34: SVM - Polinomial  - MACR", "m35: SVM - Polinomial  - MACR - caret",
            "m36: SVM - Radial - MASR","m37: SVM - Radial - MASR - caret","m38: SVM - Radial  - MACR", "m39: SVM - Radial  - MACR - caret")

# AUCs
models_AUC <- c(m1_AUROC, m2_AUROC, m3_AUROC, m4_AUROC, m5_AUROC, m6_AUROC, m7_AUROC, m8_AUROC, m9_AUROC, m10_AUROC, 
                m11_AUROC, m12_AUROC, m13_AUROC, m14_AUROC, m15_AUROC, m16_AUROC, m17_AUROC, m18_AUROC, m19_AUROC, m20_AUROC, 
                m21_AUROC, m22_AUROC, m23_AUROC, m24_AUROC, m25_AUROC, m26_AUROC, m27_AUROC, m28_AUROC, m29_AUROC, m30_AUROC, 
                m31_AUROC, m32_AUROC, m33_AUROC, m34_AUROC, m35_AUROC, m36_AUROC, m37_AUROC, m38_AUROC, m39_AUROC)
# KS
models_KS <- c(m1_KS, m2_KS, m3_KS, m4_KS, m5_KS, m6_KS, m7_KS, m8_KS, m9_KS, m10_KS, m11_KS, m12_KS, m13_KS, m14_KS, m15_KS, 
               m16_KS, m17_KS, m18_KS, m19_KS, m20_KS, m21_KS, m22_KS, m23_KS, m24_KS, m25_KS, m26_KS, m27_KS, m28_KS, m29_KS, m30_KS, 
               m31_KS, m32_KS, m33_KS, m34_KS, m35_KS, m36_KS, m37_KS, m38_KS, m39_KS)

# Gini
models_Gini <- c(m1_Gini, m2_Gini, m3_Gini, m4_Gini, m5_Gini, m6_Gini, m7_Gini, m8_Gini, m9_Gini, m10_Gini, 
                 m11_Gini, m12_Gini, m13_Gini, m14_Gini, m15_Gini, m16_Gini, m17_Gini, m18_Gini, m19_Gini, m20_Gini, 
                 m21_Gini, m22_Gini, m23_Gini, m24_Gini, m25_Gini, m26_Gini, m27_Gini, m28_Gini, m29_Gini, m30_Gini, 
                 m31_Gini, m32_Gini, m33_Gini, m34_Gini, m35_Gini, m36_Gini, m37_Gini, m38_Gini, m39_Gini)


# Accuraccy

models_accuracy <- c(m1_accuracy, m2_accuracy, m3_accuracy, m4_accuracy, m5_accuracy, m6_accuracy, m7_accuracy, m8_accuracy, m9_accuracy, 
                     m10_accuracy, m11_accuracy, m12_accuracy, m13_accuracy, m14_accuracy, m15_accuracy, m16_accuracy, m17_accuracy, 
                     m18_accuracy, m19_accuracy, m20_accuracy, m21_accuracy, m22_accuracy, m23_accuracy, m24_accuracy, m25_accuracy, 
                     m26_accuracy, m27_accuracy, m28_accuracy, m29_accuracy, m30_accuracy, m31_accuracy, m32_accuracy, m33_accuracy, 
                     m34_accuracy, m35_accuracy, m36_accuracy, m37_accuracy, m38_accuracy, m39_accuracy)


# Juntando todo
model_performance_metric <- as.data.frame(cbind(models, models_AUC, models_KS, models_Gini))




# Colnames 
colnames(model_performance_metric) <- c("Modelos", "AUC", "KS", "Gini","Accuracy")

# Display Performance Reports
kable(model_performance_metric, caption ="Comparación de modelos - OOS")








#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VI          MODELOS EN OOT
#---------------------------------------------------------------------------------------
########################################################################################


#############################################################
# PERIODO - 201706
#############################################################


#'m1:Regresión Logística - MASR'

set.seed(1992)
predic_model_rl_1_201706 <- predict(model_rl_1,type='response',b201706)

tb1_201706 <- table(b201706$flag_pago,ifelse(predic_model_rl_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb1_201706

m1_accuracy_201706 <- (121 + 280)/nrow(b201706)
m1_accuracy_201706

set.seed(1992)
#score test data set
b201706$m1_score_201706 <- predict(model_rl_1,type='response',b201706)
m1_pred_201706 <- prediction(b201706$m1_score_201706, b201706$flag_pago)
m1_perf_201706 <- performance(m1_pred_201706,"tpr","fpr")

#ROC
plot(m1_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision_201706 <- performance(m1_pred_201706, measure = "prec", x.measure = "rec")
plot(m1_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc_201706 <- performance(m1_pred_201706, measure = "acc")
plot(m1_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS_201706 <- round(max(attr(m1_perf_201706,'y.values')[[1]]-attr(m1_perf_201706,'x.values')[[1]])*100, 2)
m1_AUROC_201706 <- round(performance(m1_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini_201706 <- (2*m1_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m1_AUROC_201706,"\tKS: ", m1_KS_201706, "\tGini:", m1_Gini_201706, "\tAccuracy:", m1_accuracy_201706,"\n")



#'m2:Regresión Logística corte optimo - MASR'



tb2_201706 <- table(b201706$flag_pago,ifelse(predic_model_rl_1_201706>optCutOff,"X1_Bueno","X0_Malo"))
tb2_201706
m2_accuracy_201706 <-  ( 121+ 280)/nrow(b201706)
m2_accuracy_201706
#0.7316906

set.seed(1992)
#score test data set
b201706$m2_score <- predict(model_rl_1,type='response',b201706)
m2_pred_201706 <- prediction(ifelse(b201706$m1_score>optCutOff,1,0), b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf_201706 <- performance(m2_pred_201706,"tpr","fpr")

#ROC
plot(m2_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision_201706 <- performance(m2_pred_201706, measure = "prec", x.measure = "rec")
plot(m2_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc_201706 <- performance(m2_pred_201706, measure = "acc")
plot(m2_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS_201706 <- round(max(attr(m2_perf_201706,'y.values')[[1]]-attr(m2_perf_201706,'x.values')[[1]])*100, 2)
m2_AUROC_201706 <- round(performance(m2_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini_201706 <- (2*m2_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m2_AUROC_201706,"\tKS: ", m2_KS_201706, "\tGini:", m2_Gini_201706, "\tAccuracy:", m2_accuracy_201706,"\n")



#'m3:Regresión Logística - MASR - caret'

#score test data set
b201706$m3_score <- predict(model_rl_3_tun,type='prob',newdata=b201706)[2]
m3_pred_201706 <- prediction(b201706$m3_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3_201706 <- table(ifelse(b201706$m3_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb3_201706

m3_accuracy_201706 <- (121+280)/nrow(b201706)
m3_accuracy_201706

m3_perf_201706 <- performance(m3_pred_201706,"tpr","fpr")

#ROC
plot(m3_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic - 201706 Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision_201706 <- performance(m3_pred_201706, measure = "prec", x.measure = "rec")
plot(m3_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc_201706 <- performance(m3_pred_201706, measure = "acc")
plot(m3_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS_201706 <- round(max(attr(m3_perf_201706,'y.values')[[1]]-attr(m3_perf_201706,'x.values')[[1]])*100, 2)
m3_AUROC_201706 <- round(performance(m3_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini_201706 <- (2*m3_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m3_AUROC_201706,"\tKS: ", m3_KS_201706, "\tGini:", m3_Gini_201706,"\tAccuracy:", m3_accuracy_201706, "\n")


#'m4:Regresión Logística - MACR'

predic_model_rl_4_201706 <- predict(model_rl_4, newdata = b201706,type = "response")

summary(predic_model_rl_4_201706)


tb4_201706 <- table(b201706$flag_pago,ifelse(predic_model_rl_4_201706>0.5,"X1_Bueno","X0_Malo"))
tb4_201706

(m4_accuracy_201706 <- (122 +  280)/nrow(b201706))

#0.6711185

#score test data set
b201706$m4_score <- predict(model_rl_4,type='response',b201706)
m4_pred_201706 <- prediction(b201706$m4_score, b201706$flag_pago)
m4_perf_201706 <- performance(m4_pred_201706,"tpr","fpr")

#ROC
plot(m4_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision_201706 <- performance(m4_pred_201706, measure = "prec", x.measure = "rec")
plot(m4_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc_201706 <- performance(m4_pred_201706, measure = "acc")
plot(m4_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS_201706 <- round(max(attr(m4_perf_201706,'y.values')[[1]]-attr(m4_perf_201706,'x.values')[[1]])*100, 2)
m4_AUROC_201706 <- round(performance(m4_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini_201706 <- (2*m4_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m4_AUROC_201706,"\tKS: ", m4_KS_201706, "\tGini:", m4_Gini_201706,"\tAccuracy:", m4_accuracy_201706, "\n")



#'m5:Regresión Logística - MACR - caret'

#score test data set
b201706$m5_score <- predict(model_rl_5_tun,type='prob',newdata=b201706)[2]
m5_pred_201706 <- prediction(b201706$m5_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb201706 <- table(ifelse(b201706$m5_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb201706

m5_accuracy_201706 <- (122+280)/nrow(b201706)
m5_accuracy_201706
#0.6711185

m5_perf_201706 <- performance(m5_pred_201706,"tpr","fpr")

#ROC
plot(m5_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision_201706 <- performance(m5_pred_201706, measure = "prec", x.measure = "rec")
plot(m5_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc_201706 <- performance(m5_pred_201706, measure = "acc")
plot(m5_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS_201706 <- round(max(attr(m5_perf_201706,'y.values')[[1]]-attr(m5_perf_201706,'x.values')[[1]])*100, 2)
m5_AUROC_201706 <- round(performance(m5_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini_201706 <- (2*m5_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m5_AUROC_201706,"\tKS: ", m5_KS_201706, "\tGini:", m5_Gini_201706, "\tAccuracy:", m5_accuracy_201706,"\n")


#"m6: AD - cart - MASR"

predic_model_6_ad_1_201706 <- predict(model_6_ad_1, newdata = b201706,type = "prob")[,2]

summary(predic_model_6_ad_1_201706)


tb6_201706 <- table(b201706$flag_pago,ifelse(predic_model_6_ad_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb6_201706

m6_accuracy_201706 <- (128 + 273)/nrow(b201706)
m6_accuracy_201706
#0.6694491

#score test data set
b201706$m6_ad_score <- predict(model_6_ad_1,type='prob',b201706)[,2]
m6_pred_201706 <- prediction(b201706$m6_ad_score, b201706$flag_pago)
m6_perf_201706 <- performance(m6_pred_201706,"tpr","fpr")

#ROC
plot(m6_perf_201706, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision_201706 <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision_201706, main="m6 Árbol de decisión - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc_201706 <- performance(m6_pred_201706, measure = "acc")
plot(m6_perf_acc_201706, main="m6 Árbol de decisión - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS_201706 <- round(max(attr(m6_perf_201706,'y.values')[[1]]-attr(m6_perf_201706,'x.values')[[1]])*100, 2)
m6_AUROC_201706 <- round(performance(m6_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini_201706 <- (2*m6_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m6_AUROC_201706,"\tKS: ", m6_KS_201706, "\tGini:", m6_Gini_201706, "\tAccuracy:", m6_accuracy_201706,"\n")


#"m7: AD - cart - MASR - caret"

#score test data set
b201706$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=b201706)[2]
m7_pred_201706 <- prediction(b201706$m7_score, b201706$flag_pago)

tb7_201706 <- table(ifelse(b201706$m7_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb7_201706

m7_accuracy_201706 <- (123+ 277)/nrow(b201706)
m7_accuracy_201706
#0.6677796

m7_perf_201706 <- performance(m7_pred_201706,"tpr","fpr")

#ROC
plot(m7_perf_201706, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision_201706 <- performance(m7_pred_201706, measure = "prec", x.measure = "rec")
plot(m7_perf_precision_201706, main="m7 Arbol de decision - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc_201706 <- performance(m7_pred_201706, measure = "acc")
plot(m7_perf_acc_201706, main="m7 Arbol tuneado:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m7_KS_201706 <- round(max(attr(m7_perf_201706,'y.values')[[1]]-attr(m7_perf_201706,'x.values')[[1]])*100, 2)
m7_AUROC_201706 <- round(performance(m7_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini_201706 <- (2*m7_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")


#"m8: AD - cart - MACR"

predic_model_8_ad_2_201706 <- predict(model_8_ad_2, newdata = b201706,type = "prob")[,2]

summary(predic_model_8_ad_2_201706)


tb8_201706 <- table(b201706$flag_pago,ifelse(predic_model_8_ad_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb8_201706

m8_accuracy_201706 <- (118 +  283)/nrow(b201706)
m8_accuracy_201706
#0.6694491

#score test data set
b201706$m8_score <- predict(model_8_ad_2,type='prob',b201706)[,2]
m8_pred_201706 <- prediction(b201706$m8_score, b201706$flag_pago)
m8_perf_201706 <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf_201706, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision_201706 <- performance(m8_pred_201706, measure = "prec", x.measure = "rec")
plot(m8_perf_precision_201706, main="m8 Arbol de decision - rpart:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m8_perf_acc_201706 <- performance(m8_pred_201706, measure = "acc")
plot(m8_perf_acc_201706, main="m8 Arbol de decision - rpart:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m8_KS_201706 <- round(max(attr(m8_perf_201706,'y.values')[[1]]-attr(m8_perf_201706,'x.values')[[1]])*100, 2)
m8_AUROC_201706 <- round(performance(m8_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini_201706 <- (2*m8_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m8_AUROC_201706,"\tKS: ", m8_KS_201706, "\tGini:", m8_Gini_201706,"\tAccuracy:", m8_accuracy_201706, "\n")


#"m9: AD - cart - MACR - caret"

#score test data set
b201706$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=b201706)[,2]
m9_pred_201706 <- prediction(b201706$m9_score, b201706$flag_pago)


tb9_201706 <- table(ifelse(b201706$m9_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb9_201706

m9_accuracy_201706 <- (117+279)/nrow(b201706)
m9_accuracy_201706
#0.6611018

m9_perf_201706 <- performance(m9_pred_201706,"tpr","fpr")

#ROC
plot(m9_perf_201706, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision_201706 <- performance(m9_pred_201706, measure = "prec", x.measure = "rec")
plot(m9_perf_precision_201706, main="m9 Arbol de decision - caret:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m9_perf_acc_201706 <- performance(m9_pred_201706, measure = "acc")
plot(m9_perf_acc_201706, main="m9 Arbol de decision - caret:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m9_KS_201706 <- round(max(attr(m9_perf_201706,'y.values')[[1]]-attr(m9_perf_201706,'x.values')[[1]])*100, 2)
m9_AUROC_201706 <- round(performance(m9_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini_201706 <- (2*m9_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m9_AUROC_201706,"\tKS: ", m9_KS_201706, "\tGini:", m9_Gini_201706, "\tAccuracy:", m9_accuracy_201706,"\n")


#"m10: AD - ctree - MAsR"


predic_model_10_ad_1_201706 <- sapply(predict(model_10_ad_1, newdata = b201706,type = "prob"),'[[',2)

summary(predic_model_10_ad_1_201706)

tb10_201706 <- table(b201706$flag_pago,ifelse(predic_model_10_ad_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb10_201706

m10_accuracy_201706 <- (122 + 278)/nrow(b201706)
m10_accuracy_201706
#0.6677796

#score test data set
b201706$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = b201706,type = "prob")))))[,2]

m10_pred_201706 <- prediction(b201706$m10_ad_score, b201706$flag_pago)
m10_perf_201706 <- performance(m10_pred_201706,"tpr","fpr")

#ROC
plot(m10_perf_201706, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision_201706 <- performance(m10_pred_201706, measure = "prec", x.measure = "rec")
plot(m10_perf_precision_201706, main="m6 Árbol de decisión - CTREE:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m10_perf_acc_201706 <- performance(m10_pred_201706, measure = "acc")
plot(m10_perf_acc_201706, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m10_KS_201706 <- round(max(attr(m10_perf_201706,'y.values')[[1]]-attr(m10_perf_201706,'x.values')[[1]])*100, 2)
m10_AUROC_201706 <- round(performance(m10_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini_201706 <- (2*m10_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m10_AUROC_201706,"\tKS: ", m10_KS_201706, "\tGini:", m10_Gini_201706, "\tAccuracy:", m10_accuracy_201706,"\n")


#"m11: AD - ctree - MASR - caret"


#score test data set
b201706$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=b201706)[2]
m11_pred_201706 <- prediction(b201706$m11_score, b201706$flag_pago)

tb11_201706 <- table(ifelse(b201706$m11_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb11_201706

m11_accuracy_201706 <- (119+281)/nrow(b201706)
m11_accuracy_201706
#0.6677796

m11_perf_201706 <- performance(m11_pred_201706,"tpr","fpr")

#ROC
plot(m11_perf_201706, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision_201706 <- performance(m11_pred_201706, measure = "prec", x.measure = "rec")
plot(m11_perf_precision_201706, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m11_perf_acc_201706 <- performance(m11_pred_201706, measure = "acc")
plot(m11_perf_acc_201706, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m11_KS_201706 <- round(max(attr(m11_perf_201706,'y.values')[[1]]-attr(m11_perf_201706,'x.values')[[1]])*100, 2)
m11_AUROC_201706 <- round(performance(m11_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini_201706 <- (2*m11_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m11_AUROC_201706,"\tKS: ", m11_KS_201706, "\tGini:", m11_Gini_201706, "\tAccuracy:", m11_accuracy_201706,"\n")



#"m12: AD - ctree - MACR"

predic_model_12_ad_2_201706 <- sapply(predict(model_12_ad_2, newdata = b201706,type = "prob"),'[[',2)

summary(predic_model_12_ad_2_201706)


tb12_201706 <- table(b201706$flag_pago,ifelse(predic_model_12_ad_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb12_201706

m12_accuracy_201706 <- (130 + 276)/nrow(b201706)
m12_accuracy_201706
#0.6777963

#score test data set
b201706$m12_score <- sapply(predict(model_12_ad_2, newdata = b201706,type = "prob"),'[[',2)
m12_pred_201706 <- prediction(b201706$m12_score, b201706$flag_pago)
m12_perf_201706 <- performance(m12_pred_201706,"tpr","fpr")

#ROC
plot(m12_perf_201706, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision_201706 <- performance(m12_pred_201706, measure = "prec", x.measure = "rec")
plot(m12_perf_precision_201706, main="m12 Arbol de decision - CTREE :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m12_perf_acc_201706 <- performance(m12_pred_201706, measure = "acc")
plot(m12_perf_acc_201706, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m12_KS_201706 <- round(max(attr(m12_perf_201706,'y.values')[[1]]-attr(m12_perf_201706,'x.values')[[1]])*100, 2)
m12_AUROC_201706 <- round(performance(m12_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini_201706 <- (2*m12_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m12_AUROC_201706,"\tKS: ", m12_KS_201706, "\tGini:", m12_Gini_201706,"\tAccuracy:", m12_accuracy_201706, "\n")



#"m13: AD - ctree - MACR - caret"

#score test data set
b201706$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=b201706)[,2]
m13_pred_201706 <- prediction(b201706$m13_score, b201706$flag_pago)


table(ifelse(b201706$m13_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m13_accuracy_201706 <- (131+268)/nrow(b201706)
m13_accuracy_201706
#0.6661102

m13_perf_201706 <- performance(m13_pred_201706,"tpr","fpr")

#ROC
plot(m13_perf_201706, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision_201706 <- performance(m13_pred_201706, measure = "prec", x.measure = "rec")
plot(m13_perf_precision_201706, main="m13 Arbol de decision - CTREE:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m13_perf_acc_201706 <- performance(m13_pred_201706, measure = "acc")
plot(m13_perf_acc_201706, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m13_KS_201706 <- round(max(attr(m13_perf_201706,'y.values')[[1]]-attr(m13_perf_201706,'x.values')[[1]])*100, 2)
m13_AUROC_201706 <- round(performance(m13_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini_201706 <- (2*m13_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m13_AUROC_201706,"\tKS: ", m13_KS_201706, "\tGini:", m13_Gini_201706, "\tAccuracy:", m13_accuracy_201706,"\n")


#"m14: RF - cart - MAsR"



predic_model_14_rf_1_201706 <- predict(model_14_rf_1, newdata = b201706,type = "prob")[,2]

summary(predic_model_14_rf_1_201706)

tb14_201706 <- table(b201706$flag_pago,ifelse(predic_model_14_rf_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb14_201706

m14_accuracy_20176 <- (124 + 275)/nrow(b201706)
m14_accuracy_20176
#0.6661102

#score test data set
b201706$m14_rf_score <- predict(model_14_rf_1, newdata = b201706,type = "prob")[,2]

m14_pred_201706 <- prediction(b201706$m14_rf_score, b201706$flag_pago)
m14_perf_201706 <- performance(m14_pred_201706,"tpr","fpr")

#ROC
plot(m14_perf_201706, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision_201706 <- performance(m14_pred_201706, measure = "prec", x.measure = "rec")
plot(m14_perf_precision_201706, main="m14 Random Forest:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m14_perf_acc_201706 <- performance(m14_pred_201706, measure = "acc")
plot(m14_perf_acc_201706, main="m14 Random Forest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m14_KS_201706 <- round(max(attr(m14_perf_201706,'y.values')[[1]]-attr(m14_perf_201706,'x.values')[[1]])*100, 2)
m14_AUROC_201706 <- round(performance(m14_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini_201706 <- (2*m14_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m14_AUROC_201706,"\tKS: ", m14_KS_201706, "\tGini:", m14_Gini_201706, "\tAccuracy:", m14_accuracy_20176,"\n")


#"m15: RF - cart - MASR - caret"

#score test data set
b201706$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=b201706)[2]
m15_pred_201706 <- prediction(b201706$m15_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m15_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m15_accuracy_201706 <- (641+1501)/nrow(b201706)
m15_accuracy_201706
#0.7330595

m15_perf_201706 <- performance(m15_pred_201706,"tpr","fpr")

#ROC
plot(m15_perf_201706, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision_201706 <- performance(m15_pred_201706, measure = "prec", x.measure = "rec")
plot(m15_perf_precision_201706, main="m15 RandomForest:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m15_perf_acc_201706 <- performance(m15_pred_201706, measure = "acc")
plot(m15_perf_acc_201706, main="m15 RandomForest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m15_KS_201706 <- round(max(attr(m15_perf_201706,'y.values')[[1]]-attr(m15_perf_201706,'x.values')[[1]])*100, 2)
m15_AUROC_201706 <- round(performance(m15_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini_201706 <- (2*m15_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m15_AUROC_201706,"\tKS: ", m15_KS_201706, "\tGini:", m15_Gini_201706, "\tAccuracy:", m15_accuracy_201706,"\n")


#"m16: RF - cart - MACR"

predic_model_16_rf_2_201706 <- predict(model_16_rf_2, newdata = b201706,type = "prob")[,2]

summary(predic_model_16_rf_2_201706)

tb16_201706 <- table(b201706$flag_pago,ifelse(predic_model_16_rf_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb16_201706


m16_accuracy_201706 <- (640 + 1553)/nrow(b201706)
m16_accuracy_201706
#0.7510274

#score test data set

b201706$m16_rf_2_score <- predict(model_16_rf_2, newdata = b201706,type = "prob")[,2]

m16_pred_201706 <- prediction(b201706$m16_rf_2_score, b201706$flag_pago)
m16_perf_201706 <- performance(m16_pred_201706,"tpr","fpr")

#ROC
plot(m16_perf_201706, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision_201706 <- performance(m16_pred_201706, measure = "prec", x.measure = "rec")
plot(m16_perf_precision_201706, main="m16 RandomForest :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m16_perf_acc_201706 <- performance(m16_pred_201706, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS_201706 <- round(max(attr(m16_perf_201706,'y.values')[[1]]-attr(m16_perf_201706,'x.values')[[1]])*100, 2)
m16_AUROC_201706 <- round(performance(m16_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini_201706 <- (2*m16_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m16_AUROC_201706,"\tKS: ", m16_KS_201706, "\tGini:", m16_Gini_201706,"\tAccuracy:", m16_accuracy_201706, "\n")



#"m17: RF - cart - MACR - caret"

#score test data set
b201706$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=b201706)[,2]
m17_pred_201706 <- prediction(b201706$m17_score, b201706$flag_pago)

table(ifelse(b201706$m17_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m17_accuracy_201706 <- (639+1515)/nrow(b201706)
m17_accuracy_201706
#0.7376712

m17_perf_201706 <- performance(m17_pred_201706,"tpr","fpr")

#ROC
plot(m17_perf_201706, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision_201706 <- performance(m17_pred_201706, measure = "prec", x.measure = "rec")
plot(m17_perf_precision_201706, main="m17 RandomForest:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m17_perf_acc_201706 <- performance(m17_pred_201706, measure = "acc")
plot(m17_perf_acc_201706, main="m17 RandomForest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m17_KS_201706 <- round(max(attr(m17_perf_201706,'y.values')[[1]]-attr(m17_perf_201706,'x.values')[[1]])*100, 2)
m17_AUROC_201706 <- round(performance(m17_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini_201706 <- (2*m17_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m17_AUROC_201706,"\tKS: ", m17_KS_201706, "\tGini:", m17_Gini_201706, "\tAccuracy:", m17_accuracy_201706,"\n")


#"m18: GBM - MAsR"

predic_model_18_gbm_1_201706 <- predict(model_18_gbm_1, newdata = b201706,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1_201706, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18_201706 <- table(b201706$flag_pago,ifelse(predic_model_18_gbm_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb18_201706

m18_accuracy_201706 <- (622+1518)/nrow(b201706)
m18_accuracy_201706

#0.7323751

#score test data set
b201706$m18_GBM_score <- predict(model_18_gbm_1, newdata = b201706,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred_201706 <- prediction(b201706$m18_GBM_score, b201706$flag_pago)
m18_perf_201706 <- performance(m18_pred_201706,"tpr","fpr")

#ROC
plot(m18_perf_201706, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision_201706 <- performance(m18_pred_201706, measure = "prec", x.measure = "rec")
plot(m18_perf_precision_201706, main="m18 GBM:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m18_perf_acc_201706 <- performance(m18_pred_201706, measure = "acc")
plot(m18_perf_acc_201706, main="m18 GBM:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m18_KS_201706 <- round(max(attr(m18_perf_201706,'y.values')[[1]]-attr(m18_perf_201706,'x.values')[[1]])*100, 2)
m18_AUROC_201706 <- round(performance(m18_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini_201706 <- (2*m18_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m18_AUROC_201706,"\tKS: ", m18_KS_201706, "\tGini:", m18_Gini_201706, "\tAccuracy:", m18_accuracy_201706,"\n")


#"m19: GBM - MASR - caret"

#score test data set
b201706$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=b201706)[2]
m19_pred_201706 <- prediction(b201706$m19_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m19_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m19_accuracy_201706 <- (630+1508)/nrow(b201706)
m19_accuracy_201706
#0.7316906

m19_perf_201706 <- performance(m19_pred_201706,"tpr","fpr")

#ROC
plot(m19_perf_201706, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision_201706 <- performance(m19_pred_201706, measure = "prec", x.measure = "rec")
plot(m19_perf_precision_201706, main="m19 GBM:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m19_perf_acc_201706 <- performance(m19_pred_201706, measure = "acc")
plot(m19_perf_acc_201706, main="m19 GBM:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m19_KS_201706 <- round(max(attr(m19_perf_201706,'y.values')[[1]]-attr(m19_perf_201706,'x.values')[[1]])*100, 2)
m19_AUROC_201706 <- round(performance(m19_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini_201706 <- (2*m19_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m19_AUROC_201706,"\tKS: ", m19_KS_201706, "\tGini:", m19_Gini_201706, "\tAccuracy:", m19_accuracy_201706,"\n")



#"m20: GBM - MACR"



predic_model_20_gbm_2_201706 <- predict(model_20_gbm_2, newdata = b201706,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2_201706, n=30)


summary(predic_model_20_gbm_2_201706)

tb20_201706 <- table(b201706$flag_pago,ifelse(predic_model_20_gbm_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb20_201706


m20_accuracy_201706 <- (636 + 1514)/nrow(b201706)
m20_accuracy_201706
#0.7363014

#score test data set


b201706$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = b201706,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred_201706 <- prediction(b201706$m20_GBM_2_score, b201706$flag_pago)
m20_perf_201706 <- performance(m20_pred_201706,"tpr","fpr")

#ROC
plot(m20_perf_201706, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision_201706 <- performance(m20_pred_201706, measure = "prec", x.measure = "rec")
plot(m20_perf_precision_201706, main="m20 GBM :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m20_perf_acc_201706 <- performance(m20_pred_201706, measure = "acc")
plot(m20_perf_acc_201706, main="m20 GBM :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m20_KS_201706 <- round(max(attr(m20_perf_201706,'y.values')[[1]]-attr(m20_perf_201706,'x.values')[[1]])*100, 2)
m20_AUROC_201706 <- round(performance(m20_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini_201706 <- (2*m20_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m20_AUROC_201706,"\tKS: ", m20_KS_201706, "\tGini:", m20_Gini_201706,"\tAccuracy:", m20_accuracy_201706, "\n")


#"m21: GBM - MACR - caret"


#score test data set
b201706$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=b201706)[,2]
m21_pred_201706 <- prediction(b201706$m21_score, b201706$flag_pago)

table(ifelse(b201706$m21_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m21_accuracy_201706 <- (638+1513)/nrow(b201706)
m21_accuracy_201706
#0.7366438

m21_perf_201706 <- performance(m21_pred_201706,"tpr","fpr")

#ROC
plot(m21_perf_201706, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision_201706 <- performance(m21_pred_201706, measure = "prec", x.measure = "rec")
plot(m21_perf_precision_201706, main="m21 GBM:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m21_perf_acc_201706 <- performance(m21_pred_201706, measure = "acc")
plot(m21_perf_acc_201706, main="m21 RandomForest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m21_KS_201706 <- round(max(attr(m21_perf_201706,'y.values')[[1]]-attr(m21_perf_201706,'x.values')[[1]])*100, 2)
m21_AUROC_201706 <- round(performance(m21_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini_201706 <- (2*m21_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m21_AUROC_201706,"\tKS: ", m21_KS_201706, "\tGini:", m21_Gini_201706, "\tAccuracy:", m21_accuracy_201706,"\n")



#"m22: Xgboost - MAsR"

testx_201706 <- data.table(b201706[,c(10:16,19)])


testx_201706 <- sparse.model.matrix(flag_pago_val~., data = testx_201706)

testy_201706 = b201706[,19] == "1"



predic_model_22_xgboost_1_201706 <- predict(model_22_xgboost_1, newdata = testx_201706, type = "response")

head(predic_model_22_xgboost_1_201706, n=30)


tb22_201706 <- table(b201706$flag_pago,ifelse(predic_model_22_xgboost_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb22_201706

m22_accuracy_201706 <- (637+1499)/nrow(b201706)
m22_accuracy_201706

#0.7310062

#score test data set
b201706$m22_Xgboost_score <- predict(model_22_xgboost_1, newdata = testx_201706, type = "response")

m22_pred_201706 <- prediction(b201706$m22_Xgboost_score, b201706$flag_pago)
m22_perf_201706 <- performance(m22_pred_201706,"tpr","fpr")

#ROC
plot(m22_perf_201706, lwd=2, colorize=TRUE, main="ROC m22: Xgboost Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m22_perf_precision_201706 <- performance(m22_pred_201706, measure = "prec", x.measure = "rec")
plot(m22_perf_precision_201706, main="m22 Xgboost:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m22_perf_acc_201706 <- performance(m22_pred_201706, measure = "acc")
plot(m22_perf_acc_201706, main="m22 Xgboost:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m22_KS_201706 <- round(max(attr(m22_perf_201706,'y.values')[[1]]-attr(m22_perf_201706,'x.values')[[1]])*100, 2)
m22_AUROC_201706 <- round(performance(m22_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m22_Gini_201706 <- (2*m22_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m22_AUROC_201706,"\tKS: ", m22_KS_201706, "\tGini:", m22_Gini_201706, "\tAccuracy:", m22_accuracy_201706,"\n")



#"m23: Xgboost - MASR - caret"


#score test data set
b201706$m23_score <- predict(model_23_Xgboost_tun_1,type='prob',newdata=b201706)[2]
m23_pred_201706 <- prediction(b201706$m23_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m23_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m23_accuracy_201706 <- (626+1503)/nrow(b201706)
m23_accuracy_201706
#0.7316906

m23_perf_201706 <- performance(m23_pred_201706,"tpr","fpr")

#ROC
plot(m23_perf_201706, lwd=2, colorize=TRUE, main="ROC m23: Xgboost - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m23_perf_precision_201706 <- performance(m23_pred_201706, measure = "prec", x.measure = "rec")
plot(m23_perf_precision_201706, main="m23 Xgboost:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m23_perf_acc_201706 <- performance(m23_pred_201706, measure = "acc")
plot(m23_perf_acc_201706, main="m23 Xgboost:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m23_KS_201706 <- round(max(attr(m23_perf_201706,'y.values')[[1]]-attr(m23_perf_201706,'x.values')[[1]])*100, 2)
m23_AUROC_201706 <- round(performance(m23_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m23_Gini_201706 <- (2*m23_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m23_AUROC_201706,"\tKS: ", m23_KS_201706, "\tGini:", m23_Gini_201706, "\tAccuracy:", m23_accuracy_201706,"\n")


#"m24: Xgboost - MACR"


predic_model_24_xgboost_2_201706 <- predict(model_24_xgboost_2, newdata = testx_201706, type = "response")

head(predic_model_24_xgboost_2_201706, n=30)


summary(predic_model_24_xgboost_2_201706)

tb24_201706 <- table(b201706$flag_pago,ifelse(predic_model_24_xgboost_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb24_201706


m24_accuracy_201706 <- (634 + 1501)/nrow(b201706)
m24_accuracy_201706
#0.7311644

#score test data set


b201706$m24_Xgboost_2_score <- predict(model_24_xgboost_2, newdata = testx_201706, type = "response")



m24_pred_201706 <- prediction(b201706$m24_Xgboost_2_score, b201706$flag_pago)
m24_perf_201706 <- performance(m24_pred_201706,"tpr","fpr")

#ROC
plot(m24_perf_201706, lwd=2, colorize=TRUE, main="ROC m24: Xgboost - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m24_perf_precision_201706 <- performance(m24_pred_201706, measure = "prec", x.measure = "rec")
plot(m24_perf_precision_201706, main="m24 Xgboost :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m24_perf_acc_201706 <- performance(m24_pred_201706, measure = "acc")
plot(m24_perf_acc_201706, main="m24 Xgboost :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m24_KS_201706 <- round(max(attr(m24_perf_201706,'y.values')[[1]]-attr(m24_perf_201706,'x.values')[[1]])*100, 2)
m24_AUROC_201706 <- round(performance(m24_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m24_Gini_20706 <- (2*m24_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m24_AUROC_201706,"\tKS: ", m24_KS_201706, "\tGini:", m24_Gini_20706,"\tAccuracy:", m24_accuracy_201706, "\n")


#"m25: Xgboost - MACR - caret"

#score test data set
b201706$m25_score <- predict(model_25_xgboost_tun_2,type='prob',newdata=b201706)[,2]
m25_pred_201706 <- prediction(b201706$m25_score, b201706$flag_pago)

table(ifelse(b201706$m25_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m25_accuracy_201706 <- (615+1530)/nrow(b201706)
m25_accuracy_201706
#0.734589

m25_perf_201706 <- performance(m25_pred_201706,"tpr","fpr")

#ROC
plot(m25_perf_201706, lwd=2, colorize=TRUE, main="ROC m25: Xgboost Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m25_perf_precision_201706 <- performance(m25_pred_201706, measure = "prec", x.measure = "rec")
plot(m25_perf_precision_201706, main="m25 Xgboost:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m25_perf_acc_201706 <- performance(m25_pred_201706, measure = "acc")
plot(m25_perf_acc_201706, main="m25 Xgboost:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m25_KS_201706 <- round(max(attr(m25_perf_201706,'y.values')[[1]]-attr(m25_perf_201706,'x.values')[[1]])*100, 2)
m25_AUROC_201706 <- round(performance(m25_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m25_Gini_201706 <- (2*m25_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m25_AUROC_201706,"\tKS: ", m25_KS_201706, "\tGini:", m25_Gini_201706, "\tAccuracy:", m25_accuracy_201706,"\n")


#"m26: RF - ctree - MASR - caret"


#score test data set
b201706$m26_score <- predict(model_26_RF_ctree_tun_1,type='prob',newdata=b201706)[2]
m26_pred_201706 <- prediction(b201706$m26_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m26_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m26_accuracy_201706 <- (639+ 1488)/nrow(b201706)
m26_accuracy_201706
#0.7279261

m26_perf_201706 <- performance(m26_pred_201706,"tpr","fpr")

#ROC
plot(m26_perf_201706, lwd=2, colorize=TRUE, main="ROC m26: RandomForest - ctree - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m26_perf_precision_201706 <- performance(m26_pred_201706, measure = "prec", x.measure = "rec")
plot(m26_perf_precision_201706, main="m26 RandomForest - ctree:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m26_perf_acc_201706 <- performance(m26_pred_201706, measure = "acc")
plot(m26_perf_acc_201706, main="m26 RandomForest - ctree:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m26_KS_201706 <- round(max(attr(m26_perf_201706,'y.values')[[1]]-attr(m26_perf_201706,'x.values')[[1]])*100, 2)
m26_AUROC_201706 <- round(performance(m26_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m26_Gini_201706 <- (2*m26_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m26_AUROC_201706,"\tKS: ", m26_KS_201706, "\tGini:", m26_Gini_201706, "\tAccuracy:", m26_accuracy_201706,"\n")


#"m27: RF - ctree - MACR - caret"


#score test data set
b201706$m27_score <- predict(model_27_rf_ctree_tun_2,type='prob',newdata=b201706)[,2]
m27_pred_201706 <- prediction(b201706$m27_score, b201706$flag_pago)

table(ifelse(b201706$m27_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m27_accuracy_201706 <- (395+1677)/nrow(b201706)
m27_accuracy_201706
#0.709589

m27_perf_201706 <- performance(m27_pred_201706,"tpr","fpr")

#ROC
plot(m27_perf_201706, lwd=2, colorize=TRUE, main="ROC m27: RandomForest Ctree Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m27_perf_precision_201706 <- performance(m27_pred_201706, measure = "prec", x.measure = "rec")
plot(m27_perf_precision_201706, main="m27 RandomForest ctree :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m27_perf_acc_201706 <- performance(m27_pred_201706, measure = "acc")
plot(m27_perf_acc_201706, main="m27 RandomForest ctree :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m27_KS_201706 <- round(max(attr(m27_perf_201706,'y.values')[[1]]-attr(m27_perf_201706,'x.values')[[1]])*100, 2)
m27_AUROC_201706 <- round(performance(m27_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m27_Gini_201706 <- (2*m27_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m27_AUROC_201706,"\tKS: ", m27_KS_201706, "\tGini:", m27_Gini_201706, "\tAccuracy:", m27_accuracy_201706,"\n")



#"m28: SVM - Lineal - MASR"

test.svm_201706 <- b201706[,10:16]

predic_model_28_svm_l_1_201706 <- predict(model_28_svm_l_1, newdata = test.svm_201706,probability=TRUE)



summary(predic_model_28_svm_l_1_201706)



tb28_201706 <- table(b201706$flag_pago,predic_model_28_svm_l_1_201706)
tb28_201706

m28_accuracy_201706 <- (729 + 1316)/nrow(b201706)
m28_accuracy_201706
#0.6998631

#score test data set


b201706$m28_svm_l_score <- attr(predict(model_28_svm_l_1, newdata = test.svm_201706,probability=TRUE),"probabilities")[,1]





m28_pred_201706 <- prediction(b201706$m28_svm_l_score, b201706$flag_pago)
m28_perf_201706 <- performance(m28_pred_201706,"tpr","fpr")

#ROC
plot(m28_perf_201706, lwd=2, colorize=TRUE, main="ROC m28: SVM - Lineal Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m28_perf_precision_201706 <- performance(m28_pred_201706, measure = "prec", x.measure = "rec")
plot(m28_perf_precision_201706, main="m28 SVm lineal :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m28_perf_acc_201706 <- performance(m28_pred_201706, measure = "acc")
plot(m28_perf_acc_201706, main="m28 SVM lineal :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m28_KS_201706 <- round(max(attr(m28_perf_201706,'y.values')[[1]]-attr(m28_perf_201706,'x.values')[[1]])*100, 2)
m28_AUROC_201706 <- round(performance(m28_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m28_Gini_201706 <- (2*m28_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m28_AUROC_201706,"\tKS: ", m28_KS_201706, "\tGini:", m28_Gini_201706, "\tAccuracy:", m28_accuracy_201706,"\n")


#"m29: SVM - Lineal - MASR - caret"


#score test data set
b201706$m29_score <- predict(model_29_svm_l_tun_1,type='prob',newdata=b201706)[2]
m29_pred_201706 <- prediction(b201706$m29_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m29_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m29_accuracy_201706 <- (729+1316)/nrow(b201706)
m29_accuracy_201706
#0.6998631

m29_perf_201706 <- performance(m29_pred_201706,"tpr","fpr")

#ROC
plot(m29_perf_201706, lwd=2, colorize=TRUE, main="ROC m29: SVM - Lineal - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m29_perf_precision_201706 <- performance(m29_pred_201706, measure = "prec", x.measure = "rec")
plot(m29_perf_precision_201706, main="m29 SVM Lineal:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m29_perf_acc_201706 <- performance(m29_pred_201706, measure = "acc")
plot(m29_perf_acc_201706, main="m29 SVM - Lineal:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m29_KS_201706 <- round(max(attr(m29_perf_201706,'y.values')[[1]]-attr(m29_perf_201706,'x.values')[[1]])*100, 2)
m29_AUROC_201706 <- round(performance(m29_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m29_Gini_201706 <- (2*m29_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m29_AUROC_201706,"\tKS: ", m29_KS_201706, "\tGini:", m29_Gini_201706, "\tAccuracy:", m29_accuracy_201706,"\n")


#m30: SVM - Lineal  - MACR"

predic_model_30_svm_l_1_201706 <- predict(model_30_svm_l_2, newdata = test.svm_201706,probability=TRUE)



summary(predic_model_30_svm_l_1_201706)



tb30_201706 <- table(b201706$flag_pago,predic_model_30_svm_l_1_201706)
tb30_201706

m30_accuracy_201706 <- (725 + 1332)/nrow(b201706)
m30_accuracy_201706
#0.7039699



#score test data set


b201706$m30_svm_l_score <- attr(predict(model_30_svm_l_2, newdata = test.svm_201706,probability=TRUE),"probabilities")[,1]


m30_pred_201706 <- prediction(b201706$m30_svm_l_score, b201706$flag_pago)
m30_perf_201706 <- performance(m30_pred_201706,"tpr","fpr")

#ROC
plot(m30_perf_201706, lwd=2, colorize=TRUE, main="ROC m30: SVM - Lineal - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m30_perf_precision_201706 <- performance(m30_pred_201706, measure = "prec", x.measure = "rec")
plot(m30_perf_precision_201706, main="m30 SVM - Lineal :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m30_perf_acc_201706 <- performance(m30_pred_201706, measure = "acc")
plot(m30_perf_acc_201706, main="m30 SVM - Lineal :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m30_KS_201706 <- round(max(attr(m30_perf_201706,'y.values')[[1]]-attr(m30_perf_201706,'x.values')[[1]])*100, 2)
m30_AUROC_201706 <- round(performance(m30_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m30_Gini_201706 <- (2*m30_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m30_AUROC_201706,"\tKS: ", m30_KS_201706, "\tGini:", m30_Gini_201706,"\tAccuracy:", m30_accuracy_201706, "\n")



#"m31: SVM - Lineal  - MACR - caret"

#score test data set
b201706$m31_score <- predict(model_31_svm_l_tun_2,type='prob',newdata=b201706)[,2]
m31_pred_201706 <- prediction(b201706$m31_score, b201706$flag_pago)

table(ifelse(b201706$m31_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m31_accuracy_201706 <- (725+1332)/nrow(b201706)
m31_accuracy_201706
#0.7044521

m31_perf_201706 <- performance(m31_pred_201706,"tpr","fpr")

#ROC
plot(m31_perf_201706, lwd=2, colorize=TRUE, main="ROC m31: SVM - Lineal - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m31_perf_precision_201706 <- performance(m31_pred_201706, measure = "prec", x.measure = "rec")
plot(m31_perf_precision_201706, main="m31 SVM - Lineal:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m31_perf_acc_201706 <- performance(m31_pred_201706, measure = "acc")
plot(m31_perf_acc_201706, main="m31 SVM - Lineal:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m31_KS_201706 <- round(max(attr(m31_perf_201706,'y.values')[[1]]-attr(m31_perf_201706,'x.values')[[1]])*100, 2)
m31_AUROC_201706 <- round(performance(m31_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m31_Gini_201706 <- (2*m31_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m31_AUROC_201706,"\tKS: ", m31_KS_201706, "\tGini:", m31_Gini_201706, "\tAccuracy:", m31_accuracy_201706,"\n")


#"m32: SVM - Polinomial - MASR"

predic_model_32_svm_p_1_201706 <- predict(model_32_svm_p_1, newdata = test.svm_201706,probability=TRUE)

#predic_model_28_svm_l_1_pr <- attr(predic_model_28_svm_l_1, "probabilities")[,1]

summary(predic_model_32_svm_p_1_201706)



tb32_201706 <- table(b201706$flag_pago,predic_model_32_svm_p_1_201706)
tb32_201706

m32_accuracy_201706 <- (590 + 1520)/nrow(b201706)
m32_accuracy_201706
#0.7221081

#score test data set


b201706$m32_svm_p_score <- attr(predict(model_32_svm_p_1, newdata = test.svm_201706,probability=TRUE),"probabilities")[,1]



m32_pred_201706 <- prediction(b201706$m32_svm_p_score, b201706$flag_pago)
m32_perf_201706 <- performance(m32_pred_201706,"tpr","fpr")

#ROC
plot(m32_perf_201706, lwd=2, colorize=TRUE, main="ROC m32: SVM - Polinomial Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m32_perf_precision_201706 <- performance(m32_pred_201706, measure = "prec", x.measure = "rec")
plot(m32_perf_precision_201706, main="m32 SVm Polinomial :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m32_perf_acc_201706 <- performance(m32_pred_201706, measure = "acc")
plot(m32_perf_acc_201706, main="m32 SVM Polinomial :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m32_KS_201706 <- round(max(attr(m32_perf_201706,'y.values')[[1]]-attr(m32_perf_201706,'x.values')[[1]])*100, 2)
m32_AUROC_201706 <- round(performance(m32_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m32_Gini_201706 <- (2*m32_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m32_AUROC_201706,"\tKS: ", m32_KS_201706, "\tGini:", m32_Gini_201706, "\tAccuracy:", m32_accuracy_201706,"\n")



#"m33: SVM - Polinomial - MASR - caret"

#score test data set
b201706$m33_score <- predict(model_33_svm_p_tun_1,type='prob',newdata=b201706)[2]
m33_pred_201706 <- prediction(b201706$m33_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m33_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m33_accuracy_201706 <- (516+1575)/nrow(b201706)
m33_accuracy_201706
#0.7156057

m33_perf_201706 <- performance(m33_pred_201706,"tpr","fpr")

#ROC
plot(m33_perf_201706, lwd=2, colorize=TRUE, main="ROC m33: SVM - Polinomial - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m33_perf_precision_201706 <- performance(m33_pred_201706, measure = "prec", x.measure = "rec")
plot(m33_perf_precision_201706, main="m33 SVM Polinomial:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m33_perf_acc_201706 <- performance(m33_pred, measure = "acc")
plot(m33_perf_acc_201706, main="m33 SVM - Polinomial:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m33_KS_201706 <- round(max(attr(m33_perf_201706,'y.values')[[1]]-attr(m33_perf_201706,'x.values')[[1]])*100, 2)
m33_AUROC_201706 <- round(performance(m33_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m33_Gini_201706 <- (2*m33_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m33_AUROC_201706,"\tKS: ", m33_KS_201706, "\tGini:", m33_Gini_201706, "\tAccuracy:", m33_accuracy_201706,"\n")



#"m34: SVM - Polinomial  - MACR"


predic_model_34_svm_p_1_201706 <- predict(model_34_svm_l_2, newdata = test.svm_201706,probability=TRUE)

#predic_model_30_svm_l_1_pr <- attr(predic_model_30_svm_l_1, "probabilities")[,1]

summary(predic_model_34_svm_p_1_201706)



tb34_201706 <- table(b201706$flag_pago,predic_model_34_svm_p_1_201706)
tb34_2017006

m34_accuracy_201706 <- (610 + 1520)/nrow(b201706)
m34_accuracy_201706
#0.7289528



#score test data set


b201706$m34_svm_p_score <- attr(predict(model_34_svm_l_2, newdata = test.svm_201706,probability=TRUE),"probabilities")[,1]


m34_pred_201706 <- prediction(b201706$m34_svm_p_score, b201706$flag_pago)
m34_perf_201706 <- performance(m34_pred_201706,"tpr","fpr")

#ROC
plot(m34_perf_201706, lwd=2, colorize=TRUE, main="ROC m34: SVM - Polinomial - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m34_perf_precision_201706 <- performance(m34_pred_201706, measure = "prec", x.measure = "rec")
plot(m34_perf_precision_201706, main="m34 SVM - Polinomial :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m34_perf_acc_201706 <- performance(m34_pred_201706, measure = "acc")
plot(m34_perf_acc_201706, main="m34 SVM - Polinomial :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m34_KS_201706 <- round(max(attr(m34_perf_201706,'y.values')[[1]]-attr(m34_perf_201706,'x.values')[[1]])*100, 2)
m34_AUROC_201706 <- round(performance(m34_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m34_Gini_20706 <- (2*m34_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m34_AUROC_201706,"\tKS: ", m34_KS_201706, "\tGini:", m34_Gini_20706,"\tAccuracy:", m34_accuracy_201706, "\n")



#"m35: SVM - Polinomial  - MACR - caret"



#score test data set
b201706$m35_score <- predict(model_35_svm_p_tun_2,type='prob',newdata=b201706)[,2]
m35_pred_201706 <- prediction(b201706$m35_score, b201706$flag_pago)

table(ifelse(b201706$m35_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m35_accuracy_201706 <- (725+1332)/nrow(b201706)
m35_accuracy_201706
#0.7044521

m35_perf_201706 <- performance(m35_pred_201706,"tpr","fpr")

#ROC
plot(m35_perf_201706, lwd=2, colorize=TRUE, main="ROC m35: SVM - Polinomial - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m35_perf_precision_201706 <- performance(m35_pred_201706, measure = "prec", x.measure = "rec")
plot(m35_perf_precision_201706, main="m35 SVM - Polinomial:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m35_perf_acc_201706 <- performance(m35_pred_201706, measure = "acc")
plot(m35_perf_acc_201706, main="m35 SVM - Polinomial:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m35_KS_201706 <- round(max(attr(m35_perf_201706,'y.values')[[1]]-attr(m35_perf_201706,'x.values')[[1]])*100, 2)
m35_AUROC_201706 <- round(performance(m35_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m35_Gini_201706 <- (2*m35_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m35_AUROC_201706,"\tKS: ", m35_KS_201706, "\tGini:", m35_Gini_201706, "\tAccuracy:", m35_accuracy_201706,"\n")



#"m36: SVM - Radial - MASR"


predic_model_36_svm_r_1_201706 <- predict(model_36_svm_r_1, newdata = test.svm_201706,probability=TRUE)

#predic_model_28_svm_l_1_pr <- attr(predic_model_28_svm_l_1, "probabilities")[,1]

summary(predic_model_36_svm_r_1_201706)



tb36_201706 <- table(b201706$flag_pago,predic_model_36_svm_r_1_201706)
tb36_201706

m36_accuracy_201706 <- (617 + 1511)/nrow(b201706)
m36_accuracy_201706
#0.7282683

#score test data set


b201706$m36_svm_r_score <- attr(predict(model_36_svm_r_1, newdata = test.svm_201706,probability=TRUE),"probabilities")[,1]



m36_pred_201706 <- prediction(b201706$m36_svm_r_score, b201706$flag_pago)
m36_perf_201706 <- performance(m36_pred_201706,"tpr","fpr")

#ROC
plot(m36_perf_201706, lwd=2, colorize=TRUE, main="ROC m36: SVM - Radial Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m36_perf_precision_201706 <- performance(m36_pred_201706, measure = "prec", x.measure = "rec")
plot(m36_perf_precision_201706, main="m36 SVm Radial :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m36_perf_acc_201706 <- performance(m36_pred_201706, measure = "acc")
plot(m36_perf_acc_201706, main="m36 SVM Radial :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m36_KS_201706 <- round(max(attr(m36_perf_201706,'y.values')[[1]]-attr(m36_perf_201706,'x.values')[[1]])*100, 2)
m36_AUROC_201706 <- round(performance(m36_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m36_Gini_201706 <- (2*m36_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m36_AUROC_201706,"\tKS: ", m36_KS_201706, "\tGini:", m36_Gini_201706, "\tAccuracy:", m36_accuracy_201706,"\n")



#"m37: SVM - Radial - MASR - caret"


#score test data set
b201706$m37_score <- predict(model_37_svm_r_tun_1,type='prob',newdata=b201706)[2]
m37_pred_201706 <- prediction(b201706$m37_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m37_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m37_accuracy_201706 <- (547+1579)/nrow(b201706)
m37_accuracy_201706
#0.7275838

m37_perf_201706 <- performance(m37_pred_201706,"tpr","fpr")

#ROC
plot(m37_perf_201706, lwd=2, colorize=TRUE, main="ROC m37: SVM - Radial - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m37_perf_precision_201706 <- performance(m37_pred_201706, measure = "prec", x.measure = "rec")
plot(m37_perf_precision_201706, main="m37 SVM Radial:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m37_perf_acc_201706 <- performance(m37_pred_201706, measure = "acc")
plot(m37_perf_acc_201706, main="m37 SVM - Radial:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m37_KS_201706 <- round(max(attr(m37_perf_201706,'y.values')[[1]]-attr(m37_perf_201706,'x.values')[[1]])*100, 2)
m37_AUROC_201706 <- round(performance(m37_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m37_Gini_201706 <- (2*m37_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m37_AUROC_201706,"\tKS: ", m37_KS_201706, "\tGini:", m37_Gini_201706, "\tAccuracy:", m37_accuracy_201706,"\n")



#"m38: SVM - Radial  - MACR"

predic_model_38_svm_r_1_201706 <- predict(model_38_svm_r_2, newdata = test.svm_201706,probability=TRUE)

#predic_model_30_svm_l_1_pr <- attr(predic_model_30_svm_l_1, "probabilities")[,1]

summary(predic_model_38_svm_r_1_201706)



tb38_201706 <- table(b201706$flag_pago,predic_model_38_svm_r_1_201706)
tb38_201706

m38_accuracy_201706 <- (623 + 1526)/nrow(b201706)
m38_accuracy_201706
#0.7354552



#score test data set


b201706$m38_svm_r_score <- attr(predict(model_38_svm_r_2, newdata = test.svm_201706,probability=TRUE),"probabilities")[,1]


m38_pred_201706 <- prediction(b201706$m38_svm_r_score, b201706$flag_pago)
m38_perf_201706 <- performance(m38_pred_201706,"tpr","fpr")

#ROC
plot(m38_perf_201706, lwd=2, colorize=TRUE, main="ROC m38: SVM - Radial - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m38_perf_precision_201706 <- performance(m38_pred_201706, measure = "prec", x.measure = "rec")
plot(m38_perf_precision_201706, main="m38 SVM - Radial :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m38_perf_acc_201706 <- performance(m38_pred_201706, measure = "acc")
plot(m38_perf_acc_201706, main="m38 SVM - Radial :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m38_KS_201706 <- round(max(attr(m38_perf_201706,'y.values')[[1]]-attr(m38_perf_201706,'x.values')[[1]])*100, 2)
m38_AUROC_201706 <- round(performance(m38_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m38_Gini_201706 <- (2*m38_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m38_AUROC_201706,"\tKS: ", m38_KS_201706, "\tGini:", m38_Gini_201706,"\tAccuracy:", m38_accuracy_201706, "\n")


#"m39: SVM - Radial  - MACR - caret"

#score test data set
b201706$m39_score <- predict(model_39_svm_r_tun_2,type='prob',newdata=b201706)[,2]
m39_pred_201706 <- prediction(b201706$m39_score, b201706$flag_pago)

table(ifelse(b201706$m39_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m39_accuracy_201706 <- ( 523+1596)/nrow(b201706)
m39_accuracy_201706
#0.7256849

m39_perf_201706 <- performance(m39_pred_201706,"tpr","fpr")

#ROC
plot(m39_perf_201706, lwd=2, colorize=TRUE, main="ROC m39: SVM - Radial - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m39_perf_precision_201706 <- performance(m39_pred_201706, measure = "prec", x.measure = "rec")
plot(m39_perf_precision_201706, main="m39 SVM - Radial:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m39_perf_acc_201706 <- performance(m39_pred_201706, measure = "acc")
plot(m39_perf_acc_201706, main="m39 SVM - Radial:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m39_KS_201706 <- round(max(attr(m39_perf_201706,'y.values')[[1]]-attr(m39_perf_201706,'x.values')[[1]])*100, 2)
m39_AUROC_201706 <- round(performance(m39_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m39_Gini_201706 <- (2*m39_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m39_AUROC_201706,"\tKS: ", m39_KS_201706, "\tGini:", m39_Gini_201706, "\tAccuracy:", m39_accuracy_201706,"\n")









#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VII         COMPARACIÓN DE MEJORES PERFORMARCEN EN OOT
#---------------------------------------------------------------------------------------
########################################################################################










#####################################
### parte I
#####################################

# tABLA DE PERFORMANCE - OOS
models <- c('m1:Regresión Logística - MASR', 'm2:Regresión Logística corte optimo - MASR',
            'm3:Regresión Logística - MASR - caret','m4:Regresión Logística - MACR', 
            'm5:Regresión Logística - MACR - caret', "m6: AD - cart - MASR",
            "m7: AD - cart - MASR - caret", "m8: AD - cart - MACR",
            "m9: AD - cart - MACR - caret", "m10: AD - ctree - MAsR",
            "m11: AD - ctree - MASR - caret", "m12: AD - ctree - MACR",
            "m13: AD - ctree - MACR - caret", "m14: RF - cart - MAsR",
            "m15: RF - cart - MASR - caret", "m16: RF - cart - MACR",
            "m17: RF - cart - MACR - caret", "m18: GBM - MAsR",
            "m19: GBM - MASR - caret","m20: GBM - MACR", "m21: GBM - MACR - caret")

# AUCs
models_AUC <- c(m1_AUROC, m2_AUROC, m3_AUROC, m4_AUROC, m5_AUROC, m6_AUROC, m7_AUROC, m8_AUROC, m9_AUROC, m10_AUROC, 
                m11_AUROC, m12_AUROC, m13_AUROC, m14_AUROC, m15_AUROC, m16_AUROC, m17_AUROC, m18_AUROC, m19_AUROC, m20_AUROC, 
                m21_AUROC)
# KS
models_KS <- c(m1_KS, m2_KS, m3_KS, m4_KS, m5_KS, m6_KS, m7_KS, m8_KS, m9_KS, m10_KS, m11_KS, m12_KS, m13_KS, m14_KS, m15_KS, 
               m16_KS, m17_KS, m18_KS, m19_KS, m20_KS, m21_KS)

# Gini
models_Gini <- c(m1_Gini, m2_Gini, m3_Gini, m4_Gini, m5_Gini, m6_Gini, m7_Gini, m8_Gini, m9_Gini, m10_Gini, 
                 m11_Gini, m12_Gini, m13_Gini, m14_Gini, m15_Gini, m16_Gini, m17_Gini, m18_Gini, m19_Gini, m20_Gini, 
                 m21_Gini)


# Accuraccy

models_accuracy <- c(round(m1_accuracy,2), round(m2_accuracy,2), round(m3_accuracy,2), round(m4_accuracy,2), round(m5_accuracy,2), round(m6_accuracy,2), round(m7_accuracy,2), round(m8_accuracy,2), round(m9_accuracy,2), 
                     round(m10_accuracy,2), round(m11_accuracy,2), round(m12_accuracy,2), round(m13_accuracy,2), round(m14_accuracy,2), round(m15_accuracy,2), round(m16_accuracy,2), round(m17_accuracy,2), 
                     round(m18_accuracy,2), round(m19_accuracy,2), round(m20_accuracy,2), round(m21_accuracy,2))


#Tiempo

models_time <- c(round(time_m1,2), "NA", round(time_m3,2), round(time_m4,2), round(time_m5,2), round(time_m6,2), round(time_m7,2), round(time_m8,2), round(time_m9,2), round(time_m10,2), round(time_m11,2), round(time_m12,2), round(time_m13,2),
                 round(time_m14,2), round(time_m15,2), round(time_m16,2), round(time_m17,2), round(time_m18,2), round(time_m19,2), round(time_m20,2), round(time_m21,2) )

# Juntando todo
model_performance_metric <- as.data.frame(cbind(models, models_AUC, models_KS, models_Gini, models_accuracy, models_time))


model_performance_metric <- model_performance_metric[order(models_Gini, decreasing = T),] 

# Colnames 
colnames(model_performance_metric) <- c("Modelos", "AUC", "KS", "Gini","Accuracy", "Tiempo_proceso_Modelo_minuto")

# Display Performance Reports
kable(model_performance_metric, caption ="Comparación de modelos - OOS")








#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VI          MODELOS EN OOT
#---------------------------------------------------------------------------------------
########################################################################################


#############################################################
# PERIODO - 201706
#############################################################


#'m1:Regresión Logística - MASR'

set.seed(1992)
predic_model_rl_1_201706 <- predict(model_rl_1,type='response',b201706)

tb1_201706 <- table(b201706$flag_pago,ifelse(predic_model_rl_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb1_201706

m1_accuracy_201706 <- (121 + 280)/nrow(b201706)
m1_accuracy_201706

set.seed(1992)
#score test data set
b201706$m1_score_201706 <- predict(model_rl_1,type='response',b201706)
m1_pred_201706 <- prediction(b201706$m1_score_201706, b201706$flag_pago)
m1_perf_201706 <- performance(m1_pred_201706,"tpr","fpr")

#ROC
plot(m1_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision_201706 <- performance(m1_pred_201706, measure = "prec", x.measure = "rec")
plot(m1_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc_201706 <- performance(m1_pred_201706, measure = "acc")
plot(m1_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS_201706 <- round(max(attr(m1_perf_201706,'y.values')[[1]]-attr(m1_perf_201706,'x.values')[[1]])*100, 2)
m1_AUROC_201706 <- round(performance(m1_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini_201706 <- (2*m1_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m1_AUROC_201706,"\tKS: ", m1_KS_201706, "\tGini:", m1_Gini_201706, "\tAccuracy:", m1_accuracy_201706,"\n")



#'m2:Regresión Logística corte optimo - MASR'



tb2_201706 <- table(b201706$flag_pago,ifelse(predic_model_rl_1_201706>optCutOff,"X1_Bueno","X0_Malo"))
tb2_201706
m2_accuracy_201706 <-  ( 121+ 280)/nrow(b201706)
m2_accuracy_201706
#0.7316906

set.seed(1992)
#score test data set
b201706$m2_score <- predict(model_rl_1,type='response',b201706)
m2_pred_201706 <- prediction(ifelse(b201706$m1_score>optCutOff,1,0), b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf_201706 <- performance(m2_pred_201706,"tpr","fpr")

#ROC
plot(m2_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision_201706 <- performance(m2_pred_201706, measure = "prec", x.measure = "rec")
plot(m2_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc_201706 <- performance(m2_pred_201706, measure = "acc")
plot(m2_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS_201706 <- round(max(attr(m2_perf_201706,'y.values')[[1]]-attr(m2_perf_201706,'x.values')[[1]])*100, 2)
m2_AUROC_201706 <- round(performance(m2_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini_201706 <- (2*m2_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m2_AUROC_201706,"\tKS: ", m2_KS_201706, "\tGini:", m2_Gini_201706, "\tAccuracy:", m2_accuracy_201706,"\n")



#'m3:Regresión Logística - MASR - caret'

#score test data set
b201706$m3_score <- predict(model_rl_3_tun,type='prob',newdata=b201706)[2]
m3_pred_201706 <- prediction(b201706$m3_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3_201706 <- table(ifelse(b201706$m3_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb3_201706

m3_accuracy_201706 <- (121+280)/nrow(b201706)
m3_accuracy_201706

m3_perf_201706 <- performance(m3_pred_201706,"tpr","fpr")

#ROC
plot(m3_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic - 201706 Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision_201706 <- performance(m3_pred_201706, measure = "prec", x.measure = "rec")
plot(m3_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc_201706 <- performance(m3_pred_201706, measure = "acc")
plot(m3_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS_201706 <- round(max(attr(m3_perf_201706,'y.values')[[1]]-attr(m3_perf_201706,'x.values')[[1]])*100, 2)
m3_AUROC_201706 <- round(performance(m3_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini_201706 <- (2*m3_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m3_AUROC_201706,"\tKS: ", m3_KS_201706, "\tGini:", m3_Gini_201706,"\tAccuracy:", m3_accuracy_201706, "\n")


#'m4:Regresión Logística - MACR'

predic_model_rl_4_201706 <- predict(model_rl_4, newdata = b201706,type = "response")

summary(predic_model_rl_4_201706)


tb4_201706 <- table(b201706$flag_pago,ifelse(predic_model_rl_4_201706>0.5,"X1_Bueno","X0_Malo"))
tb4_201706

(m4_accuracy_201706 <- (122 +  280)/nrow(b201706))

#0.6711185

#score test data set
b201706$m4_score <- predict(model_rl_4,type='response',b201706)
m4_pred_201706 <- prediction(b201706$m4_score, b201706$flag_pago)
m4_perf_201706 <- performance(m4_pred_201706,"tpr","fpr")

#ROC
plot(m4_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision_201706 <- performance(m4_pred_201706, measure = "prec", x.measure = "rec")
plot(m4_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc_201706 <- performance(m4_pred_201706, measure = "acc")
plot(m4_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS_201706 <- round(max(attr(m4_perf_201706,'y.values')[[1]]-attr(m4_perf_201706,'x.values')[[1]])*100, 2)
m4_AUROC_201706 <- round(performance(m4_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini_201706 <- (2*m4_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m4_AUROC_201706,"\tKS: ", m4_KS_201706, "\tGini:", m4_Gini_201706,"\tAccuracy:", m4_accuracy_201706, "\n")



#'m5:Regresión Logística - MACR - caret'

#score test data set
b201706$m5_score <- predict(model_rl_5_tun,type='prob',newdata=b201706)[2]
m5_pred_201706 <- prediction(b201706$m5_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb201706 <- table(ifelse(b201706$m5_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb201706

m5_accuracy_201706 <- (122+280)/nrow(b201706)
m5_accuracy_201706
#0.6711185

m5_perf_201706 <- performance(m5_pred_201706,"tpr","fpr")

#ROC
plot(m5_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision_201706 <- performance(m5_pred_201706, measure = "prec", x.measure = "rec")
plot(m5_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc_201706 <- performance(m5_pred_201706, measure = "acc")
plot(m5_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS_201706 <- round(max(attr(m5_perf_201706,'y.values')[[1]]-attr(m5_perf_201706,'x.values')[[1]])*100, 2)
m5_AUROC_201706 <- round(performance(m5_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini_201706 <- (2*m5_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m5_AUROC_201706,"\tKS: ", m5_KS_201706, "\tGini:", m5_Gini_201706, "\tAccuracy:", m5_accuracy_201706,"\n")


#"m6: AD - cart - MASR"

predic_model_6_ad_1_201706 <- predict(model_6_ad_1, newdata = b201706,type = "prob")[,2]

summary(predic_model_6_ad_1_201706)


tb6_201706 <- table(b201706$flag_pago,ifelse(predic_model_6_ad_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb6_201706

m6_accuracy_201706 <- (128 + 273)/nrow(b201706)
m6_accuracy_201706
#0.6694491

#score test data set
b201706$m6_ad_score <- predict(model_6_ad_1,type='prob',b201706)[,2]
m6_pred_201706 <- prediction(b201706$m6_ad_score, b201706$flag_pago)
m6_perf_201706 <- performance(m6_pred_201706,"tpr","fpr")

#ROC
plot(m6_perf_201706, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision_201706 <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision_201706, main="m6 Árbol de decisión - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc_201706 <- performance(m6_pred_201706, measure = "acc")
plot(m6_perf_acc_201706, main="m6 Árbol de decisión - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS_201706 <- round(max(attr(m6_perf_201706,'y.values')[[1]]-attr(m6_perf_201706,'x.values')[[1]])*100, 2)
m6_AUROC_201706 <- round(performance(m6_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini_201706 <- (2*m6_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m6_AUROC_201706,"\tKS: ", m6_KS_201706, "\tGini:", m6_Gini_201706, "\tAccuracy:", m6_accuracy_201706,"\n")


#"m7: AD - cart - MASR - caret"

#score test data set
b201706$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=b201706)[2]
m7_pred_201706 <- prediction(b201706$m7_score, b201706$flag_pago)

tb7_201706 <- table(ifelse(b201706$m7_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb7_201706

m7_accuracy_201706 <- (123+ 277)/nrow(b201706)
m7_accuracy_201706
#0.6677796

m7_perf_201706 <- performance(m7_pred_201706,"tpr","fpr")

#ROC
plot(m7_perf_201706, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision_201706 <- performance(m7_pred_201706, measure = "prec", x.measure = "rec")
plot(m7_perf_precision_201706, main="m7 Arbol de decision - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc_201706 <- performance(m7_pred_201706, measure = "acc")
plot(m7_perf_acc_201706, main="m7 Arbol tuneado:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m7_KS_201706 <- round(max(attr(m7_perf_201706,'y.values')[[1]]-attr(m7_perf_201706,'x.values')[[1]])*100, 2)
m7_AUROC_201706 <- round(performance(m7_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini_201706 <- (2*m7_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")


#"m8: AD - cart - MACR"

predic_model_8_ad_2_201706 <- predict(model_8_ad_2, newdata = b201706,type = "prob")[,2]

summary(predic_model_8_ad_2_201706)


tb8_201706 <- table(b201706$flag_pago,ifelse(predic_model_8_ad_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb8_201706

m8_accuracy_201706 <- (118 +  283)/nrow(b201706)
m8_accuracy_201706
#0.6694491

#score test data set
b201706$m8_score <- predict(model_8_ad_2,type='prob',b201706)[,2]
m8_pred_201706 <- prediction(b201706$m8_score, b201706$flag_pago)
m8_perf_201706 <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf_201706, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision_201706 <- performance(m8_pred_201706, measure = "prec", x.measure = "rec")
plot(m8_perf_precision_201706, main="m8 Arbol de decision - rpart:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m8_perf_acc_201706 <- performance(m8_pred_201706, measure = "acc")
plot(m8_perf_acc_201706, main="m8 Arbol de decision - rpart:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m8_KS_201706 <- round(max(attr(m8_perf_201706,'y.values')[[1]]-attr(m8_perf_201706,'x.values')[[1]])*100, 2)
m8_AUROC_201706 <- round(performance(m8_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini_201706 <- (2*m8_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m8_AUROC_201706,"\tKS: ", m8_KS_201706, "\tGini:", m8_Gini_201706,"\tAccuracy:", m8_accuracy_201706, "\n")


#"m9: AD - cart - MACR - caret"

#score test data set
b201706$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=b201706)[,2]
m9_pred_201706 <- prediction(b201706$m9_score, b201706$flag_pago)


tb9_201706 <- table(ifelse(b201706$m9_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb9_201706

m9_accuracy_201706 <- (117+279)/nrow(b201706)
m9_accuracy_201706
#0.6611018

m9_perf_201706 <- performance(m9_pred_201706,"tpr","fpr")

#ROC
plot(m9_perf_201706, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision_201706 <- performance(m9_pred_201706, measure = "prec", x.measure = "rec")
plot(m9_perf_precision_201706, main="m9 Arbol de decision - caret:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m9_perf_acc_201706 <- performance(m9_pred_201706, measure = "acc")
plot(m9_perf_acc_201706, main="m9 Arbol de decision - caret:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m9_KS_201706 <- round(max(attr(m9_perf_201706,'y.values')[[1]]-attr(m9_perf_201706,'x.values')[[1]])*100, 2)
m9_AUROC_201706 <- round(performance(m9_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini_201706 <- (2*m9_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m9_AUROC_201706,"\tKS: ", m9_KS_201706, "\tGini:", m9_Gini_201706, "\tAccuracy:", m9_accuracy_201706,"\n")


#"m10: AD - ctree - MAsR"


predic_model_10_ad_1_201706 <- sapply(predict(model_10_ad_1, newdata = b201706,type = "prob"),'[[',2)

summary(predic_model_10_ad_1_201706)

tb10_201706 <- table(b201706$flag_pago,ifelse(predic_model_10_ad_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb10_201706

m10_accuracy_201706 <- (122 + 278)/nrow(b201706)
m10_accuracy_201706
#0.6677796

#score test data set
b201706$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = b201706,type = "prob")))))[,2]

m10_pred_201706 <- prediction(b201706$m10_ad_score, b201706$flag_pago)
m10_perf_201706 <- performance(m10_pred_201706,"tpr","fpr")

#ROC
plot(m10_perf_201706, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision_201706 <- performance(m10_pred_201706, measure = "prec", x.measure = "rec")
plot(m10_perf_precision_201706, main="m6 Árbol de decisión - CTREE:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m10_perf_acc_201706 <- performance(m10_pred_201706, measure = "acc")
plot(m10_perf_acc_201706, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m10_KS_201706 <- round(max(attr(m10_perf_201706,'y.values')[[1]]-attr(m10_perf_201706,'x.values')[[1]])*100, 2)
m10_AUROC_201706 <- round(performance(m10_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini_201706 <- (2*m10_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m10_AUROC_201706,"\tKS: ", m10_KS_201706, "\tGini:", m10_Gini_201706, "\tAccuracy:", m10_accuracy_201706,"\n")


#"m11: AD - ctree - MASR - caret"


#score test data set
b201706$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=b201706)[2]
m11_pred_201706 <- prediction(b201706$m11_score, b201706$flag_pago)

tb11_201706 <- table(ifelse(b201706$m11_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb11_201706

m11_accuracy_201706 <- (119+281)/nrow(b201706)
m11_accuracy_201706
#0.6677796

m11_perf_201706 <- performance(m11_pred_201706,"tpr","fpr")

#ROC
plot(m11_perf_201706, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision_201706 <- performance(m11_pred_201706, measure = "prec", x.measure = "rec")
plot(m11_perf_precision_201706, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m11_perf_acc_201706 <- performance(m11_pred_201706, measure = "acc")
plot(m11_perf_acc_201706, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m11_KS_201706 <- round(max(attr(m11_perf_201706,'y.values')[[1]]-attr(m11_perf_201706,'x.values')[[1]])*100, 2)
m11_AUROC_201706 <- round(performance(m11_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini_201706 <- (2*m11_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m11_AUROC_201706,"\tKS: ", m11_KS_201706, "\tGini:", m11_Gini_201706, "\tAccuracy:", m11_accuracy_201706,"\n")



#"m12: AD - ctree - MACR"

predic_model_12_ad_2_201706 <- sapply(predict(model_12_ad_2, newdata = b201706,type = "prob"),'[[',2)

summary(predic_model_12_ad_2_201706)


tb12_201706 <- table(b201706$flag_pago,ifelse(predic_model_12_ad_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb12_201706

m12_accuracy_201706 <- (130 + 276)/nrow(b201706)
m12_accuracy_201706
#0.6777963

#score test data set
b201706$m12_score <- sapply(predict(model_12_ad_2, newdata = b201706,type = "prob"),'[[',2)
m12_pred_201706 <- prediction(b201706$m12_score, b201706$flag_pago)
m12_perf_201706 <- performance(m12_pred_201706,"tpr","fpr")

#ROC
plot(m12_perf_201706, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision_201706 <- performance(m12_pred_201706, measure = "prec", x.measure = "rec")
plot(m12_perf_precision_201706, main="m12 Arbol de decision - CTREE :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m12_perf_acc_201706 <- performance(m12_pred_201706, measure = "acc")
plot(m12_perf_acc_201706, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m12_KS_201706 <- round(max(attr(m12_perf_201706,'y.values')[[1]]-attr(m12_perf_201706,'x.values')[[1]])*100, 2)
m12_AUROC_201706 <- round(performance(m12_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini_201706 <- (2*m12_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m12_AUROC_201706,"\tKS: ", m12_KS_201706, "\tGini:", m12_Gini_201706,"\tAccuracy:", m12_accuracy_201706, "\n")



#"m13: AD - ctree - MACR - caret"

#score test data set
b201706$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=b201706)[,2]
m13_pred_201706 <- prediction(b201706$m13_score, b201706$flag_pago)


table(ifelse(b201706$m13_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m13_accuracy_201706 <- (131+268)/nrow(b201706)
m13_accuracy_201706
#0.6661102

m13_perf_201706 <- performance(m13_pred_201706,"tpr","fpr")

#ROC
plot(m13_perf_201706, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision_201706 <- performance(m13_pred_201706, measure = "prec", x.measure = "rec")
plot(m13_perf_precision_201706, main="m13 Arbol de decision - CTREE:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m13_perf_acc_201706 <- performance(m13_pred_201706, measure = "acc")
plot(m13_perf_acc_201706, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m13_KS_201706 <- round(max(attr(m13_perf_201706,'y.values')[[1]]-attr(m13_perf_201706,'x.values')[[1]])*100, 2)
m13_AUROC_201706 <- round(performance(m13_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini_201706 <- (2*m13_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m13_AUROC_201706,"\tKS: ", m13_KS_201706, "\tGini:", m13_Gini_201706, "\tAccuracy:", m13_accuracy_201706,"\n")


#"m14: RF - cart - MAsR"



predic_model_14_rf_1_201706 <- predict(model_14_rf_1, newdata = b201706,type = "prob")[,2]

summary(predic_model_14_rf_1_201706)

tb14_201706 <- table(b201706$flag_pago,ifelse(predic_model_14_rf_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb14_201706

m14_accuracy_20176 <- (124 + 275)/nrow(b201706)
m14_accuracy_20176
#0.6661102

#score test data set
b201706$m14_rf_score <- predict(model_14_rf_1, newdata = b201706,type = "prob")[,2]

m14_pred_201706 <- prediction(b201706$m14_rf_score, b201706$flag_pago)
m14_perf_201706 <- performance(m14_pred_201706,"tpr","fpr")

#ROC
plot(m14_perf_201706, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision_201706 <- performance(m14_pred_201706, measure = "prec", x.measure = "rec")
plot(m14_perf_precision_201706, main="m14 Random Forest:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m14_perf_acc_201706 <- performance(m14_pred_201706, measure = "acc")
plot(m14_perf_acc_201706, main="m14 Random Forest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m14_KS_201706 <- round(max(attr(m14_perf_201706,'y.values')[[1]]-attr(m14_perf_201706,'x.values')[[1]])*100, 2)
m14_AUROC_201706 <- round(performance(m14_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini_201706 <- (2*m14_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m14_AUROC_201706,"\tKS: ", m14_KS_201706, "\tGini:", m14_Gini_201706, "\tAccuracy:", m14_accuracy_20176,"\n")


#"m15: RF - cart - MASR - caret"

#score test data set
b201706$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=b201706)[2]
m15_pred_201706 <- prediction(b201706$m15_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m15_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m15_accuracy_201706 <- (641+1501)/nrow(b201706)
m15_accuracy_201706
#0.7330595

m15_perf_201706 <- performance(m15_pred_201706,"tpr","fpr")

#ROC
plot(m15_perf_201706, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision_201706 <- performance(m15_pred_201706, measure = "prec", x.measure = "rec")
plot(m15_perf_precision_201706, main="m15 RandomForest:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m15_perf_acc_201706 <- performance(m15_pred_201706, measure = "acc")
plot(m15_perf_acc_201706, main="m15 RandomForest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m15_KS_201706 <- round(max(attr(m15_perf_201706,'y.values')[[1]]-attr(m15_perf_201706,'x.values')[[1]])*100, 2)
m15_AUROC_201706 <- round(performance(m15_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini_201706 <- (2*m15_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m15_AUROC_201706,"\tKS: ", m15_KS_201706, "\tGini:", m15_Gini_201706, "\tAccuracy:", m15_accuracy_201706,"\n")


#"m16: RF - cart - MACR"

predic_model_16_rf_2_201706 <- predict(model_16_rf_2, newdata = b201706,type = "prob")[,2]

summary(predic_model_16_rf_2_201706)

tb16_201706 <- table(b201706$flag_pago,ifelse(predic_model_16_rf_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb16_201706


m16_accuracy_201706 <- (640 + 1553)/nrow(b201706)
m16_accuracy_201706
#0.7510274

#score test data set

b201706$m16_rf_2_score <- predict(model_16_rf_2, newdata = b201706,type = "prob")[,2]

m16_pred_201706 <- prediction(b201706$m16_rf_2_score, b201706$flag_pago)
m16_perf_201706 <- performance(m16_pred_201706,"tpr","fpr")

#ROC
plot(m16_perf_201706, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision_201706 <- performance(m16_pred_201706, measure = "prec", x.measure = "rec")
plot(m16_perf_precision_201706, main="m16 RandomForest :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m16_perf_acc_201706 <- performance(m16_pred_201706, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS_201706 <- round(max(attr(m16_perf_201706,'y.values')[[1]]-attr(m16_perf_201706,'x.values')[[1]])*100, 2)
m16_AUROC_201706 <- round(performance(m16_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini_201706 <- (2*m16_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m16_AUROC_201706,"\tKS: ", m16_KS_201706, "\tGini:", m16_Gini_201706,"\tAccuracy:", m16_accuracy_201706, "\n")



#"m17: RF - cart - MACR - caret"

#score test data set
b201706$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=b201706)[,2]
m17_pred_201706 <- prediction(b201706$m17_score, b201706$flag_pago)

table(ifelse(b201706$m17_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m17_accuracy_201706 <- (639+1515)/nrow(b201706)
m17_accuracy_201706
#0.7376712

m17_perf_201706 <- performance(m17_pred_201706,"tpr","fpr")

#ROC
plot(m17_perf_201706, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision_201706 <- performance(m17_pred_201706, measure = "prec", x.measure = "rec")
plot(m17_perf_precision_201706, main="m17 RandomForest:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m17_perf_acc_201706 <- performance(m17_pred_201706, measure = "acc")
plot(m17_perf_acc_201706, main="m17 RandomForest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m17_KS_201706 <- round(max(attr(m17_perf_201706,'y.values')[[1]]-attr(m17_perf_201706,'x.values')[[1]])*100, 2)
m17_AUROC_201706 <- round(performance(m17_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini_201706 <- (2*m17_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m17_AUROC_201706,"\tKS: ", m17_KS_201706, "\tGini:", m17_Gini_201706, "\tAccuracy:", m17_accuracy_201706,"\n")


#"m18: GBM - MAsR"

predic_model_18_gbm_1_201706 <- predict(model_18_gbm_1, newdata = b201706,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1_201706, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18_201706 <- table(b201706$flag_pago,ifelse(predic_model_18_gbm_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb18_201706

m18_accuracy_201706 <- (622+1518)/nrow(b201706)
m18_accuracy_201706

#0.7323751

#score test data set
b201706$m18_GBM_score <- predict(model_18_gbm_1, newdata = b201706,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred_201706 <- prediction(b201706$m18_GBM_score, b201706$flag_pago)
m18_perf_201706 <- performance(m18_pred_201706,"tpr","fpr")

#ROC
plot(m18_perf_201706, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision_201706 <- performance(m18_pred_201706, measure = "prec", x.measure = "rec")
plot(m18_perf_precision_201706, main="m18 GBM:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m18_perf_acc_201706 <- performance(m18_pred_201706, measure = "acc")
plot(m18_perf_acc_201706, main="m18 GBM:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m18_KS_201706 <- round(max(attr(m18_perf_201706,'y.values')[[1]]-attr(m18_perf_201706,'x.values')[[1]])*100, 2)
m18_AUROC_201706 <- round(performance(m18_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini_201706 <- (2*m18_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m18_AUROC_201706,"\tKS: ", m18_KS_201706, "\tGini:", m18_Gini_201706, "\tAccuracy:", m18_accuracy_201706,"\n")


#"m19: GBM - MASR - caret"

#score test data set
b201706$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=b201706)[2]
m19_pred_201706 <- prediction(b201706$m19_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m19_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m19_accuracy_201706 <- (630+1508)/nrow(b201706)
m19_accuracy_201706
#0.7316906

m19_perf_201706 <- performance(m19_pred_201706,"tpr","fpr")

#ROC
plot(m19_perf_201706, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision_201706 <- performance(m19_pred_201706, measure = "prec", x.measure = "rec")
plot(m19_perf_precision_201706, main="m19 GBM:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m19_perf_acc_201706 <- performance(m19_pred_201706, measure = "acc")
plot(m19_perf_acc_201706, main="m19 GBM:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m19_KS_201706 <- round(max(attr(m19_perf_201706,'y.values')[[1]]-attr(m19_perf_201706,'x.values')[[1]])*100, 2)
m19_AUROC_201706 <- round(performance(m19_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini_201706 <- (2*m19_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m19_AUROC_201706,"\tKS: ", m19_KS_201706, "\tGini:", m19_Gini_201706, "\tAccuracy:", m19_accuracy_201706,"\n")



#"m20: GBM - MACR"



predic_model_20_gbm_2_201706 <- predict(model_20_gbm_2, newdata = b201706,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2_201706, n=30)


summary(predic_model_20_gbm_2_201706)

tb20_201706 <- table(b201706$flag_pago,ifelse(predic_model_20_gbm_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb20_201706


m20_accuracy_201706 <- (636 + 1514)/nrow(b201706)
m20_accuracy_201706
#0.7363014

#score test data set


b201706$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = b201706,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred_201706 <- prediction(b201706$m20_GBM_2_score, b201706$flag_pago)
m20_perf_201706 <- performance(m20_pred_201706,"tpr","fpr")

#ROC
plot(m20_perf_201706, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision_201706 <- performance(m20_pred_201706, measure = "prec", x.measure = "rec")
plot(m20_perf_precision_201706, main="m20 GBM :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m20_perf_acc_201706 <- performance(m20_pred_201706, measure = "acc")
plot(m20_perf_acc_201706, main="m20 GBM :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m20_KS_201706 <- round(max(attr(m20_perf_201706,'y.values')[[1]]-attr(m20_perf_201706,'x.values')[[1]])*100, 2)
m20_AUROC_201706 <- round(performance(m20_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini_201706 <- (2*m20_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m20_AUROC_201706,"\tKS: ", m20_KS_201706, "\tGini:", m20_Gini_201706,"\tAccuracy:", m20_accuracy_201706, "\n")


#"m21: GBM - MACR - caret"


#score test data set
b201706$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=b201706)[,2]
m21_pred_201706 <- prediction(b201706$m21_score, b201706$flag_pago)

table(ifelse(b201706$m21_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m21_accuracy_201706 <- (638+1513)/nrow(b201706)
m21_accuracy_201706
#0.7366438

m21_perf_201706 <- performance(m21_pred_201706,"tpr","fpr")

#ROC
plot(m21_perf_201706, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision_201706 <- performance(m21_pred_201706, measure = "prec", x.measure = "rec")
plot(m21_perf_precision_201706, main="m21 GBM:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m21_perf_acc_201706 <- performance(m21_pred_201706, measure = "acc")
plot(m21_perf_acc_201706, main="m21 RandomForest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m21_KS_201706 <- round(max(attr(m21_perf_201706,'y.values')[[1]]-attr(m21_perf_201706,'x.values')[[1]])*100, 2)
m21_AUROC_201706 <- round(performance(m21_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini_201706 <- (2*m21_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m21_AUROC_201706,"\tKS: ", m21_KS_201706, "\tGini:", m21_Gini_201706, "\tAccuracy:", m21_accuracy_201706,"\n")



# tABLA DE PERFORMANCE - OOS
models_201706 <- c('m1:Regresión Logística - MASR - 201706', 'm2:Regresión Logística corte optimo - MASR - 201706',
                   'm3:Regresión Logística - MASR - caret - 201706','m4:Regresión Logística - MACR - 201706', 
                   'm5:Regresión Logística - MACR - caret - 201706', "m6: AD - cart - MASR - 201706",
                   "m7: AD - cart - MASR - caret - 201706", "m8: AD - cart - MACR - 201706",
                   "m9: AD - cart - MACR - caret - 201706", "m10: AD - ctree - MAsR - 201706",
                   "m11: AD - ctree - MASR - caret - 201706", "m12: AD - ctree - MACR - 201706",
                   "m13: AD - ctree - MACR - caret - 201706", "m14: RF - cart - MAsR - 201706",
                   "m15: RF - cart - MASR - caret - 201706", "m16: RF - cart - MACR - 201706",
                   "m17: RF - cart - MACR - caret - 201706", "m18: GBM - MAsR - 201706",
                   "m19: GBM - MASR - caret - 201706","m20: GBM - MACR - 201706", "m21: GBM - MACR - caret - 201706")

# AUCs
models_AUC_201706 <- c(m1_AUROC_201706, m2_AUROC_201706, m3_AUROC_201706, m4_AUROC_201706, m5_AUROC_201706, m6_AUROC_201706, m7_AUROC_201706, m8_AUROC_201706, m9_AUROC_201706, m10_AUROC_201706, 
                       m11_AUROC_201706, m12_AUROC_201706, m13_AUROC_201706, m14_AUROC_201706, m15_AUROC_201706, m16_AUROC_201706, m17_AUROC_201706, m18_AUROC_201706, m19_AUROC_201706, m20_AUROC_201706, 
                       m21_AUROC_201706)
# KS
models_KS_201706 <- c(m1_KS_201706, m2_KS_201706, m3_KS_201706, m4_KS_201706, m5_KS_201706, m6_KS_201706, m7_KS_201706, m8_KS_201706, m9_KS_201706, m10_KS_201706, m11_KS_201706, m12_KS_201706, m13_KS_201706, m14_KS_201706, m15_KS_201706, 
                      m16_KS_201706, m17_KS_201706, m18_KS_201706, m19_KS_201706, m20_KS_201706, m21_KS_201706)

# Gini
models_Gini_201706 <- c(m1_Gini_201706, m2_Gini_201706, m3_Gini_201706, m4_Gini_201706, m5_Gini_201706, m6_Gini_201706, m7_Gini_201706, m8_Gini_201706, m9_Gini_201706, m10_Gini_201706, 
                        m11_Gini_201706, m12_Gini_201706, m13_Gini_201706, m14_Gini_201706, m15_Gini_201706, m16_Gini_201706, m17_Gini_201706, m18_Gini_201706, m19_Gini_201706, m20_Gini_201706, 
                        m21_Gini_201706)


# Accuraccy

#models_accuracy_201706 <- c(m1_accuracy_201706, m2_accuracy_201706, m3_accuracy_201706, m4_accuracy_201706, m5_accuracy_201706, m6_accuracy_201706, m7_accuracy_201706, m8_accuracy_201706, m9_accuracy_201706, 
#                     m10_accuracy_201706, m11_accuracy_201706, m12_accuracy_201706, m13_accuracy_201706, m14_accuracy_201706, m15_accuracy_201706, m16_accuracy_201706, m17_accuracy_201706, 
#                     m18_accuracy_201706, m19_accuracy_201706, m20_accuracy_201706, m21_accuracy_201706)


# Juntando todo
model_performance_metric_201706 <- as.data.frame(cbind(models_201706, models_AUC_201706, models_KS_201706, models_Gini_201706))

model_performance_metric_201706 <- model_performance_metric_201706[order(models_Gini_201706, decreasing = T),] 


# Colnames 
colnames(model_performance_metric_201706) <- c("Modelos_201706", "AUC_201706", "KS_201706", "Gini_201706","Accuracy_201706")

# Display Performance Reports
kable(model_performance_metric_201706, caption ="Comparación de modelos - OOS - 201706")


#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VI          MODELOS EN OOT
#---------------------------------------------------------------------------------------
########################################################################################


#############################################################
# PERIODO - 201707
#############################################################


#'m1:Regresión Logística - MASR'

set.seed(1992)
predic_model_rl_1_201707 <- predict(model_rl_1,type='response',b201707)

tb1_201707 <- table(b201707$flag_pago,ifelse(predic_model_rl_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb1_201707

m1_accuracy_201707 <- (121 + 280)/nrow(b201707)
m1_accuracy_201707

set.seed(1992)
#score test data set
b201707$m1_score_201707 <- predict(model_rl_1,type='response',b201707)
m1_pred_201707 <- prediction(b201707$m1_score_201707, b201707$flag_pago)
m1_perf_201707 <- performance(m1_pred_201707,"tpr","fpr")

#ROC
plot(m1_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision_201707 <- performance(m1_pred_201707, measure = "prec", x.measure = "rec")
plot(m1_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc_201707 <- performance(m1_pred_201707, measure = "acc")
plot(m1_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS_201707 <- round(max(attr(m1_perf_201707,'y.values')[[1]]-attr(m1_perf_201707,'x.values')[[1]])*100, 2)
m1_AUROC_201707 <- round(performance(m1_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini_201707 <- (2*m1_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m1_AUROC_201707,"\tKS: ", m1_KS_201707, "\tGini:", m1_Gini_201707, "\tAccuracy:", m1_accuracy_201707,"\n")



#'m2:Regresión Logística corte optimo - MASR'



tb2_201707 <- table(b201707$flag_pago,ifelse(predic_model_rl_1_201707>optCutOff,"X1_Bueno","X0_Malo"))
tb2_201707
m2_accuracy_201707 <-  ( 121+ 280)/nrow(b201707)
m2_accuracy_201707
#0.7316906

set.seed(1992)
#score test data set
b201707$m2_score <- predict(model_rl_1,type='response',b201707)
m2_pred_201707 <- prediction(ifelse(b201707$m1_score>optCutOff,1,0), b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf_201707 <- performance(m2_pred_201707,"tpr","fpr")

#ROC
plot(m2_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision_201707 <- performance(m2_pred_201707, measure = "prec", x.measure = "rec")
plot(m2_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc_201707 <- performance(m2_pred_201707, measure = "acc")
plot(m2_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS_201707 <- round(max(attr(m2_perf_201707,'y.values')[[1]]-attr(m2_perf_201707,'x.values')[[1]])*100, 2)
m2_AUROC_201707 <- round(performance(m2_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini_201707 <- (2*m2_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m2_AUROC_201707,"\tKS: ", m2_KS_201707, "\tGini:", m2_Gini_201707, "\tAccuracy:", m2_accuracy_201707,"\n")



#'m3:Regresión Logística - MASR - caret'

#score test data set
b201707$m3_score <- predict(model_rl_3_tun,type='prob',newdata=b201707)[2]
m3_pred_201707 <- prediction(b201707$m3_score, b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3_201707 <- table(ifelse(b201707$m3_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb3_201707

m3_accuracy_201707 <- (121+280)/nrow(b201707)
m3_accuracy_201707

m3_perf_201707 <- performance(m3_pred_201707,"tpr","fpr")

#ROC
plot(m3_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic - 201707 Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision_201707 <- performance(m3_pred_201707, measure = "prec", x.measure = "rec")
plot(m3_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc_201707 <- performance(m3_pred_201707, measure = "acc")
plot(m3_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS_201707 <- round(max(attr(m3_perf_201707,'y.values')[[1]]-attr(m3_perf_201707,'x.values')[[1]])*100, 2)
m3_AUROC_201707 <- round(performance(m3_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini_201707 <- (2*m3_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m3_AUROC_201707,"\tKS: ", m3_KS_201707, "\tGini:", m3_Gini_201707,"\tAccuracy:", m3_accuracy_201707, "\n")


#'m4:Regresión Logística - MACR'

predic_model_rl_4_201707 <- predict(model_rl_4, newdata = b201707,type = "response")

summary(predic_model_rl_4_201707)


tb4_201707 <- table(b201707$flag_pago,ifelse(predic_model_rl_4_201707>0.5,"X1_Bueno","X0_Malo"))
tb4_201707

(m4_accuracy_201707 <- (122 +  280)/nrow(b201707))

#0.6711185

#score test data set
b201707$m4_score <- predict(model_rl_4,type='response',b201707)
m4_pred_201707 <- prediction(b201707$m4_score, b201707$flag_pago)
m4_perf_201707 <- performance(m4_pred_201707,"tpr","fpr")

#ROC
plot(m4_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision_201707 <- performance(m4_pred_201707, measure = "prec", x.measure = "rec")
plot(m4_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc_201707 <- performance(m4_pred_201707, measure = "acc")
plot(m4_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS_201707 <- round(max(attr(m4_perf_201707,'y.values')[[1]]-attr(m4_perf_201707,'x.values')[[1]])*100, 2)
m4_AUROC_201707 <- round(performance(m4_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini_201707 <- (2*m4_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m4_AUROC_201707,"\tKS: ", m4_KS_201707, "\tGini:", m4_Gini_201707,"\tAccuracy:", m4_accuracy_201707, "\n")



#'m5:Regresión Logística - MACR - caret'

#score test data set
b201707$m5_score <- predict(model_rl_5_tun,type='prob',newdata=b201707)[2]
m5_pred_201707 <- prediction(b201707$m5_score, b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb201707 <- table(ifelse(b201707$m5_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb201707

m5_accuracy_201707 <- (122+280)/nrow(b201707)
m5_accuracy_201707
#0.6711185

m5_perf_201707 <- performance(m5_pred_201707,"tpr","fpr")

#ROC
plot(m5_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision_201707 <- performance(m5_pred_201707, measure = "prec", x.measure = "rec")
plot(m5_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc_201707 <- performance(m5_pred_201707, measure = "acc")
plot(m5_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS_201707 <- round(max(attr(m5_perf_201707,'y.values')[[1]]-attr(m5_perf_201707,'x.values')[[1]])*100, 2)
m5_AUROC_201707 <- round(performance(m5_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini_201707 <- (2*m5_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m5_AUROC_201707,"\tKS: ", m5_KS_201707, "\tGini:", m5_Gini_201707, "\tAccuracy:", m5_accuracy_201707,"\n")


#"m6: AD - cart - MASR"

predic_model_6_ad_1_201707 <- predict(model_6_ad_1, newdata = b201707,type = "prob")[,2]

summary(predic_model_6_ad_1_201707)


tb6_201707 <- table(b201707$flag_pago,ifelse(predic_model_6_ad_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb6_201707

m6_accuracy_201707 <- (128 + 273)/nrow(b201707)
m6_accuracy_201707
#0.6694491

#score test data set
b201707$m6_ad_score <- predict(model_6_ad_1,type='prob',b201707)[,2]
m6_pred_201707 <- prediction(b201707$m6_ad_score, b201707$flag_pago)
m6_perf_201707 <- performance(m6_pred_201707,"tpr","fpr")

#ROC
plot(m6_perf_201707, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision_201707 <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision_201707, main="m6 Árbol de decisión - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc_201707 <- performance(m6_pred_201707, measure = "acc")
plot(m6_perf_acc_201707, main="m6 Árbol de decisión - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS_201707 <- round(max(attr(m6_perf_201707,'y.values')[[1]]-attr(m6_perf_201707,'x.values')[[1]])*100, 2)
m6_AUROC_201707 <- round(performance(m6_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini_201707 <- (2*m6_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m6_AUROC_201707,"\tKS: ", m6_KS_201707, "\tGini:", m6_Gini_201707, "\tAccuracy:", m6_accuracy_201707,"\n")


#"m7: AD - cart - MASR - caret"

#score test data set
b201707$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=b201707)[2]
m7_pred_201707 <- prediction(b201707$m7_score, b201707$flag_pago)

tb7_201707 <- table(ifelse(b201707$m7_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb7_201707

m7_accuracy_201707 <- (123+ 277)/nrow(b201707)
m7_accuracy_201707
#0.6677796

m7_perf_201707 <- performance(m7_pred_201707,"tpr","fpr")

#ROC
plot(m7_perf_201707, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision_201707 <- performance(m7_pred_201707, measure = "prec", x.measure = "rec")
plot(m7_perf_precision_201707, main="m7 Arbol de decision - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc_201707 <- performance(m7_pred_201707, measure = "acc")
plot(m7_perf_acc_201707, main="m7 Arbol tuneado:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m7_KS_201707 <- round(max(attr(m7_perf_201707,'y.values')[[1]]-attr(m7_perf_201707,'x.values')[[1]])*100, 2)
m7_AUROC_201707 <- round(performance(m7_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini_201707 <- (2*m7_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")


#"m8: AD - cart - MACR"

predic_model_8_ad_2_201707 <- predict(model_8_ad_2, newdata = b201707,type = "prob")[,2]

summary(predic_model_8_ad_2_201707)


tb8_201707 <- table(b201707$flag_pago,ifelse(predic_model_8_ad_2_201707>0.5,"X1_Bueno","X0_Malo"))
tb8_201707

m8_accuracy_201707 <- (118 +  283)/nrow(b201707)
m8_accuracy_201707
#0.6694491

#score test data set
b201707$m8_score <- predict(model_8_ad_2,type='prob',b201707)[,2]
m8_pred_201707 <- prediction(b201707$m8_score, b201707$flag_pago)
m8_perf_201707 <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf_201707, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision_201707 <- performance(m8_pred_201707, measure = "prec", x.measure = "rec")
plot(m8_perf_precision_201707, main="m8 Arbol de decision - rpart:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m8_perf_acc_201707 <- performance(m8_pred_201707, measure = "acc")
plot(m8_perf_acc_201707, main="m8 Arbol de decision - rpart:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m8_KS_201707 <- round(max(attr(m8_perf_201707,'y.values')[[1]]-attr(m8_perf_201707,'x.values')[[1]])*100, 2)
m8_AUROC_201707 <- round(performance(m8_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini_201707 <- (2*m8_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m8_AUROC_201707,"\tKS: ", m8_KS_201707, "\tGini:", m8_Gini_201707,"\tAccuracy:", m8_accuracy_201707, "\n")


#"m9: AD - cart - MACR - caret"

#score test data set
b201707$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=b201707)[,2]
m9_pred_201707 <- prediction(b201707$m9_score, b201707$flag_pago)


tb9_201707 <- table(ifelse(b201707$m9_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb9_201707

m9_accuracy_201707 <- (117+279)/nrow(b201707)
m9_accuracy_201707
#0.6611018

m9_perf_201707 <- performance(m9_pred_201707,"tpr","fpr")

#ROC
plot(m9_perf_201707, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision_201707 <- performance(m9_pred_201707, measure = "prec", x.measure = "rec")
plot(m9_perf_precision_201707, main="m9 Arbol de decision - caret:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m9_perf_acc_201707 <- performance(m9_pred_201707, measure = "acc")
plot(m9_perf_acc_201707, main="m9 Arbol de decision - caret:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m9_KS_201707 <- round(max(attr(m9_perf_201707,'y.values')[[1]]-attr(m9_perf_201707,'x.values')[[1]])*100, 2)
m9_AUROC_201707 <- round(performance(m9_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini_201707 <- (2*m9_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m9_AUROC_201707,"\tKS: ", m9_KS_201707, "\tGini:", m9_Gini_201707, "\tAccuracy:", m9_accuracy_201707,"\n")


#"m10: AD - ctree - MAsR"


predic_model_10_ad_1_201707 <- sapply(predict(model_10_ad_1, newdata = b201707,type = "prob"),'[[',2)

summary(predic_model_10_ad_1_201707)

tb10_201707 <- table(b201707$flag_pago,ifelse(predic_model_10_ad_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb10_201707

m10_accuracy_201707 <- (122 + 278)/nrow(b201707)
m10_accuracy_201707
#0.6677796

#score test data set
b201707$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = b201707,type = "prob")))))[,2]

m10_pred_201707 <- prediction(b201707$m10_ad_score, b201707$flag_pago)
m10_perf_201707 <- performance(m10_pred_201707,"tpr","fpr")

#ROC
plot(m10_perf_201707, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision_201707 <- performance(m10_pred_201707, measure = "prec", x.measure = "rec")
plot(m10_perf_precision_201707, main="m6 Árbol de decisión - CTREE:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m10_perf_acc_201707 <- performance(m10_pred_201707, measure = "acc")
plot(m10_perf_acc_201707, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m10_KS_201707 <- round(max(attr(m10_perf_201707,'y.values')[[1]]-attr(m10_perf_201707,'x.values')[[1]])*100, 2)
m10_AUROC_201707 <- round(performance(m10_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini_201707 <- (2*m10_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m10_AUROC_201707,"\tKS: ", m10_KS_201707, "\tGini:", m10_Gini_201707, "\tAccuracy:", m10_accuracy_201707,"\n")


#"m11: AD - ctree - MASR - caret"


#score test data set
b201707$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=b201707)[2]
m11_pred_201707 <- prediction(b201707$m11_score, b201707$flag_pago)

tb11_201707 <- table(ifelse(b201707$m11_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb11_201707

m11_accuracy_201707 <- (119+281)/nrow(b201707)
m11_accuracy_201707
#0.6677796

m11_perf_201707 <- performance(m11_pred_201707,"tpr","fpr")

#ROC
plot(m11_perf_201707, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision_201707 <- performance(m11_pred_201707, measure = "prec", x.measure = "rec")
plot(m11_perf_precision_201707, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m11_perf_acc_201707 <- performance(m11_pred_201707, measure = "acc")
plot(m11_perf_acc_201707, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m11_KS_201707 <- round(max(attr(m11_perf_201707,'y.values')[[1]]-attr(m11_perf_201707,'x.values')[[1]])*100, 2)
m11_AUROC_201707 <- round(performance(m11_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini_201707 <- (2*m11_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m11_AUROC_201707,"\tKS: ", m11_KS_201707, "\tGini:", m11_Gini_201707, "\tAccuracy:", m11_accuracy_201707,"\n")



#"m12: AD - ctree - MACR"

predic_model_12_ad_2_201707 <- sapply(predict(model_12_ad_2, newdata = b201707,type = "prob"),'[[',2)

summary(predic_model_12_ad_2_201707)


tb12_201707 <- table(b201707$flag_pago,ifelse(predic_model_12_ad_2_201707>0.5,"X1_Bueno","X0_Malo"))
tb12_201707

m12_accuracy_201707 <- (130 + 276)/nrow(b201707)
m12_accuracy_201707
#0.6777963

#score test data set
b201707$m12_score <- sapply(predict(model_12_ad_2, newdata = b201707,type = "prob"),'[[',2)
m12_pred_201707 <- prediction(b201707$m12_score, b201707$flag_pago)
m12_perf_201707 <- performance(m12_pred_201707,"tpr","fpr")

#ROC
plot(m12_perf_201707, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision_201707 <- performance(m12_pred_201707, measure = "prec", x.measure = "rec")
plot(m12_perf_precision_201707, main="m12 Arbol de decision - CTREE :Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m12_perf_acc_201707 <- performance(m12_pred_201707, measure = "acc")
plot(m12_perf_acc_201707, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m12_KS_201707 <- round(max(attr(m12_perf_201707,'y.values')[[1]]-attr(m12_perf_201707,'x.values')[[1]])*100, 2)
m12_AUROC_201707 <- round(performance(m12_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini_201707 <- (2*m12_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m12_AUROC_201707,"\tKS: ", m12_KS_201707, "\tGini:", m12_Gini_201707,"\tAccuracy:", m12_accuracy_201707, "\n")



#"m13: AD - ctree - MACR - caret"

#score test data set
b201707$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=b201707)[,2]
m13_pred_201707 <- prediction(b201707$m13_score, b201707$flag_pago)


table(ifelse(b201707$m13_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m13_accuracy_201707 <- (131+268)/nrow(b201707)
m13_accuracy_201707
#0.6661102

m13_perf_201707 <- performance(m13_pred_201707,"tpr","fpr")

#ROC
plot(m13_perf_201707, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision_201707 <- performance(m13_pred_201707, measure = "prec", x.measure = "rec")
plot(m13_perf_precision_201707, main="m13 Arbol de decision - CTREE:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m13_perf_acc_201707 <- performance(m13_pred_201707, measure = "acc")
plot(m13_perf_acc_201707, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m13_KS_201707 <- round(max(attr(m13_perf_201707,'y.values')[[1]]-attr(m13_perf_201707,'x.values')[[1]])*100, 2)
m13_AUROC_201707 <- round(performance(m13_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini_201707 <- (2*m13_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m13_AUROC_201707,"\tKS: ", m13_KS_201707, "\tGini:", m13_Gini_201707, "\tAccuracy:", m13_accuracy_201707,"\n")


#"m14: RF - cart - MAsR"



predic_model_14_rf_1_201707 <- predict(model_14_rf_1, newdata = b201707,type = "prob")[,2]

summary(predic_model_14_rf_1_201707)

tb14_201707 <- table(b201707$flag_pago,ifelse(predic_model_14_rf_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb14_201707

m14_accuracy_201707 <- (124 + 275)/nrow(b201707)
m14_accuracy_201707
#0.6661102

#score test data set
b201707$m14_rf_score <- predict(model_14_rf_1, newdata = b201707,type = "prob")[,2]

m14_pred_201707 <- prediction(b201707$m14_rf_score, b201707$flag_pago)
m14_perf_201707 <- performance(m14_pred_201707,"tpr","fpr")

#ROC
plot(m14_perf_201707, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision_201707 <- performance(m14_pred_201707, measure = "prec", x.measure = "rec")
plot(m14_perf_precision_201707, main="m14 Random Forest:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m14_perf_acc_201707 <- performance(m14_pred_201707, measure = "acc")
plot(m14_perf_acc_201707, main="m14 Random Forest:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m14_KS_201707 <- round(max(attr(m14_perf_201707,'y.values')[[1]]-attr(m14_perf_201707,'x.values')[[1]])*100, 2)
m14_AUROC_201707 <- round(performance(m14_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini_201707 <- (2*m14_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m14_AUROC_201707,"\tKS: ", m14_KS_201707, "\tGini:", m14_Gini_201707, "\tAccuracy:", m14_accuracy_20176,"\n")


#"m15: RF - cart - MASR - caret"

#score test data set
b201707$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=b201707)[2]
m15_pred_201707 <- prediction(b201707$m15_score, b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201707$m15_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m15_accuracy_201707 <- (641+1501)/nrow(b201707)
m15_accuracy_201707
#0.7330595

m15_perf_201707 <- performance(m15_pred_201707,"tpr","fpr")

#ROC
plot(m15_perf_201707, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision_201707 <- performance(m15_pred_201707, measure = "prec", x.measure = "rec")
plot(m15_perf_precision_201707, main="m15 RandomForest:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m15_perf_acc_201707 <- performance(m15_pred_201707, measure = "acc")
plot(m15_perf_acc_201707, main="m15 RandomForest:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m15_KS_201707 <- round(max(attr(m15_perf_201707,'y.values')[[1]]-attr(m15_perf_201707,'x.values')[[1]])*100, 2)
m15_AUROC_201707 <- round(performance(m15_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini_201707 <- (2*m15_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m15_AUROC_201707,"\tKS: ", m15_KS_201707, "\tGini:", m15_Gini_201707, "\tAccuracy:", m15_accuracy_201707,"\n")


#"m16: RF - cart - MACR"

predic_model_16_rf_2_201707 <- predict(model_16_rf_2, newdata = b201707,type = "prob")[,2]

summary(predic_model_16_rf_2_201707)

tb16_201707 <- table(b201707$flag_pago,ifelse(predic_model_16_rf_2_201707>0.5,"X1_Bueno","X0_Malo"))
tb16_201707


m16_accuracy_201707 <- (640 + 1553)/nrow(b201707)
m16_accuracy_201707
#0.7510274

#score test data set

b201707$m16_rf_2_score <- predict(model_16_rf_2, newdata = b201707,type = "prob")[,2]

m16_pred_201707 <- prediction(b201707$m16_rf_2_score, b201707$flag_pago)
m16_perf_201707 <- performance(m16_pred_201707,"tpr","fpr")

#ROC
plot(m16_perf_201707, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision_201707 <- performance(m16_pred_201707, measure = "prec", x.measure = "rec")
plot(m16_perf_precision_201707, main="m16 RandomForest :Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m16_perf_acc_201707 <- performance(m16_pred_201707, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS_201707 <- round(max(attr(m16_perf_201707,'y.values')[[1]]-attr(m16_perf_201707,'x.values')[[1]])*100, 2)
m16_AUROC_201707 <- round(performance(m16_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini_201707 <- (2*m16_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m16_AUROC_201707,"\tKS: ", m16_KS_201707, "\tGini:", m16_Gini_201707,"\tAccuracy:", m16_accuracy_201707, "\n")



#"m17: RF - cart - MACR - caret"

#score test data set
b201707$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=b201707)[,2]
m17_pred_201707 <- prediction(b201707$m17_score, b201707$flag_pago)

table(ifelse(b201707$m17_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m17_accuracy_201707 <- (639+1515)/nrow(b201707)
m17_accuracy_201707
#0.7376712

m17_perf_201707 <- performance(m17_pred_201707,"tpr","fpr")

#ROC
plot(m17_perf_201707, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision_201707 <- performance(m17_pred_201707, measure = "prec", x.measure = "rec")
plot(m17_perf_precision_201707, main="m17 RandomForest:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m17_perf_acc_201707 <- performance(m17_pred_201707, measure = "acc")
plot(m17_perf_acc_201707, main="m17 RandomForest:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m17_KS_201707 <- round(max(attr(m17_perf_201707,'y.values')[[1]]-attr(m17_perf_201707,'x.values')[[1]])*100, 2)
m17_AUROC_201707 <- round(performance(m17_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini_201707 <- (2*m17_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m17_AUROC_201707,"\tKS: ", m17_KS_201707, "\tGini:", m17_Gini_201707, "\tAccuracy:", m17_accuracy_201707,"\n")


#"m18: GBM - MAsR"

predic_model_18_gbm_1_201707 <- predict(model_18_gbm_1, newdata = b201707,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1_201707, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18_201707 <- table(b201707$flag_pago,ifelse(predic_model_18_gbm_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb18_201707

m18_accuracy_201707 <- (622+1518)/nrow(b201707)
m18_accuracy_201707

#0.7323751

#score test data set
b201707$m18_GBM_score <- predict(model_18_gbm_1, newdata = b201707,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred_201707 <- prediction(b201707$m18_GBM_score, b201707$flag_pago)
m18_perf_201707 <- performance(m18_pred_201707,"tpr","fpr")

#ROC
plot(m18_perf_201707, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision_201707 <- performance(m18_pred_201707, measure = "prec", x.measure = "rec")
plot(m18_perf_precision_201707, main="m18 GBM:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m18_perf_acc_201707 <- performance(m18_pred_201707, measure = "acc")
plot(m18_perf_acc_201707, main="m18 GBM:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m18_KS_201707 <- round(max(attr(m18_perf_201707,'y.values')[[1]]-attr(m18_perf_201707,'x.values')[[1]])*100, 2)
m18_AUROC_201707 <- round(performance(m18_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini_201707 <- (2*m18_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m18_AUROC_201707,"\tKS: ", m18_KS_201707, "\tGini:", m18_Gini_201707, "\tAccuracy:", m18_accuracy_201707,"\n")


#"m19: GBM - MASR - caret"

#score test data set
b201707$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=b201707)[2]
m19_pred_201707 <- prediction(b201707$m19_score, b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201707$m19_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m19_accuracy_201707 <- (630+1508)/nrow(b201707)
m19_accuracy_201707
#0.7316906

m19_perf_201707 <- performance(m19_pred_201707,"tpr","fpr")

#ROC
plot(m19_perf_201707, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision_201707 <- performance(m19_pred_201707, measure = "prec", x.measure = "rec")
plot(m19_perf_precision_201707, main="m19 GBM:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m19_perf_acc_201707 <- performance(m19_pred_201707, measure = "acc")
plot(m19_perf_acc_201707, main="m19 GBM:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m19_KS_201707 <- round(max(attr(m19_perf_201707,'y.values')[[1]]-attr(m19_perf_201707,'x.values')[[1]])*100, 2)
m19_AUROC_201707 <- round(performance(m19_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini_201707 <- (2*m19_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m19_AUROC_201707,"\tKS: ", m19_KS_201707, "\tGini:", m19_Gini_201707, "\tAccuracy:", m19_accuracy_201707,"\n")



#"m20: GBM - MACR"



predic_model_20_gbm_2_201707 <- predict(model_20_gbm_2, newdata = b201707,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2_201707, n=30)


summary(predic_model_20_gbm_2_201707)

tb20_201707 <- table(b201707$flag_pago,ifelse(predic_model_20_gbm_2_201707>0.5,"X1_Bueno","X0_Malo"))
tb20_201707


m20_accuracy_201707 <- (636 + 1514)/nrow(b201707)
m20_accuracy_201707
#0.7363014

#score test data set


b201707$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = b201707,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred_201707 <- prediction(b201707$m20_GBM_2_score, b201707$flag_pago)
m20_perf_201707 <- performance(m20_pred_201707,"tpr","fpr")

#ROC
plot(m20_perf_201707, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision_201707 <- performance(m20_pred_201707, measure = "prec", x.measure = "rec")
plot(m20_perf_precision_201707, main="m20 GBM :Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m20_perf_acc_201707 <- performance(m20_pred_201707, measure = "acc")
plot(m20_perf_acc_201707, main="m20 GBM :Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m20_KS_201707 <- round(max(attr(m20_perf_201707,'y.values')[[1]]-attr(m20_perf_201707,'x.values')[[1]])*100, 2)
m20_AUROC_201707 <- round(performance(m20_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini_201707 <- (2*m20_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m20_AUROC_201707,"\tKS: ", m20_KS_201707, "\tGini:", m20_Gini_201707,"\tAccuracy:", m20_accuracy_201707, "\n")


#"m21: GBM - MACR - caret"


#score test data set
b201707$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=b201707)[,2]
m21_pred_201707 <- prediction(b201707$m21_score, b201707$flag_pago)

table(ifelse(b201707$m21_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m21_accuracy_201707 <- (638+1513)/nrow(b201707)
m21_accuracy_201707
#0.7366438

m21_perf_201707 <- performance(m21_pred_201707,"tpr","fpr")

#ROC
plot(m21_perf_201707, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision_201707 <- performance(m21_pred_201707, measure = "prec", x.measure = "rec")
plot(m21_perf_precision_201707, main="m21 GBM:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m21_perf_acc_201707 <- performance(m21_pred_201707, measure = "acc")
plot(m21_perf_acc_201707, main="m21 RandomForest:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m21_KS_201707 <- round(max(attr(m21_perf_201707,'y.values')[[1]]-attr(m21_perf_201707,'x.values')[[1]])*100, 2)
m21_AUROC_201707 <- round(performance(m21_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini_201707 <- (2*m21_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m21_AUROC_201707,"\tKS: ", m21_KS_201707, "\tGini:", m21_Gini_201707, "\tAccuracy:", m21_accuracy_201707,"\n")



# tABLA DE PERFORMANCE - OOS
models_201707 <- c('m1:Regresión Logística - MASR - 201707', 'm2:Regresión Logística corte optimo - MASR - 201707',
                   'm3:Regresión Logística - MASR - caret - 201707','m4:Regresión Logística - MACR - 201707', 
                   'm5:Regresión Logística - MACR - caret - 201707', "m6: AD - cart - MASR - 201707",
                   "m7: AD - cart - MASR - caret - 201707", "m8: AD - cart - MACR - 201707",
                   "m9: AD - cart - MACR - caret - 201707", "m10: AD - ctree - MAsR - 201707",
                   "m11: AD - ctree - MASR - caret - 201707", "m12: AD - ctree - MACR - 201707",
                   "m13: AD - ctree - MACR - caret - 201707", "m14: RF - cart - MAsR - 201707",
                   "m15: RF - cart - MASR - caret - 201707", "m16: RF - cart - MACR - 201707",
                   "m17: RF - cart - MACR - caret - 201707", "m18: GBM - MAsR - 201707",
                   "m19: GBM - MASR - caret - 201707","m20: GBM - MACR - 201707", "m21: GBM - MACR - caret - 201707")

# AUCs
models_AUC_201707 <- c(m1_AUROC_201707, m2_AUROC_201707, m3_AUROC_201707, m4_AUROC_201707, m5_AUROC_201707, m6_AUROC_201707, m7_AUROC_201707, m8_AUROC_201707, m9_AUROC_201707, m10_AUROC_201707, 
                       m11_AUROC_201707, m12_AUROC_201707, m13_AUROC_201707, m14_AUROC_201707, m15_AUROC_201707, m16_AUROC_201707, m17_AUROC_201707, m18_AUROC_201707, m19_AUROC_201707, m20_AUROC_201707, 
                       m21_AUROC_201707)
# KS
models_KS_201707 <- c(m1_KS_201707, m2_KS_201707, m3_KS_201707, m4_KS_201707, m5_KS_201707, m6_KS_201707, m7_KS_201707, m8_KS_201707, m9_KS_201707, m10_KS_201707, m11_KS_201707, m12_KS_201707, m13_KS_201707, m14_KS_201707, m15_KS_201707, 
                      m16_KS_201707, m17_KS_201707, m18_KS_201707, m19_KS_201707, m20_KS_201707, m21_KS_201707)

# Gini
models_Gini_201707 <- c(m1_Gini_201707, m2_Gini_201707, m3_Gini_201707, m4_Gini_201707, m5_Gini_201707, m6_Gini_201707, m7_Gini_201707, m8_Gini_201707, m9_Gini_201707, m10_Gini_201707, 
                        m11_Gini_201707, m12_Gini_201707, m13_Gini_201707, m14_Gini_201707, m15_Gini_201707, m16_Gini_201707, m17_Gini_201707, m18_Gini_201707, m19_Gini_201707, m20_Gini_201707, 
                        m21_Gini_201707)


# Accuraccy

#models_accuracy_201707 <- c(m1_accuracy_201707, m2_accuracy_201707, m3_accuracy_201707, m4_accuracy_201707, m5_accuracy_201707, m6_accuracy_201707, m7_accuracy_201707, m8_accuracy_201707, m9_accuracy_201707, 
#                            m10_accuracy_201707, m11_accuracy_201707, m12_accuracy_201707, m13_accuracy_201707, m14_accuracy_201707, m15_accuracy_201707, m16_accuracy_201707, m17_accuracy_201707, 
#                            m18_accuracy_201707, m19_accuracy_201707, m20_accuracy_201707, m21_accuracy_201707)


# Juntando todo
model_performance_metric_201707 <- as.data.frame(cbind(models_201707, models_AUC_201707, models_KS_201707, models_Gini_201707))

model_performance_metric_201707 <- model_performance_metric_201707[order(models_Gini_201707, decreasing = T),] 


# Colnames 
colnames(model_performance_metric_201707) <- c("Modelos_201707", "AUC_201707", "KS_201707", "Gini_201707")

# Display Performance Reports
kable(model_performance_metric_201707, caption ="Comparación de modelos - OOS - 201707")


#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VI          MODELOS EN OOT
#---------------------------------------------------------------------------------------
########################################################################################


#############################################################
# PERIODO - 201708
#############################################################


#'m1:Regresión Logística - MASR'

set.seed(1992)
predic_model_rl_1_201708 <- predict(model_rl_1,type='response',b201708)

tb1_201708 <- table(b201708$flag_pago,ifelse(predic_model_rl_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb1_201708

m1_accuracy_201708 <- (121 + 280)/nrow(b201708)
m1_accuracy_201708

set.seed(1992)
#score test data set
b201708$m1_score_201708 <- predict(model_rl_1,type='response',b201708)
m1_pred_201708 <- prediction(b201708$m1_score_201708, b201708$flag_pago)
m1_perf_201708 <- performance(m1_pred_201708,"tpr","fpr")

#ROC
plot(m1_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision_201708 <- performance(m1_pred_201708, measure = "prec", x.measure = "rec")
plot(m1_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc_201708 <- performance(m1_pred_201708, measure = "acc")
plot(m1_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS_201708 <- round(max(attr(m1_perf_201708,'y.values')[[1]]-attr(m1_perf_201708,'x.values')[[1]])*100, 2)
m1_AUROC_201708 <- round(performance(m1_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini_201708 <- (2*m1_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m1_AUROC_201708,"\tKS: ", m1_KS_201708, "\tGini:", m1_Gini_201708, "\tAccuracy:", m1_accuracy_201708,"\n")



#'m2:Regresión Logística corte optimo - MASR'



tb2_201708 <- table(b201708$flag_pago,ifelse(predic_model_rl_1_201708>optCutOff,"X1_Bueno","X0_Malo"))
tb2_201708
m2_accuracy_201708 <-  ( 121+ 280)/nrow(b201708)
m2_accuracy_201708
#0.7316906

set.seed(1992)
#score test data set
b201708$m2_score <- predict(model_rl_1,type='response',b201708)
m2_pred_201708 <- prediction(ifelse(b201708$m1_score>optCutOff,1,0), b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf_201708 <- performance(m2_pred_201708,"tpr","fpr")

#ROC
plot(m2_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision_201708 <- performance(m2_pred_201708, measure = "prec", x.measure = "rec")
plot(m2_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc_201708 <- performance(m2_pred_201708, measure = "acc")
plot(m2_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS_201708 <- round(max(attr(m2_perf_201708,'y.values')[[1]]-attr(m2_perf_201708,'x.values')[[1]])*100, 2)
m2_AUROC_201708 <- round(performance(m2_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini_201708 <- (2*m2_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m2_AUROC_201708,"\tKS: ", m2_KS_201708, "\tGini:", m2_Gini_201708, "\tAccuracy:", m2_accuracy_201708,"\n")



#'m3:Regresión Logística - MASR - caret'

#score test data set
b201708$m3_score <- predict(model_rl_3_tun,type='prob',newdata=b201708)[2]
m3_pred_201708 <- prediction(b201708$m3_score, b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3_201708 <- table(ifelse(b201708$m3_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb3_201708

m3_accuracy_201708 <- (121+280)/nrow(b201708)
m3_accuracy_201708

m3_perf_201708 <- performance(m3_pred_201708,"tpr","fpr")

#ROC
plot(m3_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic - 201708 Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision_201708 <- performance(m3_pred_201708, measure = "prec", x.measure = "rec")
plot(m3_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc_201708 <- performance(m3_pred_201708, measure = "acc")
plot(m3_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS_201708 <- round(max(attr(m3_perf_201708,'y.values')[[1]]-attr(m3_perf_201708,'x.values')[[1]])*100, 2)
m3_AUROC_201708 <- round(performance(m3_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini_201708 <- (2*m3_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m3_AUROC_201708,"\tKS: ", m3_KS_201708, "\tGini:", m3_Gini_201708,"\tAccuracy:", m3_accuracy_201708, "\n")


#'m4:Regresión Logística - MACR'

predic_model_rl_4_201708 <- predict(model_rl_4, newdata = b201708,type = "response")

summary(predic_model_rl_4_201708)


tb4_201708 <- table(b201708$flag_pago,ifelse(predic_model_rl_4_201708>0.5,"X1_Bueno","X0_Malo"))
tb4_201708

(m4_accuracy_201708 <- (122 +  280)/nrow(b201708))

#0.6711185

#score test data set
b201708$m4_score <- predict(model_rl_4,type='response',b201708)
m4_pred_201708 <- prediction(b201708$m4_score, b201708$flag_pago)
m4_perf_201708 <- performance(m4_pred_201708,"tpr","fpr")

#ROC
plot(m4_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision_201708 <- performance(m4_pred_201708, measure = "prec", x.measure = "rec")
plot(m4_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc_201708 <- performance(m4_pred_201708, measure = "acc")
plot(m4_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS_201708 <- round(max(attr(m4_perf_201708,'y.values')[[1]]-attr(m4_perf_201708,'x.values')[[1]])*100, 2)
m4_AUROC_201708 <- round(performance(m4_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini_201708 <- (2*m4_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m4_AUROC_201708,"\tKS: ", m4_KS_201708, "\tGini:", m4_Gini_201708,"\tAccuracy:", m4_accuracy_201708, "\n")



#'m5:Regresión Logística - MACR - caret'

#score test data set
b201708$m5_score <- predict(model_rl_5_tun,type='prob',newdata=b201708)[2]
m5_pred_201708 <- prediction(b201708$m5_score, b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb201708 <- table(ifelse(b201708$m5_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb201708

m5_accuracy_201708 <- (122+280)/nrow(b201708)
m5_accuracy_201708
#0.6711185

m5_perf_201708 <- performance(m5_pred_201708,"tpr","fpr")

#ROC
plot(m5_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision_201708 <- performance(m5_pred_201708, measure = "prec", x.measure = "rec")
plot(m5_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc_201708 <- performance(m5_pred_201708, measure = "acc")
plot(m5_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS_201708 <- round(max(attr(m5_perf_201708,'y.values')[[1]]-attr(m5_perf_201708,'x.values')[[1]])*100, 2)
m5_AUROC_201708 <- round(performance(m5_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini_201708 <- (2*m5_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m5_AUROC_201708,"\tKS: ", m5_KS_201708, "\tGini:", m5_Gini_201708, "\tAccuracy:", m5_accuracy_201708,"\n")


#"m6: AD - cart - MASR"

predic_model_6_ad_1_201708 <- predict(model_6_ad_1, newdata = b201708,type = "prob")[,2]

summary(predic_model_6_ad_1_201708)


tb6_201708 <- table(b201708$flag_pago,ifelse(predic_model_6_ad_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb6_201708

m6_accuracy_201708 <- (128 + 273)/nrow(b201708)
m6_accuracy_201708
#0.6694491

#score test data set
b201708$m6_ad_score <- predict(model_6_ad_1,type='prob',b201708)[,2]
m6_pred_201708 <- prediction(b201708$m6_ad_score, b201708$flag_pago)
m6_perf_201708 <- performance(m6_pred_201708,"tpr","fpr")

#ROC
plot(m6_perf_201708, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision_201708 <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision_201708, main="m6 Árbol de decisión - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc_201708 <- performance(m6_pred_201708, measure = "acc")
plot(m6_perf_acc_201708, main="m6 Árbol de decisión - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS_201708 <- round(max(attr(m6_perf_201708,'y.values')[[1]]-attr(m6_perf_201708,'x.values')[[1]])*100, 2)
m6_AUROC_201708 <- round(performance(m6_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini_201708 <- (2*m6_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m6_AUROC_201708,"\tKS: ", m6_KS_201708, "\tGini:", m6_Gini_201708, "\tAccuracy:", m6_accuracy_201708,"\n")


#"m7: AD - cart - MASR - caret"

#score test data set
b201708$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=b201708)[2]
m7_pred_201708 <- prediction(b201708$m7_score, b201708$flag_pago)

tb7_201708 <- table(ifelse(b201708$m7_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb7_201708

m7_accuracy_201708 <- (123+ 277)/nrow(b201708)
m7_accuracy_201708
#0.6677796

m7_perf_201708 <- performance(m7_pred_201708,"tpr","fpr")

#ROC
plot(m7_perf_201708, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision_201708 <- performance(m7_pred_201708, measure = "prec", x.measure = "rec")
plot(m7_perf_precision_201708, main="m7 Arbol de decision - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc_201708 <- performance(m7_pred_201708, measure = "acc")
plot(m7_perf_acc_201708, main="m7 Arbol tuneado:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m7_KS_201708 <- round(max(attr(m7_perf_201708,'y.values')[[1]]-attr(m7_perf_201708,'x.values')[[1]])*100, 2)
m7_AUROC_201708 <- round(performance(m7_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini_201708 <- (2*m7_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")


#"m8: AD - cart - MACR"

predic_model_8_ad_2_201708 <- predict(model_8_ad_2, newdata = b201708,type = "prob")[,2]

summary(predic_model_8_ad_2_201708)


tb8_201708 <- table(b201708$flag_pago,ifelse(predic_model_8_ad_2_201708>0.5,"X1_Bueno","X0_Malo"))
tb8_201708

m8_accuracy_201708 <- (118 +  283)/nrow(b201708)
m8_accuracy_201708
#0.6694491

#score test data set
b201708$m8_score <- predict(model_8_ad_2,type='prob',b201708)[,2]
m8_pred_201708 <- prediction(b201708$m8_score, b201708$flag_pago)
m8_perf_201708 <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf_201708, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision_201708 <- performance(m8_pred_201708, measure = "prec", x.measure = "rec")
plot(m8_perf_precision_201708, main="m8 Arbol de decision - rpart:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m8_perf_acc_201708 <- performance(m8_pred_201708, measure = "acc")
plot(m8_perf_acc_201708, main="m8 Arbol de decision - rpart:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m8_KS_201708 <- round(max(attr(m8_perf_201708,'y.values')[[1]]-attr(m8_perf_201708,'x.values')[[1]])*100, 2)
m8_AUROC_201708 <- round(performance(m8_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini_201708 <- (2*m8_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m8_AUROC_201708,"\tKS: ", m8_KS_201708, "\tGini:", m8_Gini_201708,"\tAccuracy:", m8_accuracy_201708, "\n")


#"m9: AD - cart - MACR - caret"

#score test data set
b201708$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=b201708)[,2]
m9_pred_201708 <- prediction(b201708$m9_score, b201708$flag_pago)


tb9_201708 <- table(ifelse(b201708$m9_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb9_201708

m9_accuracy_201708 <- (117+279)/nrow(b201708)
m9_accuracy_201708
#0.6611018

m9_perf_201708 <- performance(m9_pred_201708,"tpr","fpr")

#ROC
plot(m9_perf_201708, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision_201708 <- performance(m9_pred_201708, measure = "prec", x.measure = "rec")
plot(m9_perf_precision_201708, main="m9 Arbol de decision - caret:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m9_perf_acc_201708 <- performance(m9_pred_201708, measure = "acc")
plot(m9_perf_acc_201708, main="m9 Arbol de decision - caret:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m9_KS_201708 <- round(max(attr(m9_perf_201708,'y.values')[[1]]-attr(m9_perf_201708,'x.values')[[1]])*100, 2)
m9_AUROC_201708 <- round(performance(m9_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini_201708 <- (2*m9_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m9_AUROC_201708,"\tKS: ", m9_KS_201708, "\tGini:", m9_Gini_201708, "\tAccuracy:", m9_accuracy_201708,"\n")


#"m10: AD - ctree - MAsR"


predic_model_10_ad_1_201708 <- sapply(predict(model_10_ad_1, newdata = b201708,type = "prob"),'[[',2)

summary(predic_model_10_ad_1_201708)

tb10_201708 <- table(b201708$flag_pago,ifelse(predic_model_10_ad_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb10_201708

m10_accuracy_201708 <- (122 + 278)/nrow(b201708)
m10_accuracy_201708
#0.6677796

#score test data set
b201708$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = b201708,type = "prob")))))[,2]

m10_pred_201708 <- prediction(b201708$m10_ad_score, b201708$flag_pago)
m10_perf_201708 <- performance(m10_pred_201708,"tpr","fpr")

#ROC
plot(m10_perf_201708, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision_201708 <- performance(m10_pred_201708, measure = "prec", x.measure = "rec")
plot(m10_perf_precision_201708, main="m6 Árbol de decisión - CTREE:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m10_perf_acc_201708 <- performance(m10_pred_201708, measure = "acc")
plot(m10_perf_acc_201708, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m10_KS_201708 <- round(max(attr(m10_perf_201708,'y.values')[[1]]-attr(m10_perf_201708,'x.values')[[1]])*100, 2)
m10_AUROC_201708 <- round(performance(m10_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini_201708 <- (2*m10_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m10_AUROC_201708,"\tKS: ", m10_KS_201708, "\tGini:", m10_Gini_201708, "\tAccuracy:", m10_accuracy_201708,"\n")


#"m11: AD - ctree - MASR - caret"


#score test data set
b201708$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=b201708)[2]
m11_pred_201708 <- prediction(b201708$m11_score, b201708$flag_pago)

tb11_201708 <- table(ifelse(b201708$m11_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb11_201708

m11_accuracy_201708 <- (119+281)/nrow(b201708)
m11_accuracy_201708
#0.6677796

m11_perf_201708 <- performance(m11_pred_201708,"tpr","fpr")

#ROC
plot(m11_perf_201708, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision_201708 <- performance(m11_pred_201708, measure = "prec", x.measure = "rec")
plot(m11_perf_precision_201708, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m11_perf_acc_201708 <- performance(m11_pred_201708, measure = "acc")
plot(m11_perf_acc_201708, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m11_KS_201708 <- round(max(attr(m11_perf_201708,'y.values')[[1]]-attr(m11_perf_201708,'x.values')[[1]])*100, 2)
m11_AUROC_201708 <- round(performance(m11_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini_201708 <- (2*m11_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m11_AUROC_201708,"\tKS: ", m11_KS_201708, "\tGini:", m11_Gini_201708, "\tAccuracy:", m11_accuracy_201708,"\n")



#"m12: AD - ctree - MACR"

predic_model_12_ad_2_201708 <- sapply(predict(model_12_ad_2, newdata = b201708,type = "prob"),'[[',2)

summary(predic_model_12_ad_2_201708)


tb12_201708 <- table(b201708$flag_pago,ifelse(predic_model_12_ad_2_201708>0.5,"X1_Bueno","X0_Malo"))
tb12_201708

m12_accuracy_201708 <- (130 + 276)/nrow(b201708)
m12_accuracy_201708
#0.6777963

#score test data set
b201708$m12_score <- sapply(predict(model_12_ad_2, newdata = b201708,type = "prob"),'[[',2)
m12_pred_201708 <- prediction(b201708$m12_score, b201708$flag_pago)
m12_perf_201708 <- performance(m12_pred_201708,"tpr","fpr")

#ROC
plot(m12_perf_201708, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision_201708 <- performance(m12_pred_201708, measure = "prec", x.measure = "rec")
plot(m12_perf_precision_201708, main="m12 Arbol de decision - CTREE :Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m12_perf_acc_201708 <- performance(m12_pred_201708, measure = "acc")
plot(m12_perf_acc_201708, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m12_KS_201708 <- round(max(attr(m12_perf_201708,'y.values')[[1]]-attr(m12_perf_201708,'x.values')[[1]])*100, 2)
m12_AUROC_201708 <- round(performance(m12_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini_201708 <- (2*m12_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m12_AUROC_201708,"\tKS: ", m12_KS_201708, "\tGini:", m12_Gini_201708,"\tAccuracy:", m12_accuracy_201708, "\n")



#"m13: AD - ctree - MACR - caret"

#score test data set
b201708$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=b201708)[,2]
m13_pred_201708 <- prediction(b201708$m13_score, b201708$flag_pago)


table(ifelse(b201708$m13_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m13_accuracy_201708 <- (131+268)/nrow(b201708)
m13_accuracy_201708
#0.6661102

m13_perf_201708 <- performance(m13_pred_201708,"tpr","fpr")

#ROC
plot(m13_perf_201708, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision_201708 <- performance(m13_pred_201708, measure = "prec", x.measure = "rec")
plot(m13_perf_precision_201708, main="m13 Arbol de decision - CTREE:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m13_perf_acc_201708 <- performance(m13_pred_201708, measure = "acc")
plot(m13_perf_acc_201708, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m13_KS_201708 <- round(max(attr(m13_perf_201708,'y.values')[[1]]-attr(m13_perf_201708,'x.values')[[1]])*100, 2)
m13_AUROC_201708 <- round(performance(m13_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini_201708 <- (2*m13_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m13_AUROC_201708,"\tKS: ", m13_KS_201708, "\tGini:", m13_Gini_201708, "\tAccuracy:", m13_accuracy_201708,"\n")


#"m14: RF - cart - MAsR"



predic_model_14_rf_1_201708 <- predict(model_14_rf_1, newdata = b201708,type = "prob")[,2]

summary(predic_model_14_rf_1_201708)

tb14_201708 <- table(b201708$flag_pago,ifelse(predic_model_14_rf_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb14_201708

m14_accuracy_20176 <- (124 + 275)/nrow(b201708)
m14_accuracy_20176
#0.6661102

#score test data set
b201708$m14_rf_score <- predict(model_14_rf_1, newdata = b201708,type = "prob")[,2]

m14_pred_201708 <- prediction(b201708$m14_rf_score, b201708$flag_pago)
m14_perf_201708 <- performance(m14_pred_201708,"tpr","fpr")

#ROC
plot(m14_perf_201708, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision_201708 <- performance(m14_pred_201708, measure = "prec", x.measure = "rec")
plot(m14_perf_precision_201708, main="m14 Random Forest:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m14_perf_acc_201708 <- performance(m14_pred_201708, measure = "acc")
plot(m14_perf_acc_201708, main="m14 Random Forest:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m14_KS_201708 <- round(max(attr(m14_perf_201708,'y.values')[[1]]-attr(m14_perf_201708,'x.values')[[1]])*100, 2)
m14_AUROC_201708 <- round(performance(m14_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini_201708 <- (2*m14_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m14_AUROC_201708,"\tKS: ", m14_KS_201708, "\tGini:", m14_Gini_201708, "\tAccuracy:", m14_accuracy_20176,"\n")


#"m15: RF - cart - MASR - caret"

#score test data set
b201708$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=b201708)[2]
m15_pred_201708 <- prediction(b201708$m15_score, b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201708$m15_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m15_accuracy_201708 <- (641+1501)/nrow(b201708)
m15_accuracy_201708
#0.7330595

m15_perf_201708 <- performance(m15_pred_201708,"tpr","fpr")

#ROC
plot(m15_perf_201708, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision_201708 <- performance(m15_pred_201708, measure = "prec", x.measure = "rec")
plot(m15_perf_precision_201708, main="m15 RandomForest:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m15_perf_acc_201708 <- performance(m15_pred_201708, measure = "acc")
plot(m15_perf_acc_201708, main="m15 RandomForest:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m15_KS_201708 <- round(max(attr(m15_perf_201708,'y.values')[[1]]-attr(m15_perf_201708,'x.values')[[1]])*100, 2)
m15_AUROC_201708 <- round(performance(m15_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini_201708 <- (2*m15_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m15_AUROC_201708,"\tKS: ", m15_KS_201708, "\tGini:", m15_Gini_201708, "\tAccuracy:", m15_accuracy_201708,"\n")


#"m16: RF - cart - MACR"

predic_model_16_rf_2_201708 <- predict(model_16_rf_2, newdata = b201708,type = "prob")[,2]

summary(predic_model_16_rf_2_201708)

tb16_201708 <- table(b201708$flag_pago,ifelse(predic_model_16_rf_2_201708>0.5,"X1_Bueno","X0_Malo"))
tb16_201708


m16_accuracy_201708 <- (640 + 1553)/nrow(b201708)
m16_accuracy_201708
#0.7510274

#score test data set

b201708$m16_rf_2_score <- predict(model_16_rf_2, newdata = b201708,type = "prob")[,2]

m16_pred_201708 <- prediction(b201708$m16_rf_2_score, b201708$flag_pago)
m16_perf_201708 <- performance(m16_pred_201708,"tpr","fpr")

#ROC
plot(m16_perf_201708, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision_201708 <- performance(m16_pred_201708, measure = "prec", x.measure = "rec")
plot(m16_perf_precision_201708, main="m16 RandomForest :Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m16_perf_acc_201708 <- performance(m16_pred_201708, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS_201708 <- round(max(attr(m16_perf_201708,'y.values')[[1]]-attr(m16_perf_201708,'x.values')[[1]])*100, 2)
m16_AUROC_201708 <- round(performance(m16_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini_201708 <- (2*m16_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m16_AUROC_201708,"\tKS: ", m16_KS_201708, "\tGini:", m16_Gini_201708,"\tAccuracy:", m16_accuracy_201708, "\n")



#"m17: RF - cart - MACR - caret"

#score test data set
b201708$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=b201708)[,2]
m17_pred_201708 <- prediction(b201708$m17_score, b201708$flag_pago)

table(ifelse(b201708$m17_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m17_accuracy_201708 <- (639+1515)/nrow(b201708)
m17_accuracy_201708
#0.7376712

m17_perf_201708 <- performance(m17_pred_201708,"tpr","fpr")

#ROC
plot(m17_perf_201708, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision_201708 <- performance(m17_pred_201708, measure = "prec", x.measure = "rec")
plot(m17_perf_precision_201708, main="m17 RandomForest:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m17_perf_acc_201708 <- performance(m17_pred_201708, measure = "acc")
plot(m17_perf_acc_201708, main="m17 RandomForest:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m17_KS_201708 <- round(max(attr(m17_perf_201708,'y.values')[[1]]-attr(m17_perf_201708,'x.values')[[1]])*100, 2)
m17_AUROC_201708 <- round(performance(m17_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini_201708 <- (2*m17_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m17_AUROC_201708,"\tKS: ", m17_KS_201708, "\tGini:", m17_Gini_201708, "\tAccuracy:", m17_accuracy_201708,"\n")


#"m18: GBM - MAsR"

predic_model_18_gbm_1_201708 <- predict(model_18_gbm_1, newdata = b201708,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1_201708, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18_201708 <- table(b201708$flag_pago,ifelse(predic_model_18_gbm_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb18_201708

m18_accuracy_201708 <- (622+1518)/nrow(b201708)
m18_accuracy_201708

#0.7323751

#score test data set
b201708$m18_GBM_score <- predict(model_18_gbm_1, newdata = b201708,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred_201708 <- prediction(b201708$m18_GBM_score, b201708$flag_pago)
m18_perf_201708 <- performance(m18_pred_201708,"tpr","fpr")

#ROC
plot(m18_perf_201708, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision_201708 <- performance(m18_pred_201708, measure = "prec", x.measure = "rec")
plot(m18_perf_precision_201708, main="m18 GBM:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m18_perf_acc_201708 <- performance(m18_pred_201708, measure = "acc")
plot(m18_perf_acc_201708, main="m18 GBM:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m18_KS_201708 <- round(max(attr(m18_perf_201708,'y.values')[[1]]-attr(m18_perf_201708,'x.values')[[1]])*100, 2)
m18_AUROC_201708 <- round(performance(m18_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini_201708 <- (2*m18_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m18_AUROC_201708,"\tKS: ", m18_KS_201708, "\tGini:", m18_Gini_201708, "\tAccuracy:", m18_accuracy_201708,"\n")


#"m19: GBM - MASR - caret"

#score test data set
b201708$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=b201708)[2]
m19_pred_201708 <- prediction(b201708$m19_score, b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201708$m19_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m19_accuracy_201708 <- (630+1508)/nrow(b201708)
m19_accuracy_201708
#0.7316906

m19_perf_201708 <- performance(m19_pred_201708,"tpr","fpr")

#ROC
plot(m19_perf_201708, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision_201708 <- performance(m19_pred_201708, measure = "prec", x.measure = "rec")
plot(m19_perf_precision_201708, main="m19 GBM:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m19_perf_acc_201708 <- performance(m19_pred_201708, measure = "acc")
plot(m19_perf_acc_201708, main="m19 GBM:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m19_KS_201708 <- round(max(attr(m19_perf_201708,'y.values')[[1]]-attr(m19_perf_201708,'x.values')[[1]])*100, 2)
m19_AUROC_201708 <- round(performance(m19_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini_201708 <- (2*m19_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m19_AUROC_201708,"\tKS: ", m19_KS_201708, "\tGini:", m19_Gini_201708, "\tAccuracy:", m19_accuracy_201708,"\n")



#"m20: GBM - MACR"



predic_model_20_gbm_2_201708 <- predict(model_20_gbm_2, newdata = b201708,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2_201708, n=30)


summary(predic_model_20_gbm_2_201708)

tb20_201708 <- table(b201708$flag_pago,ifelse(predic_model_20_gbm_2_201708>0.5,"X1_Bueno","X0_Malo"))
tb20_201708


m20_accuracy_201708 <- (636 + 1514)/nrow(b201708)
m20_accuracy_201708
#0.7363014

#score test data set


b201708$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = b201708,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred_201708 <- prediction(b201708$m20_GBM_2_score, b201708$flag_pago)
m20_perf_201708 <- performance(m20_pred_201708,"tpr","fpr")

#ROC
plot(m20_perf_201708, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision_201708 <- performance(m20_pred_201708, measure = "prec", x.measure = "rec")
plot(m20_perf_precision_201708, main="m20 GBM :Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m20_perf_acc_201708 <- performance(m20_pred_201708, measure = "acc")
plot(m20_perf_acc_201708, main="m20 GBM :Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m20_KS_201708 <- round(max(attr(m20_perf_201708,'y.values')[[1]]-attr(m20_perf_201708,'x.values')[[1]])*100, 2)
m20_AUROC_201708 <- round(performance(m20_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini_201708 <- (2*m20_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m20_AUROC_201708,"\tKS: ", m20_KS_201708, "\tGini:", m20_Gini_201708,"\tAccuracy:", m20_accuracy_201708, "\n")


#"m21: GBM - MACR - caret"


#score test data set
b201708$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=b201708)[,2]
m21_pred_201708 <- prediction(b201708$m21_score, b201708$flag_pago)

table(ifelse(b201708$m21_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m21_accuracy_201708 <- (638+1513)/nrow(b201708)
m21_accuracy_201708
#0.7366438

m21_perf_201708 <- performance(m21_pred_201708,"tpr","fpr")

#ROC
plot(m21_perf_201708, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision_201708 <- performance(m21_pred_201708, measure = "prec", x.measure = "rec")
plot(m21_perf_precision_201708, main="m21 GBM:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m21_perf_acc_201708 <- performance(m21_pred_201708, measure = "acc")
plot(m21_perf_acc_201708, main="m21 RandomForest:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m21_KS_201708 <- round(max(attr(m21_perf_201708,'y.values')[[1]]-attr(m21_perf_201708,'x.values')[[1]])*100, 2)
m21_AUROC_201708 <- round(performance(m21_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini_201708 <- (2*m21_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m21_AUROC_201708,"\tKS: ", m21_KS_201708, "\tGini:", m21_Gini_201708, "\tAccuracy:", m21_accuracy_201708,"\n")



# tABLA DE PERFORMANCE - OOS
models_201708 <- c('m1:Regresión Logística - MASR - 201708', 'm2:Regresión Logística corte optimo - MASR - 201708',
                   'm3:Regresión Logística - MASR - caret - 201708','m4:Regresión Logística - MACR - 201708', 
                   'm5:Regresión Logística - MACR - caret - 201708', "m6: AD - cart - MASR - 201708",
                   "m7: AD - cart - MASR - caret - 201708", "m8: AD - cart - MACR - 201708",
                   "m9: AD - cart - MACR - caret - 201708", "m10: AD - ctree - MAsR - 201708",
                   "m11: AD - ctree - MASR - caret - 201708", "m12: AD - ctree - MACR - 201708",
                   "m13: AD - ctree - MACR - caret - 201708", "m14: RF - cart - MAsR - 201708",
                   "m15: RF - cart - MASR - caret - 201708", "m16: RF - cart - MACR - 201708",
                   "m17: RF - cart - MACR - caret - 201708", "m18: GBM - MAsR - 201708",
                   "m19: GBM - MASR - caret - 201708","m20: GBM - MACR - 201708", "m21: GBM - MACR - caret - 201708")

# AUCs
models_AUC_201708 <- c(m1_AUROC_201708, m2_AUROC_201708, m3_AUROC_201708, m4_AUROC_201708, m5_AUROC_201708, m6_AUROC_201708, m7_AUROC_201708, m8_AUROC_201708, m9_AUROC_201708, m10_AUROC_201708, 
                       m11_AUROC_201708, m12_AUROC_201708, m13_AUROC_201708, m14_AUROC_201708, m15_AUROC_201708, m16_AUROC_201708, m17_AUROC_201708, m18_AUROC_201708, m19_AUROC_201708, m20_AUROC_201708, 
                       m21_AUROC_201708)
# KS
models_KS_201708 <- c(m1_KS_201708, m2_KS_201708, m3_KS_201708, m4_KS_201708, m5_KS_201708, m6_KS_201708, m7_KS_201708, m8_KS_201708, m9_KS_201708, m10_KS_201708, m11_KS_201708, m12_KS_201708, m13_KS_201708, m14_KS_201708, m15_KS_201708, 
                      m16_KS_201708, m17_KS_201708, m18_KS_201708, m19_KS_201708, m20_KS_201708, m21_KS_201708)

# Gini
models_Gini_201708 <- c(m1_Gini_201708, m2_Gini_201708, m3_Gini_201708, m4_Gini_201708, m5_Gini_201708, m6_Gini_201708, m7_Gini_201708, m8_Gini_201708, m9_Gini_201708, m10_Gini_201708, 
                        m11_Gini_201708, m12_Gini_201708, m13_Gini_201708, m14_Gini_201708, m15_Gini_201708, m16_Gini_201708, m17_Gini_201708, m18_Gini_201708, m19_Gini_201708, m20_Gini_201708, 
                        m21_Gini_201708)


# Accuraccy

#models_accuracy_201708 <- c(m1_accuracy_201708, m2_accuracy_201708, m3_accuracy_201708, m4_accuracy_201708, m5_accuracy_201708, m6_accuracy_201708, m7_accuracy_201708, m8_accuracy_201708, m9_accuracy_201708, 
#                            m10_accuracy_201708, m11_accuracy_201708, m12_accuracy_201708, m13_accuracy_201708, m14_accuracy_201708, m15_accuracy_201708, m16_accuracy_201708, m17_accuracy_201708, 
#                            m18_accuracy_201708, m19_accuracy_201708, m20_accuracy_201708, m21_accuracy_201708)


# Juntando todo
model_performance_metric_201708 <- as.data.frame(cbind(models_201708, models_AUC_201708, models_KS_201708, models_Gini_201708))

model_performance_metric_201708 <- model_performance_metric_201708[order(models_Gini_201708, decreasing = T),] 

# Colnames 
colnames(model_performance_metric_201708) <- c("Modelos_201708", "AUC_201708", "KS_201708", "Gini_201708")

# Display Performance Reports
kable(model_performance_metric_201708, caption ="Comparación de modelos - OOS - 201708")


#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VI          MODELOS EN OOT
#---------------------------------------------------------------------------------------
########################################################################################


#############################################################
# PERIODO - 201709
#############################################################


#'m1:Regresión Logística - MASR'

set.seed(1992)
predic_model_rl_1_201709 <- predict(model_rl_1,type='response',b201709)

tb1_201709 <- table(b201709$flag_pago,ifelse(predic_model_rl_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb1_201709

m1_accuracy_201709 <- (121 + 280)/nrow(b201709)
m1_accuracy_201709

set.seed(1992)
#score test data set
b201709$m1_score_201709 <- predict(model_rl_1,type='response',b201709)
m1_pred_201709 <- prediction(b201709$m1_score_201709, b201709$flag_pago)
m1_perf_201709 <- performance(m1_pred_201709,"tpr","fpr")

#ROC
plot(m1_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision_201709 <- performance(m1_pred_201709, measure = "prec", x.measure = "rec")
plot(m1_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc_201709 <- performance(m1_pred_201709, measure = "acc")
plot(m1_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS_201709 <- round(max(attr(m1_perf_201709,'y.values')[[1]]-attr(m1_perf_201709,'x.values')[[1]])*100, 2)
m1_AUROC_201709 <- round(performance(m1_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini_201709 <- (2*m1_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m1_AUROC_201709,"\tKS: ", m1_KS_201709, "\tGini:", m1_Gini_201709, "\tAccuracy:", m1_accuracy_201709,"\n")



#'m2:Regresión Logística corte optimo - MASR'



tb2_201709 <- table(b201709$flag_pago,ifelse(predic_model_rl_1_201709>optCutOff,"X1_Bueno","X0_Malo"))
tb2_201709
m2_accuracy_201709 <-  ( 121+ 280)/nrow(b201709)
m2_accuracy_201709
#0.7316906

set.seed(1992)
#score test data set
b201709$m2_score <- predict(model_rl_1,type='response',b201709)
m2_pred_201709 <- prediction(ifelse(b201709$m1_score>optCutOff,1,0), b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf_201709 <- performance(m2_pred_201709,"tpr","fpr")

#ROC
plot(m2_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision_201709 <- performance(m2_pred_201709, measure = "prec", x.measure = "rec")
plot(m2_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc_201709 <- performance(m2_pred_201709, measure = "acc")
plot(m2_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS_201709 <- round(max(attr(m2_perf_201709,'y.values')[[1]]-attr(m2_perf_201709,'x.values')[[1]])*100, 2)
m2_AUROC_201709 <- round(performance(m2_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini_201709 <- (2*m2_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m2_AUROC_201709,"\tKS: ", m2_KS_201709, "\tGini:", m2_Gini_201709, "\tAccuracy:", m2_accuracy_201709,"\n")



#'m3:Regresión Logística - MASR - caret'

#score test data set
b201709$m3_score <- predict(model_rl_3_tun,type='prob',newdata=b201709)[2]
m3_pred_201709 <- prediction(b201709$m3_score, b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3_201709 <- table(ifelse(b201709$m3_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb3_201709

m3_accuracy_201709 <- (121+280)/nrow(b201709)
m3_accuracy_201709

m3_perf_201709 <- performance(m3_pred_201709,"tpr","fpr")

#ROC
plot(m3_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic - 201709 Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision_201709 <- performance(m3_pred_201709, measure = "prec", x.measure = "rec")
plot(m3_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc_201709 <- performance(m3_pred_201709, measure = "acc")
plot(m3_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS_201709 <- round(max(attr(m3_perf_201709,'y.values')[[1]]-attr(m3_perf_201709,'x.values')[[1]])*100, 2)
m3_AUROC_201709 <- round(performance(m3_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini_201709 <- (2*m3_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m3_AUROC_201709,"\tKS: ", m3_KS_201709, "\tGini:", m3_Gini_201709,"\tAccuracy:", m3_accuracy_201709, "\n")


#'m4:Regresión Logística - MACR'

predic_model_rl_4_201709 <- predict(model_rl_4, newdata = b201709,type = "response")

summary(predic_model_rl_4_201709)


tb4_201709 <- table(b201709$flag_pago,ifelse(predic_model_rl_4_201709>0.5,"X1_Bueno","X0_Malo"))
tb4_201709

(m4_accuracy_201709 <- (122 +  280)/nrow(b201709))

#0.6711185

#score test data set
b201709$m4_score <- predict(model_rl_4,type='response',b201709)
m4_pred_201709 <- prediction(b201709$m4_score, b201709$flag_pago)
m4_perf_201709 <- performance(m4_pred_201709,"tpr","fpr")

#ROC
plot(m4_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision_201709 <- performance(m4_pred_201709, measure = "prec", x.measure = "rec")
plot(m4_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc_201709 <- performance(m4_pred_201709, measure = "acc")
plot(m4_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS_201709 <- round(max(attr(m4_perf_201709,'y.values')[[1]]-attr(m4_perf_201709,'x.values')[[1]])*100, 2)
m4_AUROC_201709 <- round(performance(m4_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini_201709 <- (2*m4_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m4_AUROC_201709,"\tKS: ", m4_KS_201709, "\tGini:", m4_Gini_201709,"\tAccuracy:", m4_accuracy_201709, "\n")



#'m5:Regresión Logística - MACR - caret'

#score test data set
b201709$m5_score <- predict(model_rl_5_tun,type='prob',newdata=b201709)[2]
m5_pred_201709 <- prediction(b201709$m5_score, b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb201709 <- table(ifelse(b201709$m5_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb201709

m5_accuracy_201709 <- (122+280)/nrow(b201709)
m5_accuracy_201709
#0.6711185

m5_perf_201709 <- performance(m5_pred_201709,"tpr","fpr")

#ROC
plot(m5_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision_201709 <- performance(m5_pred_201709, measure = "prec", x.measure = "rec")
plot(m5_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc_201709 <- performance(m5_pred_201709, measure = "acc")
plot(m5_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS_201709 <- round(max(attr(m5_perf_201709,'y.values')[[1]]-attr(m5_perf_201709,'x.values')[[1]])*100, 2)
m5_AUROC_201709 <- round(performance(m5_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini_201709 <- (2*m5_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m5_AUROC_201709,"\tKS: ", m5_KS_201709, "\tGini:", m5_Gini_201709, "\tAccuracy:", m5_accuracy_201709,"\n")


#"m6: AD - cart - MASR"

predic_model_6_ad_1_201709 <- predict(model_6_ad_1, newdata = b201709,type = "prob")[,2]

summary(predic_model_6_ad_1_201709)


tb6_201709 <- table(b201709$flag_pago,ifelse(predic_model_6_ad_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb6_201709

m6_accuracy_201709 <- (128 + 273)/nrow(b201709)
m6_accuracy_201709
#0.6694491

#score test data set
b201709$m6_ad_score <- predict(model_6_ad_1,type='prob',b201709)[,2]
m6_pred_201709 <- prediction(b201709$m6_ad_score, b201709$flag_pago)
m6_perf_201709 <- performance(m6_pred_201709,"tpr","fpr")

#ROC
plot(m6_perf_201709, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision_201709 <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision_201709, main="m6 Árbol de decisión - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc_201709 <- performance(m6_pred_201709, measure = "acc")
plot(m6_perf_acc_201709, main="m6 Árbol de decisión - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS_201709 <- round(max(attr(m6_perf_201709,'y.values')[[1]]-attr(m6_perf_201709,'x.values')[[1]])*100, 2)
m6_AUROC_201709 <- round(performance(m6_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini_201709 <- (2*m6_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m6_AUROC_201709,"\tKS: ", m6_KS_201709, "\tGini:", m6_Gini_201709, "\tAccuracy:", m6_accuracy_201709,"\n")


#"m7: AD - cart - MASR - caret"

#score test data set
b201709$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=b201709)[2]
m7_pred_201709 <- prediction(b201709$m7_score, b201709$flag_pago)

tb7_201709 <- table(ifelse(b201709$m7_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb7_201709

m7_accuracy_201709 <- (123+ 277)/nrow(b201709)
m7_accuracy_201709
#0.6677796

m7_perf_201709 <- performance(m7_pred_201709,"tpr","fpr")

#ROC
plot(m7_perf_201709, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision_201709 <- performance(m7_pred_201709, measure = "prec", x.measure = "rec")
plot(m7_perf_precision_201709, main="m7 Arbol de decision - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc_201709 <- performance(m7_pred_201709, measure = "acc")
plot(m7_perf_acc_201709, main="m7 Arbol tuneado:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m7_KS_201709 <- round(max(attr(m7_perf_201709,'y.values')[[1]]-attr(m7_perf_201709,'x.values')[[1]])*100, 2)
m7_AUROC_201709 <- round(performance(m7_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini_201709 <- (2*m7_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")


#"m8: AD - cart - MACR"

predic_model_8_ad_2_201709 <- predict(model_8_ad_2, newdata = b201709,type = "prob")[,2]

summary(predic_model_8_ad_2_201709)


tb8_201709 <- table(b201709$flag_pago,ifelse(predic_model_8_ad_2_201709>0.5,"X1_Bueno","X0_Malo"))
tb8_201709

m8_accuracy_201709 <- (118 +  283)/nrow(b201709)
m8_accuracy_201709
#0.6694491

#score test data set
b201709$m8_score <- predict(model_8_ad_2,type='prob',b201709)[,2]
m8_pred_201709 <- prediction(b201709$m8_score, b201709$flag_pago)
m8_perf_201709 <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf_201709, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision_201709 <- performance(m8_pred_201709, measure = "prec", x.measure = "rec")
plot(m8_perf_precision_201709, main="m8 Arbol de decision - rpart:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m8_perf_acc_201709 <- performance(m8_pred_201709, measure = "acc")
plot(m8_perf_acc_201709, main="m8 Arbol de decision - rpart:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m8_KS_201709 <- round(max(attr(m8_perf_201709,'y.values')[[1]]-attr(m8_perf_201709,'x.values')[[1]])*100, 2)
m8_AUROC_201709 <- round(performance(m8_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini_201709 <- (2*m8_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m8_AUROC_201709,"\tKS: ", m8_KS_201709, "\tGini:", m8_Gini_201709,"\tAccuracy:", m8_accuracy_201709, "\n")


#"m9: AD - cart - MACR - caret"

#score test data set
b201709$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=b201709)[,2]
m9_pred_201709 <- prediction(b201709$m9_score, b201709$flag_pago)


tb9_201709 <- table(ifelse(b201709$m9_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb9_201709

m9_accuracy_201709 <- (117+279)/nrow(b201709)
m9_accuracy_201709
#0.6611018

m9_perf_201709 <- performance(m9_pred_201709,"tpr","fpr")

#ROC
plot(m9_perf_201709, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision_201709 <- performance(m9_pred_201709, measure = "prec", x.measure = "rec")
plot(m9_perf_precision_201709, main="m9 Arbol de decision - caret:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m9_perf_acc_201709 <- performance(m9_pred_201709, measure = "acc")
plot(m9_perf_acc_201709, main="m9 Arbol de decision - caret:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m9_KS_201709 <- round(max(attr(m9_perf_201709,'y.values')[[1]]-attr(m9_perf_201709,'x.values')[[1]])*100, 2)
m9_AUROC_201709 <- round(performance(m9_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini_201709 <- (2*m9_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m9_AUROC_201709,"\tKS: ", m9_KS_201709, "\tGini:", m9_Gini_201709, "\tAccuracy:", m9_accuracy_201709,"\n")


#"m10: AD - ctree - MAsR"


predic_model_10_ad_1_201709 <- sapply(predict(model_10_ad_1, newdata = b201709,type = "prob"),'[[',2)

summary(predic_model_10_ad_1_201709)

tb10_201709 <- table(b201709$flag_pago,ifelse(predic_model_10_ad_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb10_201709

m10_accuracy_201709 <- (122 + 278)/nrow(b201709)
m10_accuracy_201709
#0.6677796

#score test data set
b201709$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = b201709,type = "prob")))))[,2]

m10_pred_201709 <- prediction(b201709$m10_ad_score, b201709$flag_pago)
m10_perf_201709 <- performance(m10_pred_201709,"tpr","fpr")

#ROC
plot(m10_perf_201709, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision_201709 <- performance(m10_pred_201709, measure = "prec", x.measure = "rec")
plot(m10_perf_precision_201709, main="m6 Árbol de decisión - CTREE:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m10_perf_acc_201709 <- performance(m10_pred_201709, measure = "acc")
plot(m10_perf_acc_201709, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m10_KS_201709 <- round(max(attr(m10_perf_201709,'y.values')[[1]]-attr(m10_perf_201709,'x.values')[[1]])*100, 2)
m10_AUROC_201709 <- round(performance(m10_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini_201709 <- (2*m10_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m10_AUROC_201709,"\tKS: ", m10_KS_201709, "\tGini:", m10_Gini_201709, "\tAccuracy:", m10_accuracy_201709,"\n")


#"m11: AD - ctree - MASR - caret"


#score test data set
b201709$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=b201709)[2]
m11_pred_201709 <- prediction(b201709$m11_score, b201709$flag_pago)

tb11_201709 <- table(ifelse(b201709$m11_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb11_201709

m11_accuracy_201709 <- (119+281)/nrow(b201709)
m11_accuracy_201709
#0.6677796

m11_perf_201709 <- performance(m11_pred_201709,"tpr","fpr")

#ROC
plot(m11_perf_201709, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision_201709 <- performance(m11_pred_201709, measure = "prec", x.measure = "rec")
plot(m11_perf_precision_201709, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m11_perf_acc_201709 <- performance(m11_pred_201709, measure = "acc")
plot(m11_perf_acc_201709, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m11_KS_201709 <- round(max(attr(m11_perf_201709,'y.values')[[1]]-attr(m11_perf_201709,'x.values')[[1]])*100, 2)
m11_AUROC_201709 <- round(performance(m11_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini_201709 <- (2*m11_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m11_AUROC_201709,"\tKS: ", m11_KS_201709, "\tGini:", m11_Gini_201709, "\tAccuracy:", m11_accuracy_201709,"\n")



#"m12: AD - ctree - MACR"

predic_model_12_ad_2_201709 <- sapply(predict(model_12_ad_2, newdata = b201709,type = "prob"),'[[',2)

summary(predic_model_12_ad_2_201709)


tb12_201709 <- table(b201709$flag_pago,ifelse(predic_model_12_ad_2_201709>0.5,"X1_Bueno","X0_Malo"))
tb12_201709

m12_accuracy_201709 <- (130 + 276)/nrow(b201709)
m12_accuracy_201709
#0.6777963

#score test data set
b201709$m12_score <- sapply(predict(model_12_ad_2, newdata = b201709,type = "prob"),'[[',2)
m12_pred_201709 <- prediction(b201709$m12_score, b201709$flag_pago)
m12_perf_201709 <- performance(m12_pred_201709,"tpr","fpr")

#ROC
plot(m12_perf_201709, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision_201709 <- performance(m12_pred_201709, measure = "prec", x.measure = "rec")
plot(m12_perf_precision_201709, main="m12 Arbol de decision - CTREE :Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m12_perf_acc_201709 <- performance(m12_pred_201709, measure = "acc")
plot(m12_perf_acc_201709, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m12_KS_201709 <- round(max(attr(m12_perf_201709,'y.values')[[1]]-attr(m12_perf_201709,'x.values')[[1]])*100, 2)
m12_AUROC_201709 <- round(performance(m12_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini_201709 <- (2*m12_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m12_AUROC_201709,"\tKS: ", m12_KS_201709, "\tGini:", m12_Gini_201709,"\tAccuracy:", m12_accuracy_201709, "\n")



#"m13: AD - ctree - MACR - caret"

#score test data set
b201709$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=b201709)[,2]
m13_pred_201709 <- prediction(b201709$m13_score, b201709$flag_pago)


table(ifelse(b201709$m13_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m13_accuracy_201709 <- (131+268)/nrow(b201709)
m13_accuracy_201709
#0.6661102

m13_perf_201709 <- performance(m13_pred_201709,"tpr","fpr")

#ROC
plot(m13_perf_201709, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision_201709 <- performance(m13_pred_201709, measure = "prec", x.measure = "rec")
plot(m13_perf_precision_201709, main="m13 Arbol de decision - CTREE:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m13_perf_acc_201709 <- performance(m13_pred_201709, measure = "acc")
plot(m13_perf_acc_201709, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m13_KS_201709 <- round(max(attr(m13_perf_201709,'y.values')[[1]]-attr(m13_perf_201709,'x.values')[[1]])*100, 2)
m13_AUROC_201709 <- round(performance(m13_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini_201709 <- (2*m13_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m13_AUROC_201709,"\tKS: ", m13_KS_201709, "\tGini:", m13_Gini_201709, "\tAccuracy:", m13_accuracy_201709,"\n")


#"m14: RF - cart - MAsR"



predic_model_14_rf_1_201709 <- predict(model_14_rf_1, newdata = b201709,type = "prob")[,2]

summary(predic_model_14_rf_1_201709)

tb14_201709 <- table(b201709$flag_pago,ifelse(predic_model_14_rf_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb14_201709

m14_accuracy_20176 <- (124 + 275)/nrow(b201709)
m14_accuracy_20176
#0.6661102

#score test data set
b201709$m14_rf_score <- predict(model_14_rf_1, newdata = b201709,type = "prob")[,2]

m14_pred_201709 <- prediction(b201709$m14_rf_score, b201709$flag_pago)
m14_perf_201709 <- performance(m14_pred_201709,"tpr","fpr")

#ROC
plot(m14_perf_201709, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision_201709 <- performance(m14_pred_201709, measure = "prec", x.measure = "rec")
plot(m14_perf_precision_201709, main="m14 Random Forest:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m14_perf_acc_201709 <- performance(m14_pred_201709, measure = "acc")
plot(m14_perf_acc_201709, main="m14 Random Forest:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m14_KS_201709 <- round(max(attr(m14_perf_201709,'y.values')[[1]]-attr(m14_perf_201709,'x.values')[[1]])*100, 2)
m14_AUROC_201709 <- round(performance(m14_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini_201709 <- (2*m14_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m14_AUROC_201709,"\tKS: ", m14_KS_201709, "\tGini:", m14_Gini_201709, "\tAccuracy:", m14_accuracy_20176,"\n")


#"m15: RF - cart - MASR - caret"

#score test data set
b201709$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=b201709)[2]
m15_pred_201709 <- prediction(b201709$m15_score, b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201709$m15_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m15_accuracy_201709 <- (641+1501)/nrow(b201709)
m15_accuracy_201709
#0.7330595

m15_perf_201709 <- performance(m15_pred_201709,"tpr","fpr")

#ROC
plot(m15_perf_201709, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision_201709 <- performance(m15_pred_201709, measure = "prec", x.measure = "rec")
plot(m15_perf_precision_201709, main="m15 RandomForest:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m15_perf_acc_201709 <- performance(m15_pred_201709, measure = "acc")
plot(m15_perf_acc_201709, main="m15 RandomForest:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m15_KS_201709 <- round(max(attr(m15_perf_201709,'y.values')[[1]]-attr(m15_perf_201709,'x.values')[[1]])*100, 2)
m15_AUROC_201709 <- round(performance(m15_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini_201709 <- (2*m15_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m15_AUROC_201709,"\tKS: ", m15_KS_201709, "\tGini:", m15_Gini_201709, "\tAccuracy:", m15_accuracy_201709,"\n")


#"m16: RF - cart - MACR"

predic_model_16_rf_2_201709 <- predict(model_16_rf_2, newdata = b201709,type = "prob")[,2]

summary(predic_model_16_rf_2_201709)

tb16_201709 <- table(b201709$flag_pago,ifelse(predic_model_16_rf_2_201709>0.5,"X1_Bueno","X0_Malo"))
tb16_201709


m16_accuracy_201709 <- (640 + 1553)/nrow(b201709)
m16_accuracy_201709
#0.7510274

#score test data set

b201709$m16_rf_2_score <- predict(model_16_rf_2, newdata = b201709,type = "prob")[,2]

m16_pred_201709 <- prediction(b201709$m16_rf_2_score, b201709$flag_pago)
m16_perf_201709 <- performance(m16_pred_201709,"tpr","fpr")

#ROC
plot(m16_perf_201709, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision_201709 <- performance(m16_pred_201709, measure = "prec", x.measure = "rec")
plot(m16_perf_precision_201709, main="m16 RandomForest :Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m16_perf_acc_201709 <- performance(m16_pred_201709, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS_201709 <- round(max(attr(m16_perf_201709,'y.values')[[1]]-attr(m16_perf_201709,'x.values')[[1]])*100, 2)
m16_AUROC_201709 <- round(performance(m16_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini_201709 <- (2*m16_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m16_AUROC_201709,"\tKS: ", m16_KS_201709, "\tGini:", m16_Gini_201709,"\tAccuracy:", m16_accuracy_201709, "\n")



#"m17: RF - cart - MACR - caret"

#score test data set
b201709$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=b201709)[,2]
m17_pred_201709 <- prediction(b201709$m17_score, b201709$flag_pago)

table(ifelse(b201709$m17_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m17_accuracy_201709 <- (639+1515)/nrow(b201709)
m17_accuracy_201709
#0.7376712

m17_perf_201709 <- performance(m17_pred_201709,"tpr","fpr")

#ROC
plot(m17_perf_201709, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision_201709 <- performance(m17_pred_201709, measure = "prec", x.measure = "rec")
plot(m17_perf_precision_201709, main="m17 RandomForest:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m17_perf_acc_201709 <- performance(m17_pred_201709, measure = "acc")
plot(m17_perf_acc_201709, main="m17 RandomForest:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m17_KS_201709 <- round(max(attr(m17_perf_201709,'y.values')[[1]]-attr(m17_perf_201709,'x.values')[[1]])*100, 2)
m17_AUROC_201709 <- round(performance(m17_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini_201709 <- (2*m17_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m17_AUROC_201709,"\tKS: ", m17_KS_201709, "\tGini:", m17_Gini_201709, "\tAccuracy:", m17_accuracy_201709,"\n")


#"m18: GBM - MAsR"

predic_model_18_gbm_1_201709 <- predict(model_18_gbm_1, newdata = b201709,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1_201709, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18_201709 <- table(b201709$flag_pago,ifelse(predic_model_18_gbm_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb18_201709

m18_accuracy_201709 <- (622+1518)/nrow(b201709)
m18_accuracy_201709

#0.7323751

#score test data set
b201709$m18_GBM_score <- predict(model_18_gbm_1, newdata = b201709,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred_201709 <- prediction(b201709$m18_GBM_score, b201709$flag_pago)
m18_perf_201709 <- performance(m18_pred_201709,"tpr","fpr")

#ROC
plot(m18_perf_201709, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision_201709 <- performance(m18_pred_201709, measure = "prec", x.measure = "rec")
plot(m18_perf_precision_201709, main="m18 GBM:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m18_perf_acc_201709 <- performance(m18_pred_201709, measure = "acc")
plot(m18_perf_acc_201709, main="m18 GBM:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m18_KS_201709 <- round(max(attr(m18_perf_201709,'y.values')[[1]]-attr(m18_perf_201709,'x.values')[[1]])*100, 2)
m18_AUROC_201709 <- round(performance(m18_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini_201709 <- (2*m18_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m18_AUROC_201709,"\tKS: ", m18_KS_201709, "\tGini:", m18_Gini_201709, "\tAccuracy:", m18_accuracy_201709,"\n")


#"m19: GBM - MASR - caret"

#score test data set
b201709$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=b201709)[2]
m19_pred_201709 <- prediction(b201709$m19_score, b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201709$m19_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m19_accuracy_201709 <- (630+1508)/nrow(b201709)
m19_accuracy_201709
#0.7316906

m19_perf_201709 <- performance(m19_pred_201709,"tpr","fpr")

#ROC
plot(m19_perf_201709, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision_201709 <- performance(m19_pred_201709, measure = "prec", x.measure = "rec")
plot(m19_perf_precision_201709, main="m19 GBM:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m19_perf_acc_201709 <- performance(m19_pred_201709, measure = "acc")
plot(m19_perf_acc_201709, main="m19 GBM:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m19_KS_201709 <- round(max(attr(m19_perf_201709,'y.values')[[1]]-attr(m19_perf_201709,'x.values')[[1]])*100, 2)
m19_AUROC_201709 <- round(performance(m19_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini_201709 <- (2*m19_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m19_AUROC_201709,"\tKS: ", m19_KS_201709, "\tGini:", m19_Gini_201709, "\tAccuracy:", m19_accuracy_201709,"\n")



#"m20: GBM - MACR"



predic_model_20_gbm_2_201709 <- predict(model_20_gbm_2, newdata = b201709,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2_201709, n=30)


summary(predic_model_20_gbm_2_201709)

tb20_201709 <- table(b201709$flag_pago,ifelse(predic_model_20_gbm_2_201709>0.5,"X1_Bueno","X0_Malo"))
tb20_201709


m20_accuracy_201709 <- (636 + 1514)/nrow(b201709)
m20_accuracy_201709
#0.7363014

#score test data set


b201709$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = b201709,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred_201709 <- prediction(b201709$m20_GBM_2_score, b201709$flag_pago)
m20_perf_201709 <- performance(m20_pred_201709,"tpr","fpr")

#ROC
plot(m20_perf_201709, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision_201709 <- performance(m20_pred_201709, measure = "prec", x.measure = "rec")
plot(m20_perf_precision_201709, main="m20 GBM :Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m20_perf_acc_201709 <- performance(m20_pred_201709, measure = "acc")
plot(m20_perf_acc_201709, main="m20 GBM :Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m20_KS_201709 <- round(max(attr(m20_perf_201709,'y.values')[[1]]-attr(m20_perf_201709,'x.values')[[1]])*100, 2)
m20_AUROC_201709 <- round(performance(m20_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini_201709 <- (2*m20_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m20_AUROC_201709,"\tKS: ", m20_KS_201709, "\tGini:", m20_Gini_201709,"\tAccuracy:", m20_accuracy_201709, "\n")


#"m21: GBM - MACR - caret"


#score test data set
b201709$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=b201709)[,2]
m21_pred_201709 <- prediction(b201709$m21_score, b201709$flag_pago)

table(ifelse(b201709$m21_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m21_accuracy_201709 <- (638+1513)/nrow(b201709)
m21_accuracy_201709
#0.7366438

m21_perf_201709 <- performance(m21_pred_201709,"tpr","fpr")

#ROC
plot(m21_perf_201709, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision_201709 <- performance(m21_pred_201709, measure = "prec", x.measure = "rec")
plot(m21_perf_precision_201709, main="m21 GBM:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m21_perf_acc_201709 <- performance(m21_pred_201709, measure = "acc")
plot(m21_perf_acc_201709, main="m21 RandomForest:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m21_KS_201709 <- round(max(attr(m21_perf_201709,'y.values')[[1]]-attr(m21_perf_201709,'x.values')[[1]])*100, 2)
m21_AUROC_201709 <- round(performance(m21_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini_201709 <- (2*m21_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m21_AUROC_201709,"\tKS: ", m21_KS_201709, "\tGini:", m21_Gini_201709, "\tAccuracy:", m21_accuracy_201709,"\n")



# tABLA DE PERFORMANCE - OOS
models_201709 <- c('m1:Regresión Logística - MASR - 201709', 'm2:Regresión Logística corte optimo - MASR - 201709',
                   'm3:Regresión Logística - MASR - caret - 201709','m4:Regresión Logística - MACR - 201709', 
                   'm5:Regresión Logística - MACR - caret - 201709', "m6: AD - cart - MASR - 201709",
                   "m7: AD - cart - MASR - caret - 201709", "m8: AD - cart - MACR - 201709",
                   "m9: AD - cart - MACR - caret - 201709", "m10: AD - ctree - MAsR - 201709",
                   "m11: AD - ctree - MASR - caret - 201709", "m12: AD - ctree - MACR - 201709",
                   "m13: AD - ctree - MACR - caret - 201709", "m14: RF - cart - MAsR - 201709",
                   "m15: RF - cart - MASR - caret - 201709", "m16: RF - cart - MACR - 201709",
                   "m17: RF - cart - MACR - caret - 201709", "m18: GBM - MAsR - 201709",
                   "m19: GBM - MASR - caret - 201709","m20: GBM - MACR - 201709", "m21: GBM - MACR - caret - 201709")

# AUCs
models_AUC_201709 <- c(m1_AUROC_201709, m2_AUROC_201709, m3_AUROC_201709, m4_AUROC_201709, m5_AUROC_201709, m6_AUROC_201709, m7_AUROC_201709, m8_AUROC_201709, m9_AUROC_201709, m10_AUROC_201709, 
                       m11_AUROC_201709, m12_AUROC_201709, m13_AUROC_201709, m14_AUROC_201709, m15_AUROC_201709, m16_AUROC_201709, m17_AUROC_201709, m18_AUROC_201709, m19_AUROC_201709, m20_AUROC_201709, 
                       m21_AUROC_201709)
# KS
models_KS_201709 <- c(m1_KS_201709, m2_KS_201709, m3_KS_201709, m4_KS_201709, m5_KS_201709, m6_KS_201709, m7_KS_201709, m8_KS_201709, m9_KS_201709, m10_KS_201709, m11_KS_201709, m12_KS_201709, m13_KS_201709, m14_KS_201709, m15_KS_201709, 
                      m16_KS_201709, m17_KS_201709, m18_KS_201709, m19_KS_201709, m20_KS_201709, m21_KS_201709)

# Gini
models_Gini_201709 <- c(m1_Gini_201709, m2_Gini_201709, m3_Gini_201709, m4_Gini_201709, m5_Gini_201709, m6_Gini_201709, m7_Gini_201709, m8_Gini_201709, m9_Gini_201709, m10_Gini_201709, 
                        m11_Gini_201709, m12_Gini_201709, m13_Gini_201709, m14_Gini_201709, m15_Gini_201709, m16_Gini_201709, m17_Gini_201709, m18_Gini_201709, m19_Gini_201709, m20_Gini_201709, 
                        m21_Gini_201709)


# Accuraccy

#models_accuracy_201709 <- c(m1_accuracy_201709, m2_accuracy_201709, m3_accuracy_201709, m4_accuracy_201709, m5_accuracy_201709, m6_accuracy_201709, m7_accuracy_201709, m8_accuracy_201709, m9_accuracy_201709, 
#                            m10_accuracy_201709, m11_accuracy_201709, m12_accuracy_201709, m13_accuracy_201709, m14_accuracy_201709, m15_accuracy_201709, m16_accuracy_201709, m17_accuracy_201709, 
#                            m18_accuracy_201709, m19_accuracy_201709, m20_accuracy_201709, m21_accuracy_201709)


# Juntando todo
model_performance_metric_201709 <- as.data.frame(cbind(models_201709, models_AUC_201709, models_KS_201709, models_Gini_201709))

model_performance_metric_201709 <- model_performance_metric_201709[order(models_Gini_201709, decreasing = T),] 


# Colnames 
colnames(model_performance_metric_201709) <- c("Modelos_201709", "AUC_201709", "KS_201709", "Gini_201709")

# Display Performance Reports
kable(model_performance_metric_201709, caption ="Comparación de modelos - OOS - 201709")


ruta_modelos <- "D:/edwin/Modelo scoring cobranza"

modelos_cob <- ls()



save(model_rl_1,model_rl_3_tun,model_rl_4,model_rl_5_tun,model_6_ad_1,model_7_ad_tun_1,
     model_8_ad_2,model_9_ad_tun_2, model_10_ad_1, model_11_ad_tun_1, model_12_ad_2,model_13_ad_tun_1,
     model_14_rf_1,model_15_RF_tun_1, model_16_rf_2,model_17_rf_tun_2, model_18_gbm_1, 
     model_19_GBM_tun_1, model_20_gbm_2, model_21_rf_tun_2, model_performance_metric,
     model_performance_metric_201706,model_performance_metric_201707, model_performance_metric_201708, model_performance_metric_201709, file = paste0(ruta_modelos,"/Modelo_cobranza_parte1.rda"))



performance_total <- cbind(model_performance_metric, model_performance_metric_201706, model_performance_metric_201707, model_performance_metric_201708, model_performance_metric_201709)

write.csv(performance_total, paste0(ruta_modelos,"/Challenge_Cobranza_parte1.csv"), row.names = F)









#############################################
# POR AHORA
############################################

#####################################
### parte I
#####################################

# tABLA DE PERFORMANCE - OOS
models <- c('m1:Regresión Logística - MASR', 'm2:Regresión Logística corte optimo - MASR',
            'm3:Regresión Logística - MASR - caret','m4:Regresión Logística - MACR', 
            'm5:Regresión Logística - MACR - caret', "m6: AD - cart - MASR",
            "m7: AD - cart - MASR - caret", "m8: AD - cart - MACR",
            "m9: AD - cart - MACR - caret", "m10: AD - ctree - MAsR",
            "m11: AD - ctree - MASR - caret", "m12: AD - ctree - MACR",
            "m13: AD - ctree - MACR - caret", "m14: RF - cart - MAsR",
            "m15: RF - cart - MASR - caret", "m16: RF - cart - MACR",
            "m17: RF - cart - MACR - caret", "m18: GBM - MAsR",
            "m19: GBM - MASR - caret","m20: GBM - MACR", "m21: GBM - MACR - caret")

# AUCs
models_AUC <- c(m1_AUROC, m2_AUROC, m3_AUROC, m4_AUROC, m5_AUROC, m6_AUROC, m7_AUROC, m8_AUROC, m9_AUROC, m10_AUROC, 
                m11_AUROC, m12_AUROC, m13_AUROC, m14_AUROC, m15_AUROC, m16_AUROC, m17_AUROC)
# KS
models_KS <- c(m1_KS, m2_KS, m3_KS, m4_KS, m5_KS, m6_KS, m7_KS, m8_KS, m9_KS, m10_KS, m11_KS, m12_KS, m13_KS, m14_KS, m15_KS, 
               m16_KS, m17_KS)

# Gini
models_Gini <- c(m1_Gini, m2_Gini, m3_Gini, m4_Gini, m5_Gini, m6_Gini, m7_Gini, m8_Gini, m9_Gini, m10_Gini, 
                 m11_Gini, m12_Gini, m13_Gini, m14_Gini, m15_Gini, m16_Gini, m17_Gini)


# Accuraccy

models_accuracy <- c(round(m1_accuracy,2), round(m2_accuracy,2), round(m3_accuracy,2), round(m4_accuracy,2), round(m5_accuracy,2), round(m6_accuracy,2), round(m7_accuracy,2), round(m8_accuracy,2), round(m9_accuracy,2), 
                     round(m10_accuracy,2), round(m11_accuracy,2), round(m12_accuracy,2), round(m13_accuracy,2), round(m14_accuracy,2), round(m15_accuracy,2), round(m16_accuracy,2), round(m17_accuracy,2))


#Tiempo

models_time <- c(round(time_m1,2), "NA", round(time_m3,2), round(time_m4,2), round(time_m5,2), round(time_m6,2), round(time_m7,2), round(time_m8,2), round(time_m9,2), round(time_m10,2), round(time_m11,2), round(time_m12,2), round(time_m13,2),
                 round(time_m14,2), round(time_m15,2), round(time_m16,2), round(time_m17,2))

# Juntando todo
model_performance_metric <- as.data.frame(cbind(models, models_AUC, models_KS, models_Gini, models_accuracy, models_time))


model_performance_metric <- model_performance_metric[order(models_Gini, decreasing = T),] 

# Colnames 
colnames(model_performance_metric) <- c("Modelos", "AUC", "KS", "Gini","Accuracy", "Tiempo_proceso_Modelo_minuto")

# Display Performance Reports
kable(model_performance_metric, caption ="Comparación de modelos - OOS")








#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VI          MODELOS EN OOT
#---------------------------------------------------------------------------------------
########################################################################################


#############################################################
# PERIODO - 201706
#############################################################


#'m1:Regresión Logística - MASR'

set.seed(1992)
predic_model_rl_1_201706 <- predict(model_rl_1,type='response',b201706)

tb1_201706 <- table(b201706$flag_pago,ifelse(predic_model_rl_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb1_201706

m1_accuracy_201706 <- (121 + 280)/nrow(b201706)
m1_accuracy_201706

set.seed(1992)
#score test data set
b201706$m1_score_201706 <- predict(model_rl_1,type='response',b201706)
m1_pred_201706 <- prediction(b201706$m1_score_201706, b201706$flag_pago)
m1_perf_201706 <- performance(m1_pred_201706,"tpr","fpr")

#ROC
plot(m1_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision_201706 <- performance(m1_pred_201706, measure = "prec", x.measure = "rec")
plot(m1_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc_201706 <- performance(m1_pred_201706, measure = "acc")
plot(m1_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS_201706 <- round(max(attr(m1_perf_201706,'y.values')[[1]]-attr(m1_perf_201706,'x.values')[[1]])*100, 2)
m1_AUROC_201706 <- round(performance(m1_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini_201706 <- (2*m1_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m1_AUROC_201706,"\tKS: ", m1_KS_201706, "\tGini:", m1_Gini_201706, "\tAccuracy:", m1_accuracy_201706,"\n")



#'m2:Regresión Logística corte optimo - MASR'



tb2_201706 <- table(b201706$flag_pago,ifelse(predic_model_rl_1_201706>optCutOff,"X1_Bueno","X0_Malo"))
tb2_201706
m2_accuracy_201706 <-  ( 121+ 280)/nrow(b201706)
m2_accuracy_201706
#0.7316906

set.seed(1992)
#score test data set
b201706$m2_score <- predict(model_rl_1,type='response',b201706)
m2_pred_201706 <- prediction(ifelse(b201706$m1_score>optCutOff,1,0), b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf_201706 <- performance(m2_pred_201706,"tpr","fpr")

#ROC
plot(m2_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision_201706 <- performance(m2_pred_201706, measure = "prec", x.measure = "rec")
plot(m2_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc_201706 <- performance(m2_pred_201706, measure = "acc")
plot(m2_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS_201706 <- round(max(attr(m2_perf_201706,'y.values')[[1]]-attr(m2_perf_201706,'x.values')[[1]])*100, 2)
m2_AUROC_201706 <- round(performance(m2_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini_201706 <- (2*m2_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m2_AUROC_201706,"\tKS: ", m2_KS_201706, "\tGini:", m2_Gini_201706, "\tAccuracy:", m2_accuracy_201706,"\n")



#'m3:Regresión Logística - MASR - caret'

#score test data set
b201706$m3_score <- predict(model_rl_3_tun,type='prob',newdata=b201706)[2]
m3_pred_201706 <- prediction(b201706$m3_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3_201706 <- table(ifelse(b201706$m3_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb3_201706

m3_accuracy_201706 <- (121+280)/nrow(b201706)
m3_accuracy_201706

m3_perf_201706 <- performance(m3_pred_201706,"tpr","fpr")

#ROC
plot(m3_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic - 201706 Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision_201706 <- performance(m3_pred_201706, measure = "prec", x.measure = "rec")
plot(m3_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc_201706 <- performance(m3_pred_201706, measure = "acc")
plot(m3_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS_201706 <- round(max(attr(m3_perf_201706,'y.values')[[1]]-attr(m3_perf_201706,'x.values')[[1]])*100, 2)
m3_AUROC_201706 <- round(performance(m3_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini_201706 <- (2*m3_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m3_AUROC_201706,"\tKS: ", m3_KS_201706, "\tGini:", m3_Gini_201706,"\tAccuracy:", m3_accuracy_201706, "\n")


#'m4:Regresión Logística - MACR'

predic_model_rl_4_201706 <- predict(model_rl_4, newdata = b201706,type = "response")

summary(predic_model_rl_4_201706)


tb4_201706 <- table(b201706$flag_pago,ifelse(predic_model_rl_4_201706>0.5,"X1_Bueno","X0_Malo"))
tb4_201706

(m4_accuracy_201706 <- (122 +  280)/nrow(b201706))

#0.6711185

#score test data set
b201706$m4_score <- predict(model_rl_4,type='response',b201706)
m4_pred_201706 <- prediction(b201706$m4_score, b201706$flag_pago)
m4_perf_201706 <- performance(m4_pred_201706,"tpr","fpr")

#ROC
plot(m4_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision_201706 <- performance(m4_pred_201706, measure = "prec", x.measure = "rec")
plot(m4_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc_201706 <- performance(m4_pred_201706, measure = "acc")
plot(m4_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS_201706 <- round(max(attr(m4_perf_201706,'y.values')[[1]]-attr(m4_perf_201706,'x.values')[[1]])*100, 2)
m4_AUROC_201706 <- round(performance(m4_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini_201706 <- (2*m4_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m4_AUROC_201706,"\tKS: ", m4_KS_201706, "\tGini:", m4_Gini_201706,"\tAccuracy:", m4_accuracy_201706, "\n")



#'m5:Regresión Logística - MACR - caret'

#score test data set
b201706$m5_score <- predict(model_rl_5_tun,type='prob',newdata=b201706)[2]
m5_pred_201706 <- prediction(b201706$m5_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb201706 <- table(ifelse(b201706$m5_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb201706

m5_accuracy_201706 <- (122+280)/nrow(b201706)
m5_accuracy_201706
#0.6711185

m5_perf_201706 <- performance(m5_pred_201706,"tpr","fpr")

#ROC
plot(m5_perf_201706, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision_201706 <- performance(m5_pred_201706, measure = "prec", x.measure = "rec")
plot(m5_perf_precision_201706, main="m1 Logistic - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc_201706 <- performance(m5_pred_201706, measure = "acc")
plot(m5_perf_acc_201706, main="m1 Logistic - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS_201706 <- round(max(attr(m5_perf_201706,'y.values')[[1]]-attr(m5_perf_201706,'x.values')[[1]])*100, 2)
m5_AUROC_201706 <- round(performance(m5_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini_201706 <- (2*m5_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m5_AUROC_201706,"\tKS: ", m5_KS_201706, "\tGini:", m5_Gini_201706, "\tAccuracy:", m5_accuracy_201706,"\n")


#"m6: AD - cart - MASR"

predic_model_6_ad_1_201706 <- predict(model_6_ad_1, newdata = b201706,type = "prob")[,2]

summary(predic_model_6_ad_1_201706)


tb6_201706 <- table(b201706$flag_pago,ifelse(predic_model_6_ad_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb6_201706

m6_accuracy_201706 <- (128 + 273)/nrow(b201706)
m6_accuracy_201706
#0.6694491

#score test data set
b201706$m6_ad_score <- predict(model_6_ad_1,type='prob',b201706)[,2]
m6_pred_201706 <- prediction(b201706$m6_ad_score, b201706$flag_pago)
m6_perf_201706 <- performance(m6_pred_201706,"tpr","fpr")

#ROC
plot(m6_perf_201706, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision_201706 <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision_201706, main="m6 Árbol de decisión - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc_201706 <- performance(m6_pred_201706, measure = "acc")
plot(m6_perf_acc_201706, main="m6 Árbol de decisión - 201706:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS_201706 <- round(max(attr(m6_perf_201706,'y.values')[[1]]-attr(m6_perf_201706,'x.values')[[1]])*100, 2)
m6_AUROC_201706 <- round(performance(m6_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini_201706 <- (2*m6_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m6_AUROC_201706,"\tKS: ", m6_KS_201706, "\tGini:", m6_Gini_201706, "\tAccuracy:", m6_accuracy_201706,"\n")


#"m7: AD - cart - MASR - caret"

#score test data set
b201706$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=b201706)[2]
m7_pred_201706 <- prediction(b201706$m7_score, b201706$flag_pago)

tb7_201706 <- table(ifelse(b201706$m7_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb7_201706

m7_accuracy_201706 <- (123+ 277)/nrow(b201706)
m7_accuracy_201706
#0.6677796

m7_perf_201706 <- performance(m7_pred_201706,"tpr","fpr")

#ROC
plot(m7_perf_201706, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision_201706 <- performance(m7_pred_201706, measure = "prec", x.measure = "rec")
plot(m7_perf_precision_201706, main="m7 Arbol de decision - 201706:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc_201706 <- performance(m7_pred_201706, measure = "acc")
plot(m7_perf_acc_201706, main="m7 Arbol tuneado:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m7_KS_201706 <- round(max(attr(m7_perf_201706,'y.values')[[1]]-attr(m7_perf_201706,'x.values')[[1]])*100, 2)
m7_AUROC_201706 <- round(performance(m7_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini_201706 <- (2*m7_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")


#"m8: AD - cart - MACR"

predic_model_8_ad_2_201706 <- predict(model_8_ad_2, newdata = b201706,type = "prob")[,2]

summary(predic_model_8_ad_2_201706)


tb8_201706 <- table(b201706$flag_pago,ifelse(predic_model_8_ad_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb8_201706

m8_accuracy_201706 <- (118 +  283)/nrow(b201706)
m8_accuracy_201706
#0.6694491

#score test data set
b201706$m8_score <- predict(model_8_ad_2,type='prob',b201706)[,2]
m8_pred_201706 <- prediction(b201706$m8_score, b201706$flag_pago)
m8_perf_201706 <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf_201706, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision_201706 <- performance(m8_pred_201706, measure = "prec", x.measure = "rec")
plot(m8_perf_precision_201706, main="m8 Arbol de decision - rpart:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m8_perf_acc_201706 <- performance(m8_pred_201706, measure = "acc")
plot(m8_perf_acc_201706, main="m8 Arbol de decision - rpart:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m8_KS_201706 <- round(max(attr(m8_perf_201706,'y.values')[[1]]-attr(m8_perf_201706,'x.values')[[1]])*100, 2)
m8_AUROC_201706 <- round(performance(m8_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini_201706 <- (2*m8_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m8_AUROC_201706,"\tKS: ", m8_KS_201706, "\tGini:", m8_Gini_201706,"\tAccuracy:", m8_accuracy_201706, "\n")


#"m9: AD - cart - MACR - caret"

#score test data set
b201706$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=b201706)[,2]
m9_pred_201706 <- prediction(b201706$m9_score, b201706$flag_pago)


tb9_201706 <- table(ifelse(b201706$m9_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb9_201706

m9_accuracy_201706 <- (117+279)/nrow(b201706)
m9_accuracy_201706
#0.6611018

m9_perf_201706 <- performance(m9_pred_201706,"tpr","fpr")

#ROC
plot(m9_perf_201706, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision_201706 <- performance(m9_pred_201706, measure = "prec", x.measure = "rec")
plot(m9_perf_precision_201706, main="m9 Arbol de decision - caret:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m9_perf_acc_201706 <- performance(m9_pred_201706, measure = "acc")
plot(m9_perf_acc_201706, main="m9 Arbol de decision - caret:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m9_KS_201706 <- round(max(attr(m9_perf_201706,'y.values')[[1]]-attr(m9_perf_201706,'x.values')[[1]])*100, 2)
m9_AUROC_201706 <- round(performance(m9_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini_201706 <- (2*m9_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m9_AUROC_201706,"\tKS: ", m9_KS_201706, "\tGini:", m9_Gini_201706, "\tAccuracy:", m9_accuracy_201706,"\n")


#"m10: AD - ctree - MAsR"


predic_model_10_ad_1_201706 <- sapply(predict(model_10_ad_1, newdata = b201706,type = "prob"),'[[',2)

summary(predic_model_10_ad_1_201706)

tb10_201706 <- table(b201706$flag_pago,ifelse(predic_model_10_ad_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb10_201706

m10_accuracy_201706 <- (122 + 278)/nrow(b201706)
m10_accuracy_201706
#0.6677796

#score test data set
b201706$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = b201706,type = "prob")))))[,2]

m10_pred_201706 <- prediction(b201706$m10_ad_score, b201706$flag_pago)
m10_perf_201706 <- performance(m10_pred_201706,"tpr","fpr")

#ROC
plot(m10_perf_201706, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision_201706 <- performance(m10_pred_201706, measure = "prec", x.measure = "rec")
plot(m10_perf_precision_201706, main="m6 Árbol de decisión - CTREE:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m10_perf_acc_201706 <- performance(m10_pred_201706, measure = "acc")
plot(m10_perf_acc_201706, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m10_KS_201706 <- round(max(attr(m10_perf_201706,'y.values')[[1]]-attr(m10_perf_201706,'x.values')[[1]])*100, 2)
m10_AUROC_201706 <- round(performance(m10_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini_201706 <- (2*m10_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m10_AUROC_201706,"\tKS: ", m10_KS_201706, "\tGini:", m10_Gini_201706, "\tAccuracy:", m10_accuracy_201706,"\n")


#"m11: AD - ctree - MASR - caret"


#score test data set
b201706$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=b201706)[2]
m11_pred_201706 <- prediction(b201706$m11_score, b201706$flag_pago)

tb11_201706 <- table(ifelse(b201706$m11_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
tb11_201706

m11_accuracy_201706 <- (119+281)/nrow(b201706)
m11_accuracy_201706
#0.6677796

m11_perf_201706 <- performance(m11_pred_201706,"tpr","fpr")

#ROC
plot(m11_perf_201706, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision_201706 <- performance(m11_pred_201706, measure = "prec", x.measure = "rec")
plot(m11_perf_precision_201706, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m11_perf_acc_201706 <- performance(m11_pred_201706, measure = "acc")
plot(m11_perf_acc_201706, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m11_KS_201706 <- round(max(attr(m11_perf_201706,'y.values')[[1]]-attr(m11_perf_201706,'x.values')[[1]])*100, 2)
m11_AUROC_201706 <- round(performance(m11_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini_201706 <- (2*m11_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m11_AUROC_201706,"\tKS: ", m11_KS_201706, "\tGini:", m11_Gini_201706, "\tAccuracy:", m11_accuracy_201706,"\n")



#"m12: AD - ctree - MACR"

predic_model_12_ad_2_201706 <- sapply(predict(model_12_ad_2, newdata = b201706,type = "prob"),'[[',2)

summary(predic_model_12_ad_2_201706)


tb12_201706 <- table(b201706$flag_pago,ifelse(predic_model_12_ad_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb12_201706

m12_accuracy_201706 <- (130 + 276)/nrow(b201706)
m12_accuracy_201706
#0.6777963

#score test data set
b201706$m12_score <- sapply(predict(model_12_ad_2, newdata = b201706,type = "prob"),'[[',2)
m12_pred_201706 <- prediction(b201706$m12_score, b201706$flag_pago)
m12_perf_201706 <- performance(m12_pred_201706,"tpr","fpr")

#ROC
plot(m12_perf_201706, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision_201706 <- performance(m12_pred_201706, measure = "prec", x.measure = "rec")
plot(m12_perf_precision_201706, main="m12 Arbol de decision - CTREE :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m12_perf_acc_201706 <- performance(m12_pred_201706, measure = "acc")
plot(m12_perf_acc_201706, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m12_KS_201706 <- round(max(attr(m12_perf_201706,'y.values')[[1]]-attr(m12_perf_201706,'x.values')[[1]])*100, 2)
m12_AUROC_201706 <- round(performance(m12_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini_201706 <- (2*m12_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m12_AUROC_201706,"\tKS: ", m12_KS_201706, "\tGini:", m12_Gini_201706,"\tAccuracy:", m12_accuracy_201706, "\n")



#"m13: AD - ctree - MACR - caret"

#score test data set
b201706$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=b201706)[,2]
m13_pred_201706 <- prediction(b201706$m13_score, b201706$flag_pago)


table(ifelse(b201706$m13_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m13_accuracy_201706 <- (131+268)/nrow(b201706)
m13_accuracy_201706
#0.6661102

m13_perf_201706 <- performance(m13_pred_201706,"tpr","fpr")

#ROC
plot(m13_perf_201706, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision_201706 <- performance(m13_pred_201706, measure = "prec", x.measure = "rec")
plot(m13_perf_precision_201706, main="m13 Arbol de decision - CTREE:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m13_perf_acc_201706 <- performance(m13_pred_201706, measure = "acc")
plot(m13_perf_acc_201706, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m13_KS_201706 <- round(max(attr(m13_perf_201706,'y.values')[[1]]-attr(m13_perf_201706,'x.values')[[1]])*100, 2)
m13_AUROC_201706 <- round(performance(m13_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini_201706 <- (2*m13_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m13_AUROC_201706,"\tKS: ", m13_KS_201706, "\tGini:", m13_Gini_201706, "\tAccuracy:", m13_accuracy_201706,"\n")


#"m14: RF - cart - MAsR"



predic_model_14_rf_1_201706 <- predict(model_14_rf_1, newdata = b201706,type = "prob")[,2]

summary(predic_model_14_rf_1_201706)

tb14_201706 <- table(b201706$flag_pago,ifelse(predic_model_14_rf_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb14_201706

m14_accuracy_20176 <- (124 + 275)/nrow(b201706)
m14_accuracy_20176
#0.6661102

#score test data set
b201706$m14_rf_score <- predict(model_14_rf_1, newdata = b201706,type = "prob")[,2]

m14_pred_201706 <- prediction(b201706$m14_rf_score, b201706$flag_pago)
m14_perf_201706 <- performance(m14_pred_201706,"tpr","fpr")

#ROC
plot(m14_perf_201706, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision_201706 <- performance(m14_pred_201706, measure = "prec", x.measure = "rec")
plot(m14_perf_precision_201706, main="m14 Random Forest:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m14_perf_acc_201706 <- performance(m14_pred_201706, measure = "acc")
plot(m14_perf_acc_201706, main="m14 Random Forest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m14_KS_201706 <- round(max(attr(m14_perf_201706,'y.values')[[1]]-attr(m14_perf_201706,'x.values')[[1]])*100, 2)
m14_AUROC_201706 <- round(performance(m14_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini_201706 <- (2*m14_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m14_AUROC_201706,"\tKS: ", m14_KS_201706, "\tGini:", m14_Gini_201706, "\tAccuracy:", m14_accuracy_20176,"\n")


#"m15: RF - cart - MASR - caret"

#score test data set
b201706$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=b201706)[2]
m15_pred_201706 <- prediction(b201706$m15_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m15_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m15_accuracy_201706 <- (641+1501)/nrow(b201706)
m15_accuracy_201706
#0.7330595

m15_perf_201706 <- performance(m15_pred_201706,"tpr","fpr")

#ROC
plot(m15_perf_201706, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision_201706 <- performance(m15_pred_201706, measure = "prec", x.measure = "rec")
plot(m15_perf_precision_201706, main="m15 RandomForest:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m15_perf_acc_201706 <- performance(m15_pred_201706, measure = "acc")
plot(m15_perf_acc_201706, main="m15 RandomForest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m15_KS_201706 <- round(max(attr(m15_perf_201706,'y.values')[[1]]-attr(m15_perf_201706,'x.values')[[1]])*100, 2)
m15_AUROC_201706 <- round(performance(m15_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini_201706 <- (2*m15_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m15_AUROC_201706,"\tKS: ", m15_KS_201706, "\tGini:", m15_Gini_201706, "\tAccuracy:", m15_accuracy_201706,"\n")


#"m16: RF - cart - MACR"

predic_model_16_rf_2_201706 <- predict(model_16_rf_2, newdata = b201706,type = "prob")[,2]

summary(predic_model_16_rf_2_201706)

tb16_201706 <- table(b201706$flag_pago,ifelse(predic_model_16_rf_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb16_201706


m16_accuracy_201706 <- (640 + 1553)/nrow(b201706)
m16_accuracy_201706
#0.7510274

#score test data set

b201706$m16_rf_2_score <- predict(model_16_rf_2, newdata = b201706,type = "prob")[,2]

m16_pred_201706 <- prediction(b201706$m16_rf_2_score, b201706$flag_pago)
m16_perf_201706 <- performance(m16_pred_201706,"tpr","fpr")

#ROC
plot(m16_perf_201706, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision_201706 <- performance(m16_pred_201706, measure = "prec", x.measure = "rec")
plot(m16_perf_precision_201706, main="m16 RandomForest :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m16_perf_acc_201706 <- performance(m16_pred_201706, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS_201706 <- round(max(attr(m16_perf_201706,'y.values')[[1]]-attr(m16_perf_201706,'x.values')[[1]])*100, 2)
m16_AUROC_201706 <- round(performance(m16_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini_201706 <- (2*m16_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m16_AUROC_201706,"\tKS: ", m16_KS_201706, "\tGini:", m16_Gini_201706,"\tAccuracy:", m16_accuracy_201706, "\n")



#"m17: RF - cart - MACR - caret"

#score test data set
b201706$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=b201706)[,2]
m17_pred_201706 <- prediction(b201706$m17_score, b201706$flag_pago)

table(ifelse(b201706$m17_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m17_accuracy_201706 <- (639+1515)/nrow(b201706)
m17_accuracy_201706
#0.7376712

m17_perf_201706 <- performance(m17_pred_201706,"tpr","fpr")

#ROC
plot(m17_perf_201706, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision_201706 <- performance(m17_pred_201706, measure = "prec", x.measure = "rec")
plot(m17_perf_precision_201706, main="m17 RandomForest:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m17_perf_acc_201706 <- performance(m17_pred_201706, measure = "acc")
plot(m17_perf_acc_201706, main="m17 RandomForest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m17_KS_201706 <- round(max(attr(m17_perf_201706,'y.values')[[1]]-attr(m17_perf_201706,'x.values')[[1]])*100, 2)
m17_AUROC_201706 <- round(performance(m17_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini_201706 <- (2*m17_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m17_AUROC_201706,"\tKS: ", m17_KS_201706, "\tGini:", m17_Gini_201706, "\tAccuracy:", m17_accuracy_201706,"\n")


#"m18: GBM - MAsR"

predic_model_18_gbm_1_201706 <- predict(model_18_gbm_1, newdata = b201706,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1_201706, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18_201706 <- table(b201706$flag_pago,ifelse(predic_model_18_gbm_1_201706>0.5,"X1_Bueno","X0_Malo"))
tb18_201706

m18_accuracy_201706 <- (622+1518)/nrow(b201706)
m18_accuracy_201706

#0.7323751

#score test data set
b201706$m18_GBM_score <- predict(model_18_gbm_1, newdata = b201706,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred_201706 <- prediction(b201706$m18_GBM_score, b201706$flag_pago)
m18_perf_201706 <- performance(m18_pred_201706,"tpr","fpr")

#ROC
plot(m18_perf_201706, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision_201706 <- performance(m18_pred_201706, measure = "prec", x.measure = "rec")
plot(m18_perf_precision_201706, main="m18 GBM:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m18_perf_acc_201706 <- performance(m18_pred_201706, measure = "acc")
plot(m18_perf_acc_201706, main="m18 GBM:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m18_KS_201706 <- round(max(attr(m18_perf_201706,'y.values')[[1]]-attr(m18_perf_201706,'x.values')[[1]])*100, 2)
m18_AUROC_201706 <- round(performance(m18_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini_201706 <- (2*m18_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m18_AUROC_201706,"\tKS: ", m18_KS_201706, "\tGini:", m18_Gini_201706, "\tAccuracy:", m18_accuracy_201706,"\n")


#"m19: GBM - MASR - caret"

#score test data set
b201706$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=b201706)[2]
m19_pred_201706 <- prediction(b201706$m19_score, b201706$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201706$m19_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m19_accuracy_201706 <- (630+1508)/nrow(b201706)
m19_accuracy_201706
#0.7316906

m19_perf_201706 <- performance(m19_pred_201706,"tpr","fpr")

#ROC
plot(m19_perf_201706, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision_201706 <- performance(m19_pred_201706, measure = "prec", x.measure = "rec")
plot(m19_perf_precision_201706, main="m19 GBM:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m19_perf_acc_201706 <- performance(m19_pred_201706, measure = "acc")
plot(m19_perf_acc_201706, main="m19 GBM:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m19_KS_201706 <- round(max(attr(m19_perf_201706,'y.values')[[1]]-attr(m19_perf_201706,'x.values')[[1]])*100, 2)
m19_AUROC_201706 <- round(performance(m19_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini_201706 <- (2*m19_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m19_AUROC_201706,"\tKS: ", m19_KS_201706, "\tGini:", m19_Gini_201706, "\tAccuracy:", m19_accuracy_201706,"\n")



#"m20: GBM - MACR"



predic_model_20_gbm_2_201706 <- predict(model_20_gbm_2, newdata = b201706,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2_201706, n=30)


summary(predic_model_20_gbm_2_201706)

tb20_201706 <- table(b201706$flag_pago,ifelse(predic_model_20_gbm_2_201706>0.5,"X1_Bueno","X0_Malo"))
tb20_201706


m20_accuracy_201706 <- (636 + 1514)/nrow(b201706)
m20_accuracy_201706
#0.7363014

#score test data set


b201706$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = b201706,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred_201706 <- prediction(b201706$m20_GBM_2_score, b201706$flag_pago)
m20_perf_201706 <- performance(m20_pred_201706,"tpr","fpr")

#ROC
plot(m20_perf_201706, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision_201706 <- performance(m20_pred_201706, measure = "prec", x.measure = "rec")
plot(m20_perf_precision_201706, main="m20 GBM :Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m20_perf_acc_201706 <- performance(m20_pred_201706, measure = "acc")
plot(m20_perf_acc_201706, main="m20 GBM :Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m20_KS_201706 <- round(max(attr(m20_perf_201706,'y.values')[[1]]-attr(m20_perf_201706,'x.values')[[1]])*100, 2)
m20_AUROC_201706 <- round(performance(m20_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini_201706 <- (2*m20_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m20_AUROC_201706,"\tKS: ", m20_KS_201706, "\tGini:", m20_Gini_201706,"\tAccuracy:", m20_accuracy_201706, "\n")


#"m21: GBM - MACR - caret"


#score test data set
b201706$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=b201706)[,2]
m21_pred_201706 <- prediction(b201706$m21_score, b201706$flag_pago)

table(ifelse(b201706$m21_score>0.5,"X1_Bueno","X0_Malo"), b201706$flag_pago)
m21_accuracy_201706 <- (638+1513)/nrow(b201706)
m21_accuracy_201706
#0.7366438

m21_perf_201706 <- performance(m21_pred_201706,"tpr","fpr")

#ROC
plot(m21_perf_201706, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance - 201706")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision_201706 <- performance(m21_pred_201706, measure = "prec", x.measure = "rec")
plot(m21_perf_precision_201706, main="m21 GBM:Precision/recall curve - 201706")


# Plot accuracy as function of threshold
m21_perf_acc_201706 <- performance(m21_pred_201706, measure = "acc")
plot(m21_perf_acc_201706, main="m21 RandomForest:Accuracy as function of threshold - 201706")

#KS, Gini & AUC m1
m21_KS_201706 <- round(max(attr(m21_perf_201706,'y.values')[[1]]-attr(m21_perf_201706,'x.values')[[1]])*100, 2)
m21_AUROC_201706 <- round(performance(m21_pred_201706, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini_201706 <- (2*m21_AUROC_201706 - 100)

###########################################################################
cat("AUROC: ",m21_AUROC_201706,"\tKS: ", m21_KS_201706, "\tGini:", m21_Gini_201706, "\tAccuracy:", m21_accuracy_201706,"\n")



# tABLA DE PERFORMANCE - OOS
models_201706 <- c('m1:Regresión Logística - MASR - 201706', 'm2:Regresión Logística corte optimo - MASR - 201706',
                   'm3:Regresión Logística - MASR - caret - 201706','m4:Regresión Logística - MACR - 201706', 
                   'm5:Regresión Logística - MACR - caret - 201706', "m6: AD - cart - MASR - 201706",
                   "m7: AD - cart - MASR - caret - 201706", "m8: AD - cart - MACR - 201706",
                   "m9: AD - cart - MACR - caret - 201706", "m10: AD - ctree - MAsR - 201706",
                   "m11: AD - ctree - MASR - caret - 201706", "m12: AD - ctree - MACR - 201706",
                   "m13: AD - ctree - MACR - caret - 201706", "m14: RF - cart - MAsR - 201706",
                   "m15: RF - cart - MASR - caret - 201706", "m16: RF - cart - MACR - 201706",
                   "m17: RF - cart - MACR - caret - 201706")

# AUCs
models_AUC_201706 <- c(m1_AUROC_201706, m2_AUROC_201706, m3_AUROC_201706, m4_AUROC_201706, m5_AUROC_201706, m6_AUROC_201706, m7_AUROC_201706, m8_AUROC_201706, m9_AUROC_201706, m10_AUROC_201706, 
                       m11_AUROC_201706, m12_AUROC_201706, m13_AUROC_201706, m14_AUROC_201706, m15_AUROC_201706, m16_AUROC_201706, m17_AUROC_201706)
# KS
models_KS_201706 <- c(m1_KS_201706, m2_KS_201706, m3_KS_201706, m4_KS_201706, m5_KS_201706, m6_KS_201706, m7_KS_201706, m8_KS_201706, m9_KS_201706, m10_KS_201706, m11_KS_201706, m12_KS_201706, m13_KS_201706, m14_KS_201706, m15_KS_201706, 
                      m16_KS_201706, m17_KS_201706)

# Gini
models_Gini_201706 <- c(m1_Gini_201706, m2_Gini_201706, m3_Gini_201706, m4_Gini_201706, m5_Gini_201706, m6_Gini_201706, m7_Gini_201706, m8_Gini_201706, m9_Gini_201706, m10_Gini_201706, 
                        m11_Gini_201706, m12_Gini_201706, m13_Gini_201706, m14_Gini_201706, m15_Gini_201706, m16_Gini_201706, m17_Gini_201706)


# Accuraccy

#models_accuracy_201706 <- c(m1_accuracy_201706, m2_accuracy_201706, m3_accuracy_201706, m4_accuracy_201706, m5_accuracy_201706, m6_accuracy_201706, m7_accuracy_201706, m8_accuracy_201706, m9_accuracy_201706, 
#                     m10_accuracy_201706, m11_accuracy_201706, m12_accuracy_201706, m13_accuracy_201706, m14_accuracy_201706, m15_accuracy_201706, m16_accuracy_201706, m17_accuracy_201706, 
#                     m18_accuracy_201706, m19_accuracy_201706, m20_accuracy_201706, m21_accuracy_201706)


# Juntando todo
model_performance_metric_201706 <- as.data.frame(cbind(models_201706, models_AUC_201706, models_KS_201706, models_Gini_201706))

model_performance_metric_201706 <- model_performance_metric_201706[order(models_Gini_201706, decreasing = T),] 


# Colnames 
colnames(model_performance_metric_201706) <- c("Modelos_201706", "AUC_201706", "KS_201706", "Gini_201706","Accuracy_201706")

# Display Performance Reports
kable(model_performance_metric_201706, caption ="Comparación de modelos - OOS - 201706")


#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VI          MODELOS EN OOT
#---------------------------------------------------------------------------------------
########################################################################################


#############################################################
# PERIODO - 201707
#############################################################


#'m1:Regresión Logística - MASR'

set.seed(1992)
predic_model_rl_1_201707 <- predict(model_rl_1,type='response',b201707)

tb1_201707 <- table(b201707$flag_pago,ifelse(predic_model_rl_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb1_201707

m1_accuracy_201707 <- (121 + 280)/nrow(b201707)
m1_accuracy_201707

set.seed(1992)
#score test data set
b201707$m1_score_201707 <- predict(model_rl_1,type='response',b201707)
m1_pred_201707 <- prediction(b201707$m1_score_201707, b201707$flag_pago)
m1_perf_201707 <- performance(m1_pred_201707,"tpr","fpr")

#ROC
plot(m1_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision_201707 <- performance(m1_pred_201707, measure = "prec", x.measure = "rec")
plot(m1_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc_201707 <- performance(m1_pred_201707, measure = "acc")
plot(m1_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS_201707 <- round(max(attr(m1_perf_201707,'y.values')[[1]]-attr(m1_perf_201707,'x.values')[[1]])*100, 2)
m1_AUROC_201707 <- round(performance(m1_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini_201707 <- (2*m1_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m1_AUROC_201707,"\tKS: ", m1_KS_201707, "\tGini:", m1_Gini_201707, "\tAccuracy:", m1_accuracy_201707,"\n")



#'m2:Regresión Logística corte optimo - MASR'



tb2_201707 <- table(b201707$flag_pago,ifelse(predic_model_rl_1_201707>optCutOff,"X1_Bueno","X0_Malo"))
tb2_201707
m2_accuracy_201707 <-  ( 121+ 280)/nrow(b201707)
m2_accuracy_201707
#0.7316906

set.seed(1992)
#score test data set
b201707$m2_score <- predict(model_rl_1,type='response',b201707)
m2_pred_201707 <- prediction(ifelse(b201707$m1_score>optCutOff,1,0), b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf_201707 <- performance(m2_pred_201707,"tpr","fpr")

#ROC
plot(m2_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision_201707 <- performance(m2_pred_201707, measure = "prec", x.measure = "rec")
plot(m2_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc_201707 <- performance(m2_pred_201707, measure = "acc")
plot(m2_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS_201707 <- round(max(attr(m2_perf_201707,'y.values')[[1]]-attr(m2_perf_201707,'x.values')[[1]])*100, 2)
m2_AUROC_201707 <- round(performance(m2_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini_201707 <- (2*m2_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m2_AUROC_201707,"\tKS: ", m2_KS_201707, "\tGini:", m2_Gini_201707, "\tAccuracy:", m2_accuracy_201707,"\n")



#'m3:Regresión Logística - MASR - caret'

#score test data set
b201707$m3_score <- predict(model_rl_3_tun,type='prob',newdata=b201707)[2]
m3_pred_201707 <- prediction(b201707$m3_score, b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3_201707 <- table(ifelse(b201707$m3_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb3_201707

m3_accuracy_201707 <- (121+280)/nrow(b201707)
m3_accuracy_201707

m3_perf_201707 <- performance(m3_pred_201707,"tpr","fpr")

#ROC
plot(m3_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic - 201707 Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision_201707 <- performance(m3_pred_201707, measure = "prec", x.measure = "rec")
plot(m3_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc_201707 <- performance(m3_pred_201707, measure = "acc")
plot(m3_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS_201707 <- round(max(attr(m3_perf_201707,'y.values')[[1]]-attr(m3_perf_201707,'x.values')[[1]])*100, 2)
m3_AUROC_201707 <- round(performance(m3_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini_201707 <- (2*m3_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m3_AUROC_201707,"\tKS: ", m3_KS_201707, "\tGini:", m3_Gini_201707,"\tAccuracy:", m3_accuracy_201707, "\n")


#'m4:Regresión Logística - MACR'

predic_model_rl_4_201707 <- predict(model_rl_4, newdata = b201707,type = "response")

summary(predic_model_rl_4_201707)


tb4_201707 <- table(b201707$flag_pago,ifelse(predic_model_rl_4_201707>0.5,"X1_Bueno","X0_Malo"))
tb4_201707

(m4_accuracy_201707 <- (122 +  280)/nrow(b201707))

#0.6711185

#score test data set
b201707$m4_score <- predict(model_rl_4,type='response',b201707)
m4_pred_201707 <- prediction(b201707$m4_score, b201707$flag_pago)
m4_perf_201707 <- performance(m4_pred_201707,"tpr","fpr")

#ROC
plot(m4_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision_201707 <- performance(m4_pred_201707, measure = "prec", x.measure = "rec")
plot(m4_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc_201707 <- performance(m4_pred_201707, measure = "acc")
plot(m4_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS_201707 <- round(max(attr(m4_perf_201707,'y.values')[[1]]-attr(m4_perf_201707,'x.values')[[1]])*100, 2)
m4_AUROC_201707 <- round(performance(m4_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini_201707 <- (2*m4_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m4_AUROC_201707,"\tKS: ", m4_KS_201707, "\tGini:", m4_Gini_201707,"\tAccuracy:", m4_accuracy_201707, "\n")



#'m5:Regresión Logística - MACR - caret'

#score test data set
b201707$m5_score <- predict(model_rl_5_tun,type='prob',newdata=b201707)[2]
m5_pred_201707 <- prediction(b201707$m5_score, b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb201707 <- table(ifelse(b201707$m5_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb201707

m5_accuracy_201707 <- (122+280)/nrow(b201707)
m5_accuracy_201707
#0.6711185

m5_perf_201707 <- performance(m5_pred_201707,"tpr","fpr")

#ROC
plot(m5_perf_201707, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision_201707 <- performance(m5_pred_201707, measure = "prec", x.measure = "rec")
plot(m5_perf_precision_201707, main="m1 Logistic - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc_201707 <- performance(m5_pred_201707, measure = "acc")
plot(m5_perf_acc_201707, main="m1 Logistic - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS_201707 <- round(max(attr(m5_perf_201707,'y.values')[[1]]-attr(m5_perf_201707,'x.values')[[1]])*100, 2)
m5_AUROC_201707 <- round(performance(m5_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini_201707 <- (2*m5_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m5_AUROC_201707,"\tKS: ", m5_KS_201707, "\tGini:", m5_Gini_201707, "\tAccuracy:", m5_accuracy_201707,"\n")


#"m6: AD - cart - MASR"

predic_model_6_ad_1_201707 <- predict(model_6_ad_1, newdata = b201707,type = "prob")[,2]

summary(predic_model_6_ad_1_201707)


tb6_201707 <- table(b201707$flag_pago,ifelse(predic_model_6_ad_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb6_201707

m6_accuracy_201707 <- (128 + 273)/nrow(b201707)
m6_accuracy_201707
#0.6694491

#score test data set
b201707$m6_ad_score <- predict(model_6_ad_1,type='prob',b201707)[,2]
m6_pred_201707 <- prediction(b201707$m6_ad_score, b201707$flag_pago)
m6_perf_201707 <- performance(m6_pred_201707,"tpr","fpr")

#ROC
plot(m6_perf_201707, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision_201707 <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision_201707, main="m6 Árbol de decisión - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc_201707 <- performance(m6_pred_201707, measure = "acc")
plot(m6_perf_acc_201707, main="m6 Árbol de decisión - 201707:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS_201707 <- round(max(attr(m6_perf_201707,'y.values')[[1]]-attr(m6_perf_201707,'x.values')[[1]])*100, 2)
m6_AUROC_201707 <- round(performance(m6_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini_201707 <- (2*m6_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m6_AUROC_201707,"\tKS: ", m6_KS_201707, "\tGini:", m6_Gini_201707, "\tAccuracy:", m6_accuracy_201707,"\n")


#"m7: AD - cart - MASR - caret"

#score test data set
b201707$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=b201707)[2]
m7_pred_201707 <- prediction(b201707$m7_score, b201707$flag_pago)

tb7_201707 <- table(ifelse(b201707$m7_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb7_201707

m7_accuracy_201707 <- (123+ 277)/nrow(b201707)
m7_accuracy_201707
#0.6677796

m7_perf_201707 <- performance(m7_pred_201707,"tpr","fpr")

#ROC
plot(m7_perf_201707, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision_201707 <- performance(m7_pred_201707, measure = "prec", x.measure = "rec")
plot(m7_perf_precision_201707, main="m7 Arbol de decision - 201707:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc_201707 <- performance(m7_pred_201707, measure = "acc")
plot(m7_perf_acc_201707, main="m7 Arbol tuneado:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m7_KS_201707 <- round(max(attr(m7_perf_201707,'y.values')[[1]]-attr(m7_perf_201707,'x.values')[[1]])*100, 2)
m7_AUROC_201707 <- round(performance(m7_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini_201707 <- (2*m7_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")


#"m8: AD - cart - MACR"

predic_model_8_ad_2_201707 <- predict(model_8_ad_2, newdata = b201707,type = "prob")[,2]

summary(predic_model_8_ad_2_201707)


tb8_201707 <- table(b201707$flag_pago,ifelse(predic_model_8_ad_2_201707>0.5,"X1_Bueno","X0_Malo"))
tb8_201707

m8_accuracy_201707 <- (118 +  283)/nrow(b201707)
m8_accuracy_201707
#0.6694491

#score test data set
b201707$m8_score <- predict(model_8_ad_2,type='prob',b201707)[,2]
m8_pred_201707 <- prediction(b201707$m8_score, b201707$flag_pago)
m8_perf_201707 <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf_201707, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision_201707 <- performance(m8_pred_201707, measure = "prec", x.measure = "rec")
plot(m8_perf_precision_201707, main="m8 Arbol de decision - rpart:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m8_perf_acc_201707 <- performance(m8_pred_201707, measure = "acc")
plot(m8_perf_acc_201707, main="m8 Arbol de decision - rpart:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m8_KS_201707 <- round(max(attr(m8_perf_201707,'y.values')[[1]]-attr(m8_perf_201707,'x.values')[[1]])*100, 2)
m8_AUROC_201707 <- round(performance(m8_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini_201707 <- (2*m8_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m8_AUROC_201707,"\tKS: ", m8_KS_201707, "\tGini:", m8_Gini_201707,"\tAccuracy:", m8_accuracy_201707, "\n")


#"m9: AD - cart - MACR - caret"

#score test data set
b201707$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=b201707)[,2]
m9_pred_201707 <- prediction(b201707$m9_score, b201707$flag_pago)


tb9_201707 <- table(ifelse(b201707$m9_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb9_201707

m9_accuracy_201707 <- (117+279)/nrow(b201707)
m9_accuracy_201707
#0.6611018

m9_perf_201707 <- performance(m9_pred_201707,"tpr","fpr")

#ROC
plot(m9_perf_201707, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision_201707 <- performance(m9_pred_201707, measure = "prec", x.measure = "rec")
plot(m9_perf_precision_201707, main="m9 Arbol de decision - caret:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m9_perf_acc_201707 <- performance(m9_pred_201707, measure = "acc")
plot(m9_perf_acc_201707, main="m9 Arbol de decision - caret:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m9_KS_201707 <- round(max(attr(m9_perf_201707,'y.values')[[1]]-attr(m9_perf_201707,'x.values')[[1]])*100, 2)
m9_AUROC_201707 <- round(performance(m9_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini_201707 <- (2*m9_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m9_AUROC_201707,"\tKS: ", m9_KS_201707, "\tGini:", m9_Gini_201707, "\tAccuracy:", m9_accuracy_201707,"\n")


#"m10: AD - ctree - MAsR"


predic_model_10_ad_1_201707 <- sapply(predict(model_10_ad_1, newdata = b201707,type = "prob"),'[[',2)

summary(predic_model_10_ad_1_201707)

tb10_201707 <- table(b201707$flag_pago,ifelse(predic_model_10_ad_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb10_201707

m10_accuracy_201707 <- (122 + 278)/nrow(b201707)
m10_accuracy_201707
#0.6677796

#score test data set
b201707$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = b201707,type = "prob")))))[,2]

m10_pred_201707 <- prediction(b201707$m10_ad_score, b201707$flag_pago)
m10_perf_201707 <- performance(m10_pred_201707,"tpr","fpr")

#ROC
plot(m10_perf_201707, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision_201707 <- performance(m10_pred_201707, measure = "prec", x.measure = "rec")
plot(m10_perf_precision_201707, main="m6 Árbol de decisión - CTREE:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m10_perf_acc_201707 <- performance(m10_pred_201707, measure = "acc")
plot(m10_perf_acc_201707, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m10_KS_201707 <- round(max(attr(m10_perf_201707,'y.values')[[1]]-attr(m10_perf_201707,'x.values')[[1]])*100, 2)
m10_AUROC_201707 <- round(performance(m10_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini_201707 <- (2*m10_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m10_AUROC_201707,"\tKS: ", m10_KS_201707, "\tGini:", m10_Gini_201707, "\tAccuracy:", m10_accuracy_201707,"\n")


#"m11: AD - ctree - MASR - caret"


#score test data set
b201707$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=b201707)[2]
m11_pred_201707 <- prediction(b201707$m11_score, b201707$flag_pago)

tb11_201707 <- table(ifelse(b201707$m11_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
tb11_201707

m11_accuracy_201707 <- (119+281)/nrow(b201707)
m11_accuracy_201707
#0.6677796

m11_perf_201707 <- performance(m11_pred_201707,"tpr","fpr")

#ROC
plot(m11_perf_201707, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision_201707 <- performance(m11_pred_201707, measure = "prec", x.measure = "rec")
plot(m11_perf_precision_201707, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m11_perf_acc_201707 <- performance(m11_pred_201707, measure = "acc")
plot(m11_perf_acc_201707, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m11_KS_201707 <- round(max(attr(m11_perf_201707,'y.values')[[1]]-attr(m11_perf_201707,'x.values')[[1]])*100, 2)
m11_AUROC_201707 <- round(performance(m11_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini_201707 <- (2*m11_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m11_AUROC_201707,"\tKS: ", m11_KS_201707, "\tGini:", m11_Gini_201707, "\tAccuracy:", m11_accuracy_201707,"\n")



#"m12: AD - ctree - MACR"

predic_model_12_ad_2_201707 <- sapply(predict(model_12_ad_2, newdata = b201707,type = "prob"),'[[',2)

summary(predic_model_12_ad_2_201707)


tb12_201707 <- table(b201707$flag_pago,ifelse(predic_model_12_ad_2_201707>0.5,"X1_Bueno","X0_Malo"))
tb12_201707

m12_accuracy_201707 <- (130 + 276)/nrow(b201707)
m12_accuracy_201707
#0.6777963

#score test data set
b201707$m12_score <- sapply(predict(model_12_ad_2, newdata = b201707,type = "prob"),'[[',2)
m12_pred_201707 <- prediction(b201707$m12_score, b201707$flag_pago)
m12_perf_201707 <- performance(m12_pred_201707,"tpr","fpr")

#ROC
plot(m12_perf_201707, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision_201707 <- performance(m12_pred_201707, measure = "prec", x.measure = "rec")
plot(m12_perf_precision_201707, main="m12 Arbol de decision - CTREE :Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m12_perf_acc_201707 <- performance(m12_pred_201707, measure = "acc")
plot(m12_perf_acc_201707, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m12_KS_201707 <- round(max(attr(m12_perf_201707,'y.values')[[1]]-attr(m12_perf_201707,'x.values')[[1]])*100, 2)
m12_AUROC_201707 <- round(performance(m12_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini_201707 <- (2*m12_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m12_AUROC_201707,"\tKS: ", m12_KS_201707, "\tGini:", m12_Gini_201707,"\tAccuracy:", m12_accuracy_201707, "\n")



#"m13: AD - ctree - MACR - caret"

#score test data set
b201707$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=b201707)[,2]
m13_pred_201707 <- prediction(b201707$m13_score, b201707$flag_pago)


table(ifelse(b201707$m13_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m13_accuracy_201707 <- (131+268)/nrow(b201707)
m13_accuracy_201707
#0.6661102

m13_perf_201707 <- performance(m13_pred_201707,"tpr","fpr")

#ROC
plot(m13_perf_201707, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision_201707 <- performance(m13_pred_201707, measure = "prec", x.measure = "rec")
plot(m13_perf_precision_201707, main="m13 Arbol de decision - CTREE:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m13_perf_acc_201707 <- performance(m13_pred_201707, measure = "acc")
plot(m13_perf_acc_201707, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m13_KS_201707 <- round(max(attr(m13_perf_201707,'y.values')[[1]]-attr(m13_perf_201707,'x.values')[[1]])*100, 2)
m13_AUROC_201707 <- round(performance(m13_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini_201707 <- (2*m13_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m13_AUROC_201707,"\tKS: ", m13_KS_201707, "\tGini:", m13_Gini_201707, "\tAccuracy:", m13_accuracy_201707,"\n")


#"m14: RF - cart - MAsR"



predic_model_14_rf_1_201707 <- predict(model_14_rf_1, newdata = b201707,type = "prob")[,2]

summary(predic_model_14_rf_1_201707)

tb14_201707 <- table(b201707$flag_pago,ifelse(predic_model_14_rf_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb14_201707

m14_accuracy_201707 <- (124 + 275)/nrow(b201707)
m14_accuracy_201707
#0.6661102

#score test data set
b201707$m14_rf_score <- predict(model_14_rf_1, newdata = b201707,type = "prob")[,2]

m14_pred_201707 <- prediction(b201707$m14_rf_score, b201707$flag_pago)
m14_perf_201707 <- performance(m14_pred_201707,"tpr","fpr")

#ROC
plot(m14_perf_201707, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision_201707 <- performance(m14_pred_201707, measure = "prec", x.measure = "rec")
plot(m14_perf_precision_201707, main="m14 Random Forest:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m14_perf_acc_201707 <- performance(m14_pred_201707, measure = "acc")
plot(m14_perf_acc_201707, main="m14 Random Forest:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m14_KS_201707 <- round(max(attr(m14_perf_201707,'y.values')[[1]]-attr(m14_perf_201707,'x.values')[[1]])*100, 2)
m14_AUROC_201707 <- round(performance(m14_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini_201707 <- (2*m14_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m14_AUROC_201707,"\tKS: ", m14_KS_201707, "\tGini:", m14_Gini_201707, "\tAccuracy:", m14_accuracy_20176,"\n")


#"m15: RF - cart - MASR - caret"

#score test data set
b201707$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=b201707)[2]
m15_pred_201707 <- prediction(b201707$m15_score, b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201707$m15_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m15_accuracy_201707 <- (641+1501)/nrow(b201707)
m15_accuracy_201707
#0.7330595

m15_perf_201707 <- performance(m15_pred_201707,"tpr","fpr")

#ROC
plot(m15_perf_201707, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision_201707 <- performance(m15_pred_201707, measure = "prec", x.measure = "rec")
plot(m15_perf_precision_201707, main="m15 RandomForest:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m15_perf_acc_201707 <- performance(m15_pred_201707, measure = "acc")
plot(m15_perf_acc_201707, main="m15 RandomForest:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m15_KS_201707 <- round(max(attr(m15_perf_201707,'y.values')[[1]]-attr(m15_perf_201707,'x.values')[[1]])*100, 2)
m15_AUROC_201707 <- round(performance(m15_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini_201707 <- (2*m15_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m15_AUROC_201707,"\tKS: ", m15_KS_201707, "\tGini:", m15_Gini_201707, "\tAccuracy:", m15_accuracy_201707,"\n")


#"m16: RF - cart - MACR"

predic_model_16_rf_2_201707 <- predict(model_16_rf_2, newdata = b201707,type = "prob")[,2]

summary(predic_model_16_rf_2_201707)

tb16_201707 <- table(b201707$flag_pago,ifelse(predic_model_16_rf_2_201707>0.5,"X1_Bueno","X0_Malo"))
tb16_201707


m16_accuracy_201707 <- (640 + 1553)/nrow(b201707)
m16_accuracy_201707
#0.7510274

#score test data set

b201707$m16_rf_2_score <- predict(model_16_rf_2, newdata = b201707,type = "prob")[,2]

m16_pred_201707 <- prediction(b201707$m16_rf_2_score, b201707$flag_pago)
m16_perf_201707 <- performance(m16_pred_201707,"tpr","fpr")

#ROC
plot(m16_perf_201707, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision_201707 <- performance(m16_pred_201707, measure = "prec", x.measure = "rec")
plot(m16_perf_precision_201707, main="m16 RandomForest :Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m16_perf_acc_201707 <- performance(m16_pred_201707, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS_201707 <- round(max(attr(m16_perf_201707,'y.values')[[1]]-attr(m16_perf_201707,'x.values')[[1]])*100, 2)
m16_AUROC_201707 <- round(performance(m16_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini_201707 <- (2*m16_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m16_AUROC_201707,"\tKS: ", m16_KS_201707, "\tGini:", m16_Gini_201707,"\tAccuracy:", m16_accuracy_201707, "\n")



#"m17: RF - cart - MACR - caret"

#score test data set
b201707$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=b201707)[,2]
m17_pred_201707 <- prediction(b201707$m17_score, b201707$flag_pago)

table(ifelse(b201707$m17_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m17_accuracy_201707 <- (639+1515)/nrow(b201707)
m17_accuracy_201707
#0.7376712

m17_perf_201707 <- performance(m17_pred_201707,"tpr","fpr")

#ROC
plot(m17_perf_201707, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision_201707 <- performance(m17_pred_201707, measure = "prec", x.measure = "rec")
plot(m17_perf_precision_201707, main="m17 RandomForest:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m17_perf_acc_201707 <- performance(m17_pred_201707, measure = "acc")
plot(m17_perf_acc_201707, main="m17 RandomForest:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m17_KS_201707 <- round(max(attr(m17_perf_201707,'y.values')[[1]]-attr(m17_perf_201707,'x.values')[[1]])*100, 2)
m17_AUROC_201707 <- round(performance(m17_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini_201707 <- (2*m17_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m17_AUROC_201707,"\tKS: ", m17_KS_201707, "\tGini:", m17_Gini_201707, "\tAccuracy:", m17_accuracy_201707,"\n")


#"m18: GBM - MAsR"

predic_model_18_gbm_1_201707 <- predict(model_18_gbm_1, newdata = b201707,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1_201707, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18_201707 <- table(b201707$flag_pago,ifelse(predic_model_18_gbm_1_201707>0.5,"X1_Bueno","X0_Malo"))
tb18_201707

m18_accuracy_201707 <- (622+1518)/nrow(b201707)
m18_accuracy_201707

#0.7323751

#score test data set
b201707$m18_GBM_score <- predict(model_18_gbm_1, newdata = b201707,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred_201707 <- prediction(b201707$m18_GBM_score, b201707$flag_pago)
m18_perf_201707 <- performance(m18_pred_201707,"tpr","fpr")

#ROC
plot(m18_perf_201707, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision_201707 <- performance(m18_pred_201707, measure = "prec", x.measure = "rec")
plot(m18_perf_precision_201707, main="m18 GBM:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m18_perf_acc_201707 <- performance(m18_pred_201707, measure = "acc")
plot(m18_perf_acc_201707, main="m18 GBM:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m18_KS_201707 <- round(max(attr(m18_perf_201707,'y.values')[[1]]-attr(m18_perf_201707,'x.values')[[1]])*100, 2)
m18_AUROC_201707 <- round(performance(m18_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini_201707 <- (2*m18_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m18_AUROC_201707,"\tKS: ", m18_KS_201707, "\tGini:", m18_Gini_201707, "\tAccuracy:", m18_accuracy_201707,"\n")


#"m19: GBM - MASR - caret"

#score test data set
b201707$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=b201707)[2]
m19_pred_201707 <- prediction(b201707$m19_score, b201707$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201707$m19_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m19_accuracy_201707 <- (630+1508)/nrow(b201707)
m19_accuracy_201707
#0.7316906

m19_perf_201707 <- performance(m19_pred_201707,"tpr","fpr")

#ROC
plot(m19_perf_201707, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision_201707 <- performance(m19_pred_201707, measure = "prec", x.measure = "rec")
plot(m19_perf_precision_201707, main="m19 GBM:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m19_perf_acc_201707 <- performance(m19_pred_201707, measure = "acc")
plot(m19_perf_acc_201707, main="m19 GBM:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m19_KS_201707 <- round(max(attr(m19_perf_201707,'y.values')[[1]]-attr(m19_perf_201707,'x.values')[[1]])*100, 2)
m19_AUROC_201707 <- round(performance(m19_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini_201707 <- (2*m19_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m19_AUROC_201707,"\tKS: ", m19_KS_201707, "\tGini:", m19_Gini_201707, "\tAccuracy:", m19_accuracy_201707,"\n")



#"m20: GBM - MACR"



predic_model_20_gbm_2_201707 <- predict(model_20_gbm_2, newdata = b201707,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2_201707, n=30)


summary(predic_model_20_gbm_2_201707)

tb20_201707 <- table(b201707$flag_pago,ifelse(predic_model_20_gbm_2_201707>0.5,"X1_Bueno","X0_Malo"))
tb20_201707


m20_accuracy_201707 <- (636 + 1514)/nrow(b201707)
m20_accuracy_201707
#0.7363014

#score test data set


b201707$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = b201707,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred_201707 <- prediction(b201707$m20_GBM_2_score, b201707$flag_pago)
m20_perf_201707 <- performance(m20_pred_201707,"tpr","fpr")

#ROC
plot(m20_perf_201707, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision_201707 <- performance(m20_pred_201707, measure = "prec", x.measure = "rec")
plot(m20_perf_precision_201707, main="m20 GBM :Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m20_perf_acc_201707 <- performance(m20_pred_201707, measure = "acc")
plot(m20_perf_acc_201707, main="m20 GBM :Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m20_KS_201707 <- round(max(attr(m20_perf_201707,'y.values')[[1]]-attr(m20_perf_201707,'x.values')[[1]])*100, 2)
m20_AUROC_201707 <- round(performance(m20_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini_201707 <- (2*m20_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m20_AUROC_201707,"\tKS: ", m20_KS_201707, "\tGini:", m20_Gini_201707,"\tAccuracy:", m20_accuracy_201707, "\n")


#"m21: GBM - MACR - caret"


#score test data set
b201707$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=b201707)[,2]
m21_pred_201707 <- prediction(b201707$m21_score, b201707$flag_pago)

table(ifelse(b201707$m21_score>0.5,"X1_Bueno","X0_Malo"), b201707$flag_pago)
m21_accuracy_201707 <- (638+1513)/nrow(b201707)
m21_accuracy_201707
#0.7366438

m21_perf_201707 <- performance(m21_pred_201707,"tpr","fpr")

#ROC
plot(m21_perf_201707, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance - 201707")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision_201707 <- performance(m21_pred_201707, measure = "prec", x.measure = "rec")
plot(m21_perf_precision_201707, main="m21 GBM:Precision/recall curve - 201707")


# Plot accuracy as function of threshold
m21_perf_acc_201707 <- performance(m21_pred_201707, measure = "acc")
plot(m21_perf_acc_201707, main="m21 RandomForest:Accuracy as function of threshold - 201707")

#KS, Gini & AUC m1
m21_KS_201707 <- round(max(attr(m21_perf_201707,'y.values')[[1]]-attr(m21_perf_201707,'x.values')[[1]])*100, 2)
m21_AUROC_201707 <- round(performance(m21_pred_201707, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini_201707 <- (2*m21_AUROC_201707 - 100)

###########################################################################
cat("AUROC: ",m21_AUROC_201707,"\tKS: ", m21_KS_201707, "\tGini:", m21_Gini_201707, "\tAccuracy:", m21_accuracy_201707,"\n")



# tABLA DE PERFORMANCE - OOS
models_201707 <- c('m1:Regresión Logística - MASR - 201707', 'm2:Regresión Logística corte optimo - MASR - 201707',
                   'm3:Regresión Logística - MASR - caret - 201707','m4:Regresión Logística - MACR - 201707', 
                   'm5:Regresión Logística - MACR - caret - 201707', "m6: AD - cart - MASR - 201707",
                   "m7: AD - cart - MASR - caret - 201707", "m8: AD - cart - MACR - 201707",
                   "m9: AD - cart - MACR - caret - 201707", "m10: AD - ctree - MAsR - 201707",
                   "m11: AD - ctree - MASR - caret - 201707", "m12: AD - ctree - MACR - 201707",
                   "m13: AD - ctree - MACR - caret - 201707", "m14: RF - cart - MAsR - 201707",
                   "m15: RF - cart - MASR - caret - 201707", "m16: RF - cart - MACR - 201707",
                   "m17: RF - cart - MACR - caret - 201707")

# AUCs
models_AUC_201707 <- c(m1_AUROC_201707, m2_AUROC_201707, m3_AUROC_201707, m4_AUROC_201707, m5_AUROC_201707, m6_AUROC_201707, m7_AUROC_201707, m8_AUROC_201707, m9_AUROC_201707, m10_AUROC_201707, 
                       m11_AUROC_201707, m12_AUROC_201707, m13_AUROC_201707, m14_AUROC_201707, m15_AUROC_201707, m16_AUROC_201707, m17_AUROC_201707)
# KS
models_KS_201707 <- c(m1_KS_201707, m2_KS_201707, m3_KS_201707, m4_KS_201707, m5_KS_201707, m6_KS_201707, m7_KS_201707, m8_KS_201707, m9_KS_201707, m10_KS_201707, m11_KS_201707, m12_KS_201707, m13_KS_201707, m14_KS_201707, m15_KS_201707, 
                      m16_KS_201707, m17_KS_201707)

# Gini
models_Gini_201707 <- c(m1_Gini_201707, m2_Gini_201707, m3_Gini_201707, m4_Gini_201707, m5_Gini_201707, m6_Gini_201707, m7_Gini_201707, m8_Gini_201707, m9_Gini_201707, m10_Gini_201707, 
                        m11_Gini_201707, m12_Gini_201707, m13_Gini_201707, m14_Gini_201707, m15_Gini_201707, m16_Gini_201707, m17_Gini_201707)


# Accuraccy

#models_accuracy_201707 <- c(m1_accuracy_201707, m2_accuracy_201707, m3_accuracy_201707, m4_accuracy_201707, m5_accuracy_201707, m6_accuracy_201707, m7_accuracy_201707, m8_accuracy_201707, m9_accuracy_201707, 
#                            m10_accuracy_201707, m11_accuracy_201707, m12_accuracy_201707, m13_accuracy_201707, m14_accuracy_201707, m15_accuracy_201707, m16_accuracy_201707, m17_accuracy_201707, 
#                            m18_accuracy_201707, m19_accuracy_201707, m20_accuracy_201707, m21_accuracy_201707)


# Juntando todo
model_performance_metric_201707 <- as.data.frame(cbind(models_201707, models_AUC_201707, models_KS_201707, models_Gini_201707))

model_performance_metric_201707 <- model_performance_metric_201707[order(models_Gini_201707, decreasing = T),] 


# Colnames 
colnames(model_performance_metric_201707) <- c("Modelos_201707", "AUC_201707", "KS_201707", "Gini_201707")

# Display Performance Reports
kable(model_performance_metric_201707, caption ="Comparación de modelos - OOS - 201707")


#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VI          MODELOS EN OOT
#---------------------------------------------------------------------------------------
########################################################################################


#############################################################
# PERIODO - 201708
#############################################################


#'m1:Regresión Logística - MASR'

set.seed(1992)
predic_model_rl_1_201708 <- predict(model_rl_1,type='response',b201708)

tb1_201708 <- table(b201708$flag_pago,ifelse(predic_model_rl_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb1_201708

m1_accuracy_201708 <- (121 + 280)/nrow(b201708)
m1_accuracy_201708

set.seed(1992)
#score test data set
b201708$m1_score_201708 <- predict(model_rl_1,type='response',b201708)
m1_pred_201708 <- prediction(b201708$m1_score_201708, b201708$flag_pago)
m1_perf_201708 <- performance(m1_pred_201708,"tpr","fpr")

#ROC
plot(m1_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision_201708 <- performance(m1_pred_201708, measure = "prec", x.measure = "rec")
plot(m1_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc_201708 <- performance(m1_pred_201708, measure = "acc")
plot(m1_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS_201708 <- round(max(attr(m1_perf_201708,'y.values')[[1]]-attr(m1_perf_201708,'x.values')[[1]])*100, 2)
m1_AUROC_201708 <- round(performance(m1_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini_201708 <- (2*m1_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m1_AUROC_201708,"\tKS: ", m1_KS_201708, "\tGini:", m1_Gini_201708, "\tAccuracy:", m1_accuracy_201708,"\n")



#'m2:Regresión Logística corte optimo - MASR'



tb2_201708 <- table(b201708$flag_pago,ifelse(predic_model_rl_1_201708>optCutOff,"X1_Bueno","X0_Malo"))
tb2_201708
m2_accuracy_201708 <-  ( 121+ 280)/nrow(b201708)
m2_accuracy_201708
#0.7316906

set.seed(1992)
#score test data set
b201708$m2_score <- predict(model_rl_1,type='response',b201708)
m2_pred_201708 <- prediction(ifelse(b201708$m1_score>optCutOff,1,0), b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf_201708 <- performance(m2_pred_201708,"tpr","fpr")

#ROC
plot(m2_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision_201708 <- performance(m2_pred_201708, measure = "prec", x.measure = "rec")
plot(m2_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc_201708 <- performance(m2_pred_201708, measure = "acc")
plot(m2_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS_201708 <- round(max(attr(m2_perf_201708,'y.values')[[1]]-attr(m2_perf_201708,'x.values')[[1]])*100, 2)
m2_AUROC_201708 <- round(performance(m2_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini_201708 <- (2*m2_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m2_AUROC_201708,"\tKS: ", m2_KS_201708, "\tGini:", m2_Gini_201708, "\tAccuracy:", m2_accuracy_201708,"\n")



#'m3:Regresión Logística - MASR - caret'

#score test data set
b201708$m3_score <- predict(model_rl_3_tun,type='prob',newdata=b201708)[2]
m3_pred_201708 <- prediction(b201708$m3_score, b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3_201708 <- table(ifelse(b201708$m3_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb3_201708

m3_accuracy_201708 <- (121+280)/nrow(b201708)
m3_accuracy_201708

m3_perf_201708 <- performance(m3_pred_201708,"tpr","fpr")

#ROC
plot(m3_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic - 201708 Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision_201708 <- performance(m3_pred_201708, measure = "prec", x.measure = "rec")
plot(m3_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc_201708 <- performance(m3_pred_201708, measure = "acc")
plot(m3_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS_201708 <- round(max(attr(m3_perf_201708,'y.values')[[1]]-attr(m3_perf_201708,'x.values')[[1]])*100, 2)
m3_AUROC_201708 <- round(performance(m3_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini_201708 <- (2*m3_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m3_AUROC_201708,"\tKS: ", m3_KS_201708, "\tGini:", m3_Gini_201708,"\tAccuracy:", m3_accuracy_201708, "\n")


#'m4:Regresión Logística - MACR'

predic_model_rl_4_201708 <- predict(model_rl_4, newdata = b201708,type = "response")

summary(predic_model_rl_4_201708)


tb4_201708 <- table(b201708$flag_pago,ifelse(predic_model_rl_4_201708>0.5,"X1_Bueno","X0_Malo"))
tb4_201708

(m4_accuracy_201708 <- (122 +  280)/nrow(b201708))

#0.6711185

#score test data set
b201708$m4_score <- predict(model_rl_4,type='response',b201708)
m4_pred_201708 <- prediction(b201708$m4_score, b201708$flag_pago)
m4_perf_201708 <- performance(m4_pred_201708,"tpr","fpr")

#ROC
plot(m4_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision_201708 <- performance(m4_pred_201708, measure = "prec", x.measure = "rec")
plot(m4_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc_201708 <- performance(m4_pred_201708, measure = "acc")
plot(m4_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS_201708 <- round(max(attr(m4_perf_201708,'y.values')[[1]]-attr(m4_perf_201708,'x.values')[[1]])*100, 2)
m4_AUROC_201708 <- round(performance(m4_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini_201708 <- (2*m4_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m4_AUROC_201708,"\tKS: ", m4_KS_201708, "\tGini:", m4_Gini_201708,"\tAccuracy:", m4_accuracy_201708, "\n")



#'m5:Regresión Logística - MACR - caret'

#score test data set
b201708$m5_score <- predict(model_rl_5_tun,type='prob',newdata=b201708)[2]
m5_pred_201708 <- prediction(b201708$m5_score, b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb201708 <- table(ifelse(b201708$m5_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb201708

m5_accuracy_201708 <- (122+280)/nrow(b201708)
m5_accuracy_201708
#0.6711185

m5_perf_201708 <- performance(m5_pred_201708,"tpr","fpr")

#ROC
plot(m5_perf_201708, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision_201708 <- performance(m5_pred_201708, measure = "prec", x.measure = "rec")
plot(m5_perf_precision_201708, main="m1 Logistic - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc_201708 <- performance(m5_pred_201708, measure = "acc")
plot(m5_perf_acc_201708, main="m1 Logistic - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS_201708 <- round(max(attr(m5_perf_201708,'y.values')[[1]]-attr(m5_perf_201708,'x.values')[[1]])*100, 2)
m5_AUROC_201708 <- round(performance(m5_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini_201708 <- (2*m5_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m5_AUROC_201708,"\tKS: ", m5_KS_201708, "\tGini:", m5_Gini_201708, "\tAccuracy:", m5_accuracy_201708,"\n")


#"m6: AD - cart - MASR"

predic_model_6_ad_1_201708 <- predict(model_6_ad_1, newdata = b201708,type = "prob")[,2]

summary(predic_model_6_ad_1_201708)


tb6_201708 <- table(b201708$flag_pago,ifelse(predic_model_6_ad_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb6_201708

m6_accuracy_201708 <- (128 + 273)/nrow(b201708)
m6_accuracy_201708
#0.6694491

#score test data set
b201708$m6_ad_score <- predict(model_6_ad_1,type='prob',b201708)[,2]
m6_pred_201708 <- prediction(b201708$m6_ad_score, b201708$flag_pago)
m6_perf_201708 <- performance(m6_pred_201708,"tpr","fpr")

#ROC
plot(m6_perf_201708, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision_201708 <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision_201708, main="m6 Árbol de decisión - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc_201708 <- performance(m6_pred_201708, measure = "acc")
plot(m6_perf_acc_201708, main="m6 Árbol de decisión - 201708:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS_201708 <- round(max(attr(m6_perf_201708,'y.values')[[1]]-attr(m6_perf_201708,'x.values')[[1]])*100, 2)
m6_AUROC_201708 <- round(performance(m6_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini_201708 <- (2*m6_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m6_AUROC_201708,"\tKS: ", m6_KS_201708, "\tGini:", m6_Gini_201708, "\tAccuracy:", m6_accuracy_201708,"\n")


#"m7: AD - cart - MASR - caret"

#score test data set
b201708$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=b201708)[2]
m7_pred_201708 <- prediction(b201708$m7_score, b201708$flag_pago)

tb7_201708 <- table(ifelse(b201708$m7_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb7_201708

m7_accuracy_201708 <- (123+ 277)/nrow(b201708)
m7_accuracy_201708
#0.6677796

m7_perf_201708 <- performance(m7_pred_201708,"tpr","fpr")

#ROC
plot(m7_perf_201708, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision_201708 <- performance(m7_pred_201708, measure = "prec", x.measure = "rec")
plot(m7_perf_precision_201708, main="m7 Arbol de decision - 201708:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc_201708 <- performance(m7_pred_201708, measure = "acc")
plot(m7_perf_acc_201708, main="m7 Arbol tuneado:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m7_KS_201708 <- round(max(attr(m7_perf_201708,'y.values')[[1]]-attr(m7_perf_201708,'x.values')[[1]])*100, 2)
m7_AUROC_201708 <- round(performance(m7_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini_201708 <- (2*m7_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")


#"m8: AD - cart - MACR"

predic_model_8_ad_2_201708 <- predict(model_8_ad_2, newdata = b201708,type = "prob")[,2]

summary(predic_model_8_ad_2_201708)


tb8_201708 <- table(b201708$flag_pago,ifelse(predic_model_8_ad_2_201708>0.5,"X1_Bueno","X0_Malo"))
tb8_201708

m8_accuracy_201708 <- (118 +  283)/nrow(b201708)
m8_accuracy_201708
#0.6694491

#score test data set
b201708$m8_score <- predict(model_8_ad_2,type='prob',b201708)[,2]
m8_pred_201708 <- prediction(b201708$m8_score, b201708$flag_pago)
m8_perf_201708 <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf_201708, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision_201708 <- performance(m8_pred_201708, measure = "prec", x.measure = "rec")
plot(m8_perf_precision_201708, main="m8 Arbol de decision - rpart:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m8_perf_acc_201708 <- performance(m8_pred_201708, measure = "acc")
plot(m8_perf_acc_201708, main="m8 Arbol de decision - rpart:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m8_KS_201708 <- round(max(attr(m8_perf_201708,'y.values')[[1]]-attr(m8_perf_201708,'x.values')[[1]])*100, 2)
m8_AUROC_201708 <- round(performance(m8_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini_201708 <- (2*m8_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m8_AUROC_201708,"\tKS: ", m8_KS_201708, "\tGini:", m8_Gini_201708,"\tAccuracy:", m8_accuracy_201708, "\n")


#"m9: AD - cart - MACR - caret"

#score test data set
b201708$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=b201708)[,2]
m9_pred_201708 <- prediction(b201708$m9_score, b201708$flag_pago)


tb9_201708 <- table(ifelse(b201708$m9_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb9_201708

m9_accuracy_201708 <- (117+279)/nrow(b201708)
m9_accuracy_201708
#0.6611018

m9_perf_201708 <- performance(m9_pred_201708,"tpr","fpr")

#ROC
plot(m9_perf_201708, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision_201708 <- performance(m9_pred_201708, measure = "prec", x.measure = "rec")
plot(m9_perf_precision_201708, main="m9 Arbol de decision - caret:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m9_perf_acc_201708 <- performance(m9_pred_201708, measure = "acc")
plot(m9_perf_acc_201708, main="m9 Arbol de decision - caret:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m9_KS_201708 <- round(max(attr(m9_perf_201708,'y.values')[[1]]-attr(m9_perf_201708,'x.values')[[1]])*100, 2)
m9_AUROC_201708 <- round(performance(m9_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini_201708 <- (2*m9_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m9_AUROC_201708,"\tKS: ", m9_KS_201708, "\tGini:", m9_Gini_201708, "\tAccuracy:", m9_accuracy_201708,"\n")


#"m10: AD - ctree - MAsR"


predic_model_10_ad_1_201708 <- sapply(predict(model_10_ad_1, newdata = b201708,type = "prob"),'[[',2)

summary(predic_model_10_ad_1_201708)

tb10_201708 <- table(b201708$flag_pago,ifelse(predic_model_10_ad_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb10_201708

m10_accuracy_201708 <- (122 + 278)/nrow(b201708)
m10_accuracy_201708
#0.6677796

#score test data set
b201708$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = b201708,type = "prob")))))[,2]

m10_pred_201708 <- prediction(b201708$m10_ad_score, b201708$flag_pago)
m10_perf_201708 <- performance(m10_pred_201708,"tpr","fpr")

#ROC
plot(m10_perf_201708, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision_201708 <- performance(m10_pred_201708, measure = "prec", x.measure = "rec")
plot(m10_perf_precision_201708, main="m6 Árbol de decisión - CTREE:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m10_perf_acc_201708 <- performance(m10_pred_201708, measure = "acc")
plot(m10_perf_acc_201708, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m10_KS_201708 <- round(max(attr(m10_perf_201708,'y.values')[[1]]-attr(m10_perf_201708,'x.values')[[1]])*100, 2)
m10_AUROC_201708 <- round(performance(m10_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini_201708 <- (2*m10_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m10_AUROC_201708,"\tKS: ", m10_KS_201708, "\tGini:", m10_Gini_201708, "\tAccuracy:", m10_accuracy_201708,"\n")


#"m11: AD - ctree - MASR - caret"


#score test data set
b201708$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=b201708)[2]
m11_pred_201708 <- prediction(b201708$m11_score, b201708$flag_pago)

tb11_201708 <- table(ifelse(b201708$m11_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
tb11_201708

m11_accuracy_201708 <- (119+281)/nrow(b201708)
m11_accuracy_201708
#0.6677796

m11_perf_201708 <- performance(m11_pred_201708,"tpr","fpr")

#ROC
plot(m11_perf_201708, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision_201708 <- performance(m11_pred_201708, measure = "prec", x.measure = "rec")
plot(m11_perf_precision_201708, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m11_perf_acc_201708 <- performance(m11_pred_201708, measure = "acc")
plot(m11_perf_acc_201708, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m11_KS_201708 <- round(max(attr(m11_perf_201708,'y.values')[[1]]-attr(m11_perf_201708,'x.values')[[1]])*100, 2)
m11_AUROC_201708 <- round(performance(m11_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini_201708 <- (2*m11_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m11_AUROC_201708,"\tKS: ", m11_KS_201708, "\tGini:", m11_Gini_201708, "\tAccuracy:", m11_accuracy_201708,"\n")



#"m12: AD - ctree - MACR"

predic_model_12_ad_2_201708 <- sapply(predict(model_12_ad_2, newdata = b201708,type = "prob"),'[[',2)

summary(predic_model_12_ad_2_201708)


tb12_201708 <- table(b201708$flag_pago,ifelse(predic_model_12_ad_2_201708>0.5,"X1_Bueno","X0_Malo"))
tb12_201708

m12_accuracy_201708 <- (130 + 276)/nrow(b201708)
m12_accuracy_201708
#0.6777963

#score test data set
b201708$m12_score <- sapply(predict(model_12_ad_2, newdata = b201708,type = "prob"),'[[',2)
m12_pred_201708 <- prediction(b201708$m12_score, b201708$flag_pago)
m12_perf_201708 <- performance(m12_pred_201708,"tpr","fpr")

#ROC
plot(m12_perf_201708, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision_201708 <- performance(m12_pred_201708, measure = "prec", x.measure = "rec")
plot(m12_perf_precision_201708, main="m12 Arbol de decision - CTREE :Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m12_perf_acc_201708 <- performance(m12_pred_201708, measure = "acc")
plot(m12_perf_acc_201708, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m12_KS_201708 <- round(max(attr(m12_perf_201708,'y.values')[[1]]-attr(m12_perf_201708,'x.values')[[1]])*100, 2)
m12_AUROC_201708 <- round(performance(m12_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini_201708 <- (2*m12_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m12_AUROC_201708,"\tKS: ", m12_KS_201708, "\tGini:", m12_Gini_201708,"\tAccuracy:", m12_accuracy_201708, "\n")



#"m13: AD - ctree - MACR - caret"

#score test data set
b201708$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=b201708)[,2]
m13_pred_201708 <- prediction(b201708$m13_score, b201708$flag_pago)


table(ifelse(b201708$m13_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m13_accuracy_201708 <- (131+268)/nrow(b201708)
m13_accuracy_201708
#0.6661102

m13_perf_201708 <- performance(m13_pred_201708,"tpr","fpr")

#ROC
plot(m13_perf_201708, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision_201708 <- performance(m13_pred_201708, measure = "prec", x.measure = "rec")
plot(m13_perf_precision_201708, main="m13 Arbol de decision - CTREE:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m13_perf_acc_201708 <- performance(m13_pred_201708, measure = "acc")
plot(m13_perf_acc_201708, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m13_KS_201708 <- round(max(attr(m13_perf_201708,'y.values')[[1]]-attr(m13_perf_201708,'x.values')[[1]])*100, 2)
m13_AUROC_201708 <- round(performance(m13_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini_201708 <- (2*m13_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m13_AUROC_201708,"\tKS: ", m13_KS_201708, "\tGini:", m13_Gini_201708, "\tAccuracy:", m13_accuracy_201708,"\n")


#"m14: RF - cart - MAsR"



predic_model_14_rf_1_201708 <- predict(model_14_rf_1, newdata = b201708,type = "prob")[,2]

summary(predic_model_14_rf_1_201708)

tb14_201708 <- table(b201708$flag_pago,ifelse(predic_model_14_rf_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb14_201708

m14_accuracy_20176 <- (124 + 275)/nrow(b201708)
m14_accuracy_20176
#0.6661102

#score test data set
b201708$m14_rf_score <- predict(model_14_rf_1, newdata = b201708,type = "prob")[,2]

m14_pred_201708 <- prediction(b201708$m14_rf_score, b201708$flag_pago)
m14_perf_201708 <- performance(m14_pred_201708,"tpr","fpr")

#ROC
plot(m14_perf_201708, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision_201708 <- performance(m14_pred_201708, measure = "prec", x.measure = "rec")
plot(m14_perf_precision_201708, main="m14 Random Forest:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m14_perf_acc_201708 <- performance(m14_pred_201708, measure = "acc")
plot(m14_perf_acc_201708, main="m14 Random Forest:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m14_KS_201708 <- round(max(attr(m14_perf_201708,'y.values')[[1]]-attr(m14_perf_201708,'x.values')[[1]])*100, 2)
m14_AUROC_201708 <- round(performance(m14_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini_201708 <- (2*m14_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m14_AUROC_201708,"\tKS: ", m14_KS_201708, "\tGini:", m14_Gini_201708, "\tAccuracy:", m14_accuracy_20176,"\n")


#"m15: RF - cart - MASR - caret"

#score test data set
b201708$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=b201708)[2]
m15_pred_201708 <- prediction(b201708$m15_score, b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201708$m15_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m15_accuracy_201708 <- (641+1501)/nrow(b201708)
m15_accuracy_201708
#0.7330595

m15_perf_201708 <- performance(m15_pred_201708,"tpr","fpr")

#ROC
plot(m15_perf_201708, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision_201708 <- performance(m15_pred_201708, measure = "prec", x.measure = "rec")
plot(m15_perf_precision_201708, main="m15 RandomForest:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m15_perf_acc_201708 <- performance(m15_pred_201708, measure = "acc")
plot(m15_perf_acc_201708, main="m15 RandomForest:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m15_KS_201708 <- round(max(attr(m15_perf_201708,'y.values')[[1]]-attr(m15_perf_201708,'x.values')[[1]])*100, 2)
m15_AUROC_201708 <- round(performance(m15_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini_201708 <- (2*m15_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m15_AUROC_201708,"\tKS: ", m15_KS_201708, "\tGini:", m15_Gini_201708, "\tAccuracy:", m15_accuracy_201708,"\n")


#"m16: RF - cart - MACR"

predic_model_16_rf_2_201708 <- predict(model_16_rf_2, newdata = b201708,type = "prob")[,2]

summary(predic_model_16_rf_2_201708)

tb16_201708 <- table(b201708$flag_pago,ifelse(predic_model_16_rf_2_201708>0.5,"X1_Bueno","X0_Malo"))
tb16_201708


m16_accuracy_201708 <- (640 + 1553)/nrow(b201708)
m16_accuracy_201708
#0.7510274

#score test data set

b201708$m16_rf_2_score <- predict(model_16_rf_2, newdata = b201708,type = "prob")[,2]

m16_pred_201708 <- prediction(b201708$m16_rf_2_score, b201708$flag_pago)
m16_perf_201708 <- performance(m16_pred_201708,"tpr","fpr")

#ROC
plot(m16_perf_201708, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision_201708 <- performance(m16_pred_201708, measure = "prec", x.measure = "rec")
plot(m16_perf_precision_201708, main="m16 RandomForest :Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m16_perf_acc_201708 <- performance(m16_pred_201708, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS_201708 <- round(max(attr(m16_perf_201708,'y.values')[[1]]-attr(m16_perf_201708,'x.values')[[1]])*100, 2)
m16_AUROC_201708 <- round(performance(m16_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini_201708 <- (2*m16_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m16_AUROC_201708,"\tKS: ", m16_KS_201708, "\tGini:", m16_Gini_201708,"\tAccuracy:", m16_accuracy_201708, "\n")



#"m17: RF - cart - MACR - caret"

#score test data set
b201708$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=b201708)[,2]
m17_pred_201708 <- prediction(b201708$m17_score, b201708$flag_pago)

table(ifelse(b201708$m17_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m17_accuracy_201708 <- (639+1515)/nrow(b201708)
m17_accuracy_201708
#0.7376712

m17_perf_201708 <- performance(m17_pred_201708,"tpr","fpr")

#ROC
plot(m17_perf_201708, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision_201708 <- performance(m17_pred_201708, measure = "prec", x.measure = "rec")
plot(m17_perf_precision_201708, main="m17 RandomForest:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m17_perf_acc_201708 <- performance(m17_pred_201708, measure = "acc")
plot(m17_perf_acc_201708, main="m17 RandomForest:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m17_KS_201708 <- round(max(attr(m17_perf_201708,'y.values')[[1]]-attr(m17_perf_201708,'x.values')[[1]])*100, 2)
m17_AUROC_201708 <- round(performance(m17_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini_201708 <- (2*m17_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m17_AUROC_201708,"\tKS: ", m17_KS_201708, "\tGini:", m17_Gini_201708, "\tAccuracy:", m17_accuracy_201708,"\n")


#"m18: GBM - MAsR"

predic_model_18_gbm_1_201708 <- predict(model_18_gbm_1, newdata = b201708,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1_201708, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18_201708 <- table(b201708$flag_pago,ifelse(predic_model_18_gbm_1_201708>0.5,"X1_Bueno","X0_Malo"))
tb18_201708

m18_accuracy_201708 <- (622+1518)/nrow(b201708)
m18_accuracy_201708

#0.7323751

#score test data set
b201708$m18_GBM_score <- predict(model_18_gbm_1, newdata = b201708,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred_201708 <- prediction(b201708$m18_GBM_score, b201708$flag_pago)
m18_perf_201708 <- performance(m18_pred_201708,"tpr","fpr")

#ROC
plot(m18_perf_201708, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision_201708 <- performance(m18_pred_201708, measure = "prec", x.measure = "rec")
plot(m18_perf_precision_201708, main="m18 GBM:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m18_perf_acc_201708 <- performance(m18_pred_201708, measure = "acc")
plot(m18_perf_acc_201708, main="m18 GBM:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m18_KS_201708 <- round(max(attr(m18_perf_201708,'y.values')[[1]]-attr(m18_perf_201708,'x.values')[[1]])*100, 2)
m18_AUROC_201708 <- round(performance(m18_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini_201708 <- (2*m18_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m18_AUROC_201708,"\tKS: ", m18_KS_201708, "\tGini:", m18_Gini_201708, "\tAccuracy:", m18_accuracy_201708,"\n")


#"m19: GBM - MASR - caret"

#score test data set
b201708$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=b201708)[2]
m19_pred_201708 <- prediction(b201708$m19_score, b201708$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201708$m19_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m19_accuracy_201708 <- (630+1508)/nrow(b201708)
m19_accuracy_201708
#0.7316906

m19_perf_201708 <- performance(m19_pred_201708,"tpr","fpr")

#ROC
plot(m19_perf_201708, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision_201708 <- performance(m19_pred_201708, measure = "prec", x.measure = "rec")
plot(m19_perf_precision_201708, main="m19 GBM:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m19_perf_acc_201708 <- performance(m19_pred_201708, measure = "acc")
plot(m19_perf_acc_201708, main="m19 GBM:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m19_KS_201708 <- round(max(attr(m19_perf_201708,'y.values')[[1]]-attr(m19_perf_201708,'x.values')[[1]])*100, 2)
m19_AUROC_201708 <- round(performance(m19_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini_201708 <- (2*m19_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m19_AUROC_201708,"\tKS: ", m19_KS_201708, "\tGini:", m19_Gini_201708, "\tAccuracy:", m19_accuracy_201708,"\n")



#"m20: GBM - MACR"



predic_model_20_gbm_2_201708 <- predict(model_20_gbm_2, newdata = b201708,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2_201708, n=30)


summary(predic_model_20_gbm_2_201708)

tb20_201708 <- table(b201708$flag_pago,ifelse(predic_model_20_gbm_2_201708>0.5,"X1_Bueno","X0_Malo"))
tb20_201708


m20_accuracy_201708 <- (636 + 1514)/nrow(b201708)
m20_accuracy_201708
#0.7363014

#score test data set


b201708$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = b201708,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred_201708 <- prediction(b201708$m20_GBM_2_score, b201708$flag_pago)
m20_perf_201708 <- performance(m20_pred_201708,"tpr","fpr")

#ROC
plot(m20_perf_201708, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision_201708 <- performance(m20_pred_201708, measure = "prec", x.measure = "rec")
plot(m20_perf_precision_201708, main="m20 GBM :Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m20_perf_acc_201708 <- performance(m20_pred_201708, measure = "acc")
plot(m20_perf_acc_201708, main="m20 GBM :Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m20_KS_201708 <- round(max(attr(m20_perf_201708,'y.values')[[1]]-attr(m20_perf_201708,'x.values')[[1]])*100, 2)
m20_AUROC_201708 <- round(performance(m20_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini_201708 <- (2*m20_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m20_AUROC_201708,"\tKS: ", m20_KS_201708, "\tGini:", m20_Gini_201708,"\tAccuracy:", m20_accuracy_201708, "\n")


#"m21: GBM - MACR - caret"


#score test data set
b201708$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=b201708)[,2]
m21_pred_201708 <- prediction(b201708$m21_score, b201708$flag_pago)

table(ifelse(b201708$m21_score>0.5,"X1_Bueno","X0_Malo"), b201708$flag_pago)
m21_accuracy_201708 <- (638+1513)/nrow(b201708)
m21_accuracy_201708
#0.7366438

m21_perf_201708 <- performance(m21_pred_201708,"tpr","fpr")

#ROC
plot(m21_perf_201708, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance - 201708")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision_201708 <- performance(m21_pred_201708, measure = "prec", x.measure = "rec")
plot(m21_perf_precision_201708, main="m21 GBM:Precision/recall curve - 201708")


# Plot accuracy as function of threshold
m21_perf_acc_201708 <- performance(m21_pred_201708, measure = "acc")
plot(m21_perf_acc_201708, main="m21 RandomForest:Accuracy as function of threshold - 201708")

#KS, Gini & AUC m1
m21_KS_201708 <- round(max(attr(m21_perf_201708,'y.values')[[1]]-attr(m21_perf_201708,'x.values')[[1]])*100, 2)
m21_AUROC_201708 <- round(performance(m21_pred_201708, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini_201708 <- (2*m21_AUROC_201708 - 100)

###########################################################################
cat("AUROC: ",m21_AUROC_201708,"\tKS: ", m21_KS_201708, "\tGini:", m21_Gini_201708, "\tAccuracy:", m21_accuracy_201708,"\n")



# tABLA DE PERFORMANCE - OOS
models_201708 <- c('m1:Regresión Logística - MASR - 201708', 'm2:Regresión Logística corte optimo - MASR - 201708',
                   'm3:Regresión Logística - MASR - caret - 201708','m4:Regresión Logística - MACR - 201708', 
                   'm5:Regresión Logística - MACR - caret - 201708', "m6: AD - cart - MASR - 201708",
                   "m7: AD - cart - MASR - caret - 201708", "m8: AD - cart - MACR - 201708",
                   "m9: AD - cart - MACR - caret - 201708", "m10: AD - ctree - MAsR - 201708",
                   "m11: AD - ctree - MASR - caret - 201708", "m12: AD - ctree - MACR - 201708",
                   "m13: AD - ctree - MACR - caret - 201708", "m14: RF - cart - MAsR - 201708",
                   "m15: RF - cart - MASR - caret - 201708", "m16: RF - cart - MACR - 201708",
                   "m17: RF - cart - MACR - caret - 201708")

# AUCs
models_AUC_201708 <- c(m1_AUROC_201708, m2_AUROC_201708, m3_AUROC_201708, m4_AUROC_201708, m5_AUROC_201708, m6_AUROC_201708, m7_AUROC_201708, m8_AUROC_201708, m9_AUROC_201708, m10_AUROC_201708, 
                       m11_AUROC_201708, m12_AUROC_201708, m13_AUROC_201708, m14_AUROC_201708, m15_AUROC_201708, m16_AUROC_201708, m17_AUROC_201708)
# KS
models_KS_201708 <- c(m1_KS_201708, m2_KS_201708, m3_KS_201708, m4_KS_201708, m5_KS_201708, m6_KS_201708, m7_KS_201708, m8_KS_201708, m9_KS_201708, m10_KS_201708, m11_KS_201708, m12_KS_201708, m13_KS_201708, m14_KS_201708, m15_KS_201708, 
                      m16_KS_201708, m17_KS_201708)

# Gini
models_Gini_201708 <- c(m1_Gini_201708, m2_Gini_201708, m3_Gini_201708, m4_Gini_201708, m5_Gini_201708, m6_Gini_201708, m7_Gini_201708, m8_Gini_201708, m9_Gini_201708, m10_Gini_201708, 
                        m11_Gini_201708, m12_Gini_201708, m13_Gini_201708, m14_Gini_201708, m15_Gini_201708, m16_Gini_201708, m17_Gini_201708)


# Accuraccy

#models_accuracy_201708 <- c(m1_accuracy_201708, m2_accuracy_201708, m3_accuracy_201708, m4_accuracy_201708, m5_accuracy_201708, m6_accuracy_201708, m7_accuracy_201708, m8_accuracy_201708, m9_accuracy_201708, 
#                            m10_accuracy_201708, m11_accuracy_201708, m12_accuracy_201708, m13_accuracy_201708, m14_accuracy_201708, m15_accuracy_201708, m16_accuracy_201708, m17_accuracy_201708, 
#                            m18_accuracy_201708, m19_accuracy_201708, m20_accuracy_201708, m21_accuracy_201708)


# Juntando todo
model_performance_metric_201708 <- as.data.frame(cbind(models_201708, models_AUC_201708, models_KS_201708, models_Gini_201708))

model_performance_metric_201708 <- model_performance_metric_201708[order(models_Gini_201708, decreasing = T),] 

# Colnames 
colnames(model_performance_metric_201708) <- c("Modelos_201708", "AUC_201708", "KS_201708", "Gini_201708")

# Display Performance Reports
kable(model_performance_metric_201708, caption ="Comparación de modelos - OOS - 201708")


#########################################################################################
#---------------------------------------------------------------------------------------
#             PARTE VI          MODELOS EN OOT
#---------------------------------------------------------------------------------------
########################################################################################


#############################################################
# PERIODO - 201709
#############################################################


#'m1:Regresión Logística - MASR'

set.seed(1992)
predic_model_rl_1_201709 <- predict(model_rl_1,type='response',b201709)

tb1_201709 <- table(b201709$flag_pago,ifelse(predic_model_rl_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb1_201709

m1_accuracy_201709 <- (121 + 280)/nrow(b201709)
m1_accuracy_201709

set.seed(1992)
#score test data set
b201709$m1_score_201709 <- predict(model_rl_1,type='response',b201709)
m1_pred_201709 <- prediction(b201709$m1_score_201709, b201709$flag_pago)
m1_perf_201709 <- performance(m1_pred_201709,"tpr","fpr")

#ROC
plot(m1_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m1_perf_precision_201709 <- performance(m1_pred_201709, measure = "prec", x.measure = "rec")
plot(m1_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m1_perf_acc_201709 <- performance(m1_pred_201709, measure = "acc")
plot(m1_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m1_KS_201709 <- round(max(attr(m1_perf_201709,'y.values')[[1]]-attr(m1_perf_201709,'x.values')[[1]])*100, 2)
m1_AUROC_201709 <- round(performance(m1_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m1_Gini_201709 <- (2*m1_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m1_AUROC_201709,"\tKS: ", m1_KS_201709, "\tGini:", m1_Gini_201709, "\tAccuracy:", m1_accuracy_201709,"\n")



#'m2:Regresión Logística corte optimo - MASR'



tb2_201709 <- table(b201709$flag_pago,ifelse(predic_model_rl_1_201709>optCutOff,"X1_Bueno","X0_Malo"))
tb2_201709
m2_accuracy_201709 <-  ( 121+ 280)/nrow(b201709)
m2_accuracy_201709
#0.7316906

set.seed(1992)
#score test data set
b201709$m2_score <- predict(model_rl_1,type='response',b201709)
m2_pred_201709 <- prediction(ifelse(b201709$m1_score>optCutOff,1,0), b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

m2_perf_201709 <- performance(m2_pred_201709,"tpr","fpr")

#ROC
plot(m2_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m2_perf_precision_201709 <- performance(m2_pred_201709, measure = "prec", x.measure = "rec")
plot(m2_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m2_perf_acc_201709 <- performance(m2_pred_201709, measure = "acc")
plot(m2_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m2_KS_201709 <- round(max(attr(m2_perf_201709,'y.values')[[1]]-attr(m2_perf_201709,'x.values')[[1]])*100, 2)
m2_AUROC_201709 <- round(performance(m2_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m2_Gini_201709 <- (2*m2_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m2_AUROC_201709,"\tKS: ", m2_KS_201709, "\tGini:", m2_Gini_201709, "\tAccuracy:", m2_accuracy_201709,"\n")



#'m3:Regresión Logística - MASR - caret'

#score test data set
b201709$m3_score <- predict(model_rl_3_tun,type='prob',newdata=b201709)[2]
m3_pred_201709 <- prediction(b201709$m3_score, b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)
tb3_201709 <- table(ifelse(b201709$m3_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb3_201709

m3_accuracy_201709 <- (121+280)/nrow(b201709)
m3_accuracy_201709

m3_perf_201709 <- performance(m3_pred_201709,"tpr","fpr")

#ROC
plot(m3_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic - 201709 Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m3_perf_precision_201709 <- performance(m3_pred_201709, measure = "prec", x.measure = "rec")
plot(m3_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m3_perf_acc_201709 <- performance(m3_pred_201709, measure = "acc")
plot(m3_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m3_KS_201709 <- round(max(attr(m3_perf_201709,'y.values')[[1]]-attr(m3_perf_201709,'x.values')[[1]])*100, 2)
m3_AUROC_201709 <- round(performance(m3_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m3_Gini_201709 <- (2*m3_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m3_AUROC_201709,"\tKS: ", m3_KS_201709, "\tGini:", m3_Gini_201709,"\tAccuracy:", m3_accuracy_201709, "\n")


#'m4:Regresión Logística - MACR'

predic_model_rl_4_201709 <- predict(model_rl_4, newdata = b201709,type = "response")

summary(predic_model_rl_4_201709)


tb4_201709 <- table(b201709$flag_pago,ifelse(predic_model_rl_4_201709>0.5,"X1_Bueno","X0_Malo"))
tb4_201709

(m4_accuracy_201709 <- (122 +  280)/nrow(b201709))

#0.6711185

#score test data set
b201709$m4_score <- predict(model_rl_4,type='response',b201709)
m4_pred_201709 <- prediction(b201709$m4_score, b201709$flag_pago)
m4_perf_201709 <- performance(m4_pred_201709,"tpr","fpr")

#ROC
plot(m4_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m4_perf_precision_201709 <- performance(m4_pred_201709, measure = "prec", x.measure = "rec")
plot(m4_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m4_perf_acc_201709 <- performance(m4_pred_201709, measure = "acc")
plot(m4_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m4_KS_201709 <- round(max(attr(m4_perf_201709,'y.values')[[1]]-attr(m4_perf_201709,'x.values')[[1]])*100, 2)
m4_AUROC_201709 <- round(performance(m4_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m4_Gini_201709 <- (2*m4_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m4_AUROC_201709,"\tKS: ", m4_KS_201709, "\tGini:", m4_Gini_201709,"\tAccuracy:", m4_accuracy_201709, "\n")



#'m5:Regresión Logística - MACR - caret'

#score test data set
b201709$m5_score <- predict(model_rl_5_tun,type='prob',newdata=b201709)[2]
m5_pred_201709 <- prediction(b201709$m5_score, b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

tb201709 <- table(ifelse(b201709$m5_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb201709

m5_accuracy_201709 <- (122+280)/nrow(b201709)
m5_accuracy_201709
#0.6711185

m5_perf_201709 <- performance(m5_pred_201709,"tpr","fpr")

#ROC
plot(m5_perf_201709, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m5_perf_precision_201709 <- performance(m5_pred_201709, measure = "prec", x.measure = "rec")
plot(m5_perf_precision_201709, main="m1 Logistic - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m5_perf_acc_201709 <- performance(m5_pred_201709, measure = "acc")
plot(m5_perf_acc_201709, main="m1 Logistic - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m5_KS_201709 <- round(max(attr(m5_perf_201709,'y.values')[[1]]-attr(m5_perf_201709,'x.values')[[1]])*100, 2)
m5_AUROC_201709 <- round(performance(m5_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m5_Gini_201709 <- (2*m5_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m5_AUROC_201709,"\tKS: ", m5_KS_201709, "\tGini:", m5_Gini_201709, "\tAccuracy:", m5_accuracy_201709,"\n")


#"m6: AD - cart - MASR"

predic_model_6_ad_1_201709 <- predict(model_6_ad_1, newdata = b201709,type = "prob")[,2]

summary(predic_model_6_ad_1_201709)


tb6_201709 <- table(b201709$flag_pago,ifelse(predic_model_6_ad_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb6_201709

m6_accuracy_201709 <- (128 + 273)/nrow(b201709)
m6_accuracy_201709
#0.6694491

#score test data set
b201709$m6_ad_score <- predict(model_6_ad_1,type='prob',b201709)[,2]
m6_pred_201709 <- prediction(b201709$m6_ad_score, b201709$flag_pago)
m6_perf_201709 <- performance(m6_pred_201709,"tpr","fpr")

#ROC
plot(m6_perf_201709, lwd=2, colorize=TRUE, main="ROC m6: arbol de decision Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m6_perf_precision_201709 <- performance(m6_pred, measure = "prec", x.measure = "rec")
plot(m6_perf_precision_201709, main="m6 Árbol de decisión - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m6_perf_acc_201709 <- performance(m6_pred_201709, measure = "acc")
plot(m6_perf_acc_201709, main="m6 Árbol de decisión - 201709:Accuracy as function of threshold")

#KS, Gini & AUC m1
m6_KS_201709 <- round(max(attr(m6_perf_201709,'y.values')[[1]]-attr(m6_perf_201709,'x.values')[[1]])*100, 2)
m6_AUROC_201709 <- round(performance(m6_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m6_Gini_201709 <- (2*m6_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m6_AUROC_201709,"\tKS: ", m6_KS_201709, "\tGini:", m6_Gini_201709, "\tAccuracy:", m6_accuracy_201709,"\n")


#"m7: AD - cart - MASR - caret"

#score test data set
b201709$m7_score <- predict(model_7_ad_tun_1,type='prob',newdata=b201709)[2]
m7_pred_201709 <- prediction(b201709$m7_score, b201709$flag_pago)

tb7_201709 <- table(ifelse(b201709$m7_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb7_201709

m7_accuracy_201709 <- (123+ 277)/nrow(b201709)
m7_accuracy_201709
#0.6677796

m7_perf_201709 <- performance(m7_pred_201709,"tpr","fpr")

#ROC
plot(m7_perf_201709, lwd=2, colorize=TRUE, main="ROC m7: Arbol de decision Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m7_perf_precision_201709 <- performance(m7_pred_201709, measure = "prec", x.measure = "rec")
plot(m7_perf_precision_201709, main="m7 Arbol de decision - 201709:Precision/recall curve")


# Plot accuracy as function of threshold
m7_perf_acc_201709 <- performance(m7_pred_201709, measure = "acc")
plot(m7_perf_acc_201709, main="m7 Arbol tuneado:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m7_KS_201709 <- round(max(attr(m7_perf_201709,'y.values')[[1]]-attr(m7_perf_201709,'x.values')[[1]])*100, 2)
m7_AUROC_201709 <- round(performance(m7_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m7_Gini_201709 <- (2*m7_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m7_AUROC,"\tKS: ", m7_KS, "\tGini:", m7_Gini, "\tAccuracy:", m7_accuracy,"\n")


#"m8: AD - cart - MACR"

predic_model_8_ad_2_201709 <- predict(model_8_ad_2, newdata = b201709,type = "prob")[,2]

summary(predic_model_8_ad_2_201709)


tb8_201709 <- table(b201709$flag_pago,ifelse(predic_model_8_ad_2_201709>0.5,"X1_Bueno","X0_Malo"))
tb8_201709

m8_accuracy_201709 <- (118 +  283)/nrow(b201709)
m8_accuracy_201709
#0.6694491

#score test data set
b201709$m8_score <- predict(model_8_ad_2,type='prob',b201709)[,2]
m8_pred_201709 <- prediction(b201709$m8_score, b201709$flag_pago)
m8_perf_201709 <- performance(m8_pred,"tpr","fpr")

#ROC
plot(m8_perf_201709, lwd=2, colorize=TRUE, main="ROC m8: Arbol de decision rpart- data testing - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m8_perf_precision_201709 <- performance(m8_pred_201709, measure = "prec", x.measure = "rec")
plot(m8_perf_precision_201709, main="m8 Arbol de decision - rpart:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m8_perf_acc_201709 <- performance(m8_pred_201709, measure = "acc")
plot(m8_perf_acc_201709, main="m8 Arbol de decision - rpart:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m8_KS_201709 <- round(max(attr(m8_perf_201709,'y.values')[[1]]-attr(m8_perf_201709,'x.values')[[1]])*100, 2)
m8_AUROC_201709 <- round(performance(m8_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m8_Gini_201709 <- (2*m8_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m8_AUROC_201709,"\tKS: ", m8_KS_201709, "\tGini:", m8_Gini_201709,"\tAccuracy:", m8_accuracy_201709, "\n")


#"m9: AD - cart - MACR - caret"

#score test data set
b201709$m9_score <- predict(model_9_ad_tun_2,type='prob',newdata=b201709)[,2]
m9_pred_201709 <- prediction(b201709$m9_score, b201709$flag_pago)


tb9_201709 <- table(ifelse(b201709$m9_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb9_201709

m9_accuracy_201709 <- (117+279)/nrow(b201709)
m9_accuracy_201709
#0.6611018

m9_perf_201709 <- performance(m9_pred_201709,"tpr","fpr")

#ROC
plot(m9_perf_201709, lwd=2, colorize=TRUE, main="ROC m9: Arbol de decision - caret Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m9_perf_precision_201709 <- performance(m9_pred_201709, measure = "prec", x.measure = "rec")
plot(m9_perf_precision_201709, main="m9 Arbol de decision - caret:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m9_perf_acc_201709 <- performance(m9_pred_201709, measure = "acc")
plot(m9_perf_acc_201709, main="m9 Arbol de decision - caret:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m9_KS_201709 <- round(max(attr(m9_perf_201709,'y.values')[[1]]-attr(m9_perf_201709,'x.values')[[1]])*100, 2)
m9_AUROC_201709 <- round(performance(m9_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m9_Gini_201709 <- (2*m9_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m9_AUROC_201709,"\tKS: ", m9_KS_201709, "\tGini:", m9_Gini_201709, "\tAccuracy:", m9_accuracy_201709,"\n")


#"m10: AD - ctree - MAsR"


predic_model_10_ad_1_201709 <- sapply(predict(model_10_ad_1, newdata = b201709,type = "prob"),'[[',2)

summary(predic_model_10_ad_1_201709)

tb10_201709 <- table(b201709$flag_pago,ifelse(predic_model_10_ad_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb10_201709

m10_accuracy_201709 <- (122 + 278)/nrow(b201709)
m10_accuracy_201709
#0.6677796

#score test data set
b201709$m10_ad_score <- as.matrix(t(as.data.frame((predict(model_10_ad_1, newdata = b201709,type = "prob")))))[,2]

m10_pred_201709 <- prediction(b201709$m10_ad_score, b201709$flag_pago)
m10_perf_201709 <- performance(m10_pred_201709,"tpr","fpr")

#ROC
plot(m10_perf_201709, lwd=2, colorize=TRUE, main="ROC m10: Arbol de decision - CTREE Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m10_perf_precision_201709 <- performance(m10_pred_201709, measure = "prec", x.measure = "rec")
plot(m10_perf_precision_201709, main="m6 Árbol de decisión - CTREE:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m10_perf_acc_201709 <- performance(m10_pred_201709, measure = "acc")
plot(m10_perf_acc_201709, main="m6 Árbol de decisión - CTREE:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m10_KS_201709 <- round(max(attr(m10_perf_201709,'y.values')[[1]]-attr(m10_perf_201709,'x.values')[[1]])*100, 2)
m10_AUROC_201709 <- round(performance(m10_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m10_Gini_201709 <- (2*m10_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m10_AUROC_201709,"\tKS: ", m10_KS_201709, "\tGini:", m10_Gini_201709, "\tAccuracy:", m10_accuracy_201709,"\n")


#"m11: AD - ctree - MASR - caret"


#score test data set
b201709$m11_score <- predict(model_11_ad_tun_1,type='prob',newdata=b201709)[2]
m11_pred_201709 <- prediction(b201709$m11_score, b201709$flag_pago)

tb11_201709 <- table(ifelse(b201709$m11_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
tb11_201709

m11_accuracy_201709 <- (119+281)/nrow(b201709)
m11_accuracy_201709
#0.6677796

m11_perf_201709 <- performance(m11_pred_201709,"tpr","fpr")

#ROC
plot(m11_perf_201709, lwd=2, colorize=TRUE, main="ROC m11: Arbol de decision - CTREE - Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m11_perf_precision_201709 <- performance(m11_pred_201709, measure = "prec", x.measure = "rec")
plot(m11_perf_precision_201709, main="m11 Arbol de decision tuneado - CTREE :Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m11_perf_acc_201709 <- performance(m11_pred_201709, measure = "acc")
plot(m11_perf_acc_201709, main="m11 Arbol de decision tuneado - CTREE:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m11_KS_201709 <- round(max(attr(m11_perf_201709,'y.values')[[1]]-attr(m11_perf_201709,'x.values')[[1]])*100, 2)
m11_AUROC_201709 <- round(performance(m11_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m11_Gini_201709 <- (2*m11_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m11_AUROC_201709,"\tKS: ", m11_KS_201709, "\tGini:", m11_Gini_201709, "\tAccuracy:", m11_accuracy_201709,"\n")



#"m12: AD - ctree - MACR"

predic_model_12_ad_2_201709 <- sapply(predict(model_12_ad_2, newdata = b201709,type = "prob"),'[[',2)

summary(predic_model_12_ad_2_201709)


tb12_201709 <- table(b201709$flag_pago,ifelse(predic_model_12_ad_2_201709>0.5,"X1_Bueno","X0_Malo"))
tb12_201709

m12_accuracy_201709 <- (130 + 276)/nrow(b201709)
m12_accuracy_201709
#0.6777963

#score test data set
b201709$m12_score <- sapply(predict(model_12_ad_2, newdata = b201709,type = "prob"),'[[',2)
m12_pred_201709 <- prediction(b201709$m12_score, b201709$flag_pago)
m12_perf_201709 <- performance(m12_pred_201709,"tpr","fpr")

#ROC
plot(m12_perf_201709, lwd=2, colorize=TRUE, main="ROC m12: Arbol de decision CTREE- data testing - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m12_perf_precision_201709 <- performance(m12_pred_201709, measure = "prec", x.measure = "rec")
plot(m12_perf_precision_201709, main="m12 Arbol de decision - CTREE :Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m12_perf_acc_201709 <- performance(m12_pred_201709, measure = "acc")
plot(m12_perf_acc_201709, main="m12 Arbol de decision - CTREE :Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m12_KS_201709 <- round(max(attr(m12_perf_201709,'y.values')[[1]]-attr(m12_perf_201709,'x.values')[[1]])*100, 2)
m12_AUROC_201709 <- round(performance(m12_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m12_Gini_201709 <- (2*m12_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m12_AUROC_201709,"\tKS: ", m12_KS_201709, "\tGini:", m12_Gini_201709,"\tAccuracy:", m12_accuracy_201709, "\n")



#"m13: AD - ctree - MACR - caret"

#score test data set
b201709$m13_score <- predict(model_13_ad_tun_1,type='prob',newdata=b201709)[,2]
m13_pred_201709 <- prediction(b201709$m13_score, b201709$flag_pago)


table(ifelse(b201709$m13_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m13_accuracy_201709 <- (131+268)/nrow(b201709)
m13_accuracy_201709
#0.6661102

m13_perf_201709 <- performance(m13_pred_201709,"tpr","fpr")

#ROC
plot(m13_perf_201709, lwd=2, colorize=TRUE, main="ROC m13: Arbol de decision - CTREE Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m13_perf_precision_201709 <- performance(m13_pred_201709, measure = "prec", x.measure = "rec")
plot(m13_perf_precision_201709, main="m13 Arbol de decision - CTREE:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m13_perf_acc_201709 <- performance(m13_pred_201709, measure = "acc")
plot(m13_perf_acc_201709, main="m13 Arbol de decision - CTREE:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m13_KS_201709 <- round(max(attr(m13_perf_201709,'y.values')[[1]]-attr(m13_perf_201709,'x.values')[[1]])*100, 2)
m13_AUROC_201709 <- round(performance(m13_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m13_Gini_201709 <- (2*m13_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m13_AUROC_201709,"\tKS: ", m13_KS_201709, "\tGini:", m13_Gini_201709, "\tAccuracy:", m13_accuracy_201709,"\n")


#"m14: RF - cart - MAsR"



predic_model_14_rf_1_201709 <- predict(model_14_rf_1, newdata = b201709,type = "prob")[,2]

summary(predic_model_14_rf_1_201709)

tb14_201709 <- table(b201709$flag_pago,ifelse(predic_model_14_rf_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb14_201709

m14_accuracy_20176 <- (124 + 275)/nrow(b201709)
m14_accuracy_20176
#0.6661102

#score test data set
b201709$m14_rf_score <- predict(model_14_rf_1, newdata = b201709,type = "prob")[,2]

m14_pred_201709 <- prediction(b201709$m14_rf_score, b201709$flag_pago)
m14_perf_201709 <- performance(m14_pred_201709,"tpr","fpr")

#ROC
plot(m14_perf_201709, lwd=2, colorize=TRUE, main="ROC m14: Random Forest Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m14_perf_precision_201709 <- performance(m14_pred_201709, measure = "prec", x.measure = "rec")
plot(m14_perf_precision_201709, main="m14 Random Forest:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m14_perf_acc_201709 <- performance(m14_pred_201709, measure = "acc")
plot(m14_perf_acc_201709, main="m14 Random Forest:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m14_KS_201709 <- round(max(attr(m14_perf_201709,'y.values')[[1]]-attr(m14_perf_201709,'x.values')[[1]])*100, 2)
m14_AUROC_201709 <- round(performance(m14_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m14_Gini_201709 <- (2*m14_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m14_AUROC_201709,"\tKS: ", m14_KS_201709, "\tGini:", m14_Gini_201709, "\tAccuracy:", m14_accuracy_20176,"\n")


#"m15: RF - cart - MASR - caret"

#score test data set
b201709$m15_score <- predict(model_15_RF_tun_1,type='prob',newdata=b201709)[2]
m15_pred_201709 <- prediction(b201709$m15_score, b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201709$m15_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m15_accuracy_201709 <- (641+1501)/nrow(b201709)
m15_accuracy_201709
#0.7330595

m15_perf_201709 <- performance(m15_pred_201709,"tpr","fpr")

#ROC
plot(m15_perf_201709, lwd=2, colorize=TRUE, main="ROC m15: RandomForest - Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m15_perf_precision_201709 <- performance(m15_pred_201709, measure = "prec", x.measure = "rec")
plot(m15_perf_precision_201709, main="m15 RandomForest:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m15_perf_acc_201709 <- performance(m15_pred_201709, measure = "acc")
plot(m15_perf_acc_201709, main="m15 RandomForest:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m15_KS_201709 <- round(max(attr(m15_perf_201709,'y.values')[[1]]-attr(m15_perf_201709,'x.values')[[1]])*100, 2)
m15_AUROC_201709 <- round(performance(m15_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m15_Gini_201709 <- (2*m15_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m15_AUROC_201709,"\tKS: ", m15_KS_201709, "\tGini:", m15_Gini_201709, "\tAccuracy:", m15_accuracy_201709,"\n")


#"m16: RF - cart - MACR"

predic_model_16_rf_2_201709 <- predict(model_16_rf_2, newdata = b201709,type = "prob")[,2]

summary(predic_model_16_rf_2_201709)

tb16_201709 <- table(b201709$flag_pago,ifelse(predic_model_16_rf_2_201709>0.5,"X1_Bueno","X0_Malo"))
tb16_201709


m16_accuracy_201709 <- (640 + 1553)/nrow(b201709)
m16_accuracy_201709
#0.7510274

#score test data set

b201709$m16_rf_2_score <- predict(model_16_rf_2, newdata = b201709,type = "prob")[,2]

m16_pred_201709 <- prediction(b201709$m16_rf_2_score, b201709$flag_pago)
m16_perf_201709 <- performance(m16_pred_201709,"tpr","fpr")

#ROC
plot(m16_perf_201709, lwd=2, colorize=TRUE, main="ROC m16: RandomForest - data testing - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m16_perf_precision_201709 <- performance(m16_pred_201709, measure = "prec", x.measure = "rec")
plot(m16_perf_precision_201709, main="m16 RandomForest :Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m16_perf_acc_201709 <- performance(m16_pred_201709, measure = "acc")
plot(m16_perf_acc, main="m16 RandomForest :Accuracy as function of threshold")

#KS, Gini & AUC m1
m16_KS_201709 <- round(max(attr(m16_perf_201709,'y.values')[[1]]-attr(m16_perf_201709,'x.values')[[1]])*100, 2)
m16_AUROC_201709 <- round(performance(m16_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m16_Gini_201709 <- (2*m16_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m16_AUROC_201709,"\tKS: ", m16_KS_201709, "\tGini:", m16_Gini_201709,"\tAccuracy:", m16_accuracy_201709, "\n")



#"m17: RF - cart - MACR - caret"

#score test data set
b201709$m17_score <- predict(model_17_rf_tun_2,type='prob',newdata=b201709)[,2]
m17_pred_201709 <- prediction(b201709$m17_score, b201709$flag_pago)

table(ifelse(b201709$m17_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m17_accuracy_201709 <- (639+1515)/nrow(b201709)
m17_accuracy_201709
#0.7376712

m17_perf_201709 <- performance(m17_pred_201709,"tpr","fpr")

#ROC
plot(m17_perf_201709, lwd=2, colorize=TRUE, main="ROC m17: RandomForest Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m17_perf_precision_201709 <- performance(m17_pred_201709, measure = "prec", x.measure = "rec")
plot(m17_perf_precision_201709, main="m17 RandomForest:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m17_perf_acc_201709 <- performance(m17_pred_201709, measure = "acc")
plot(m17_perf_acc_201709, main="m17 RandomForest:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m17_KS_201709 <- round(max(attr(m17_perf_201709,'y.values')[[1]]-attr(m17_perf_201709,'x.values')[[1]])*100, 2)
m17_AUROC_201709 <- round(performance(m17_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m17_Gini_201709 <- (2*m17_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m17_AUROC_201709,"\tKS: ", m17_KS_201709, "\tGini:", m17_Gini_201709, "\tAccuracy:", m17_accuracy_201709,"\n")


#"m18: GBM - MAsR"

predic_model_18_gbm_1_201709 <- predict(model_18_gbm_1, newdata = b201709,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

head(predic_model_18_gbm_1_201709, n=30)

#--------------------------------------------------------------
#UNA MANERA DE VER VARIOS VALORES CON DIFERENTES ARBOLES
#---------------------------------------------------------------
#INICIO
#n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree

#predic_model_18_gbm_1 <- predict(model_18_gbm_1, newdata = test,n.trees = n.trees, type = "response")

#summary(predic_model_18_gbm_1)
#FIN


tb18_201709 <- table(b201709$flag_pago,ifelse(predic_model_18_gbm_1_201709>0.5,"X1_Bueno","X0_Malo"))
tb18_201709

m18_accuracy_201709 <- (622+1518)/nrow(b201709)
m18_accuracy_201709

#0.7323751

#score test data set
b201709$m18_GBM_score <- predict(model_18_gbm_1, newdata = b201709,n.trees = gbm.perf(model_18_gbm_1, plot.it = FALSE), type = "response")

m18_pred_201709 <- prediction(b201709$m18_GBM_score, b201709$flag_pago)
m18_perf_201709 <- performance(m18_pred_201709,"tpr","fpr")

#ROC
plot(m18_perf_201709, lwd=2, colorize=TRUE, main="ROC m18: GBM Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m18_perf_precision_201709 <- performance(m18_pred_201709, measure = "prec", x.measure = "rec")
plot(m18_perf_precision_201709, main="m18 GBM:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m18_perf_acc_201709 <- performance(m18_pred_201709, measure = "acc")
plot(m18_perf_acc_201709, main="m18 GBM:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m18_KS_201709 <- round(max(attr(m18_perf_201709,'y.values')[[1]]-attr(m18_perf_201709,'x.values')[[1]])*100, 2)
m18_AUROC_201709 <- round(performance(m18_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m18_Gini_201709 <- (2*m18_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m18_AUROC_201709,"\tKS: ", m18_KS_201709, "\tGini:", m18_Gini_201709, "\tAccuracy:", m18_accuracy_201709,"\n")


#"m19: GBM - MASR - caret"

#score test data set
b201709$m19_score <- predict(model_19_GBM_tun_1,type='prob',newdata=b201709)[2]
m19_pred_201709 <- prediction(b201709$m19_score, b201709$flag_pago)
#m2_pred <- prediction(test$m1_score, test$flag_pago)

table(ifelse(b201709$m19_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m19_accuracy_201709 <- (630+1508)/nrow(b201709)
m19_accuracy_201709
#0.7316906

m19_perf_201709 <- performance(m19_pred_201709,"tpr","fpr")

#ROC
plot(m19_perf_201709, lwd=2, colorize=TRUE, main="ROC m19: GBM - Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m19_perf_precision_201709 <- performance(m19_pred_201709, measure = "prec", x.measure = "rec")
plot(m19_perf_precision_201709, main="m19 GBM:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m19_perf_acc_201709 <- performance(m19_pred_201709, measure = "acc")
plot(m19_perf_acc_201709, main="m19 GBM:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m19_KS_201709 <- round(max(attr(m19_perf_201709,'y.values')[[1]]-attr(m19_perf_201709,'x.values')[[1]])*100, 2)
m19_AUROC_201709 <- round(performance(m19_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m19_Gini_201709 <- (2*m19_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m19_AUROC_201709,"\tKS: ", m19_KS_201709, "\tGini:", m19_Gini_201709, "\tAccuracy:", m19_accuracy_201709,"\n")



#"m20: GBM - MACR"



predic_model_20_gbm_2_201709 <- predict(model_20_gbm_2, newdata = b201709,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")

head(predic_model_20_gbm_2_201709, n=30)


summary(predic_model_20_gbm_2_201709)

tb20_201709 <- table(b201709$flag_pago,ifelse(predic_model_20_gbm_2_201709>0.5,"X1_Bueno","X0_Malo"))
tb20_201709


m20_accuracy_201709 <- (636 + 1514)/nrow(b201709)
m20_accuracy_201709
#0.7363014

#score test data set


b201709$m20_GBM_2_score <- predict(model_20_gbm_2, newdata = b201709,n.trees = gbm.perf(model_20_gbm_2, plot.it = FALSE), type = "response")



m20_pred_201709 <- prediction(b201709$m20_GBM_2_score, b201709$flag_pago)
m20_perf_201709 <- performance(m20_pred_201709,"tpr","fpr")

#ROC
plot(m20_perf_201709, lwd=2, colorize=TRUE, main="ROC m20: GBM - data testing - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m20_perf_precision_201709 <- performance(m20_pred_201709, measure = "prec", x.measure = "rec")
plot(m20_perf_precision_201709, main="m20 GBM :Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m20_perf_acc_201709 <- performance(m20_pred_201709, measure = "acc")
plot(m20_perf_acc_201709, main="m20 GBM :Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m20_KS_201709 <- round(max(attr(m20_perf_201709,'y.values')[[1]]-attr(m20_perf_201709,'x.values')[[1]])*100, 2)
m20_AUROC_201709 <- round(performance(m20_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m20_Gini_201709 <- (2*m20_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m20_AUROC_201709,"\tKS: ", m20_KS_201709, "\tGini:", m20_Gini_201709,"\tAccuracy:", m20_accuracy_201709, "\n")


#"m21: GBM - MACR - caret"


#score test data set
b201709$m21_score <- predict(model_21_rf_tun_2,type='prob',newdata=b201709)[,2]
m21_pred_201709 <- prediction(b201709$m21_score, b201709$flag_pago)

table(ifelse(b201709$m21_score>0.5,"X1_Bueno","X0_Malo"), b201709$flag_pago)
m21_accuracy_201709 <- (638+1513)/nrow(b201709)
m21_accuracy_201709
#0.7366438

m21_perf_201709 <- performance(m21_pred_201709,"tpr","fpr")

#ROC
plot(m21_perf_201709, lwd=2, colorize=TRUE, main="ROC m21: GBM Performance - 201709")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)


# Plot precision/recall curve
m21_perf_precision_201709 <- performance(m21_pred_201709, measure = "prec", x.measure = "rec")
plot(m21_perf_precision_201709, main="m21 GBM:Precision/recall curve - 201709")


# Plot accuracy as function of threshold
m21_perf_acc_201709 <- performance(m21_pred_201709, measure = "acc")
plot(m21_perf_acc_201709, main="m21 RandomForest:Accuracy as function of threshold - 201709")

#KS, Gini & AUC m1
m21_KS_201709 <- round(max(attr(m21_perf_201709,'y.values')[[1]]-attr(m21_perf_201709,'x.values')[[1]])*100, 2)
m21_AUROC_201709 <- round(performance(m21_pred_201709, measure = "auc")@y.values[[1]]*100, 2)
m21_Gini_201709 <- (2*m21_AUROC_201709 - 100)

###########################################################################
cat("AUROC: ",m21_AUROC_201709,"\tKS: ", m21_KS_201709, "\tGini:", m21_Gini_201709, "\tAccuracy:", m21_accuracy_201709,"\n")



# tABLA DE PERFORMANCE - OOS
models_201709 <- c('m1:Regresión Logística - MASR - 201709', 'm2:Regresión Logística corte optimo - MASR - 201709',
                   'm3:Regresión Logística - MASR - caret - 201709','m4:Regresión Logística - MACR - 201709', 
                   'm5:Regresión Logística - MACR - caret - 201709', "m6: AD - cart - MASR - 201709",
                   "m7: AD - cart - MASR - caret - 201709", "m8: AD - cart - MACR - 201709",
                   "m9: AD - cart - MACR - caret - 201709", "m10: AD - ctree - MAsR - 201709",
                   "m11: AD - ctree - MASR - caret - 201709", "m12: AD - ctree - MACR - 201709",
                   "m13: AD - ctree - MACR - caret - 201709", "m14: RF - cart - MAsR - 201709",
                   "m15: RF - cart - MASR - caret - 201709", "m16: RF - cart - MACR - 201709",
                   "m17: RF - cart - MACR - caret - 201709")

# AUCs
models_AUC_201709 <- c(m1_AUROC_201709, m2_AUROC_201709, m3_AUROC_201709, m4_AUROC_201709, m5_AUROC_201709, m6_AUROC_201709, m7_AUROC_201709, m8_AUROC_201709, m9_AUROC_201709, m10_AUROC_201709, 
                       m11_AUROC_201709, m12_AUROC_201709, m13_AUROC_201709, m14_AUROC_201709, m15_AUROC_201709, m16_AUROC_201709, m17_AUROC_201709)
# KS
models_KS_201709 <- c(m1_KS_201709, m2_KS_201709, m3_KS_201709, m4_KS_201709, m5_KS_201709, m6_KS_201709, m7_KS_201709, m8_KS_201709, m9_KS_201709, m10_KS_201709, m11_KS_201709, m12_KS_201709, m13_KS_201709, m14_KS_201709, m15_KS_201709, 
                      m16_KS_201709, m17_KS_201709)

# Gini
models_Gini_201709 <- c(m1_Gini_201709, m2_Gini_201709, m3_Gini_201709, m4_Gini_201709, m5_Gini_201709, m6_Gini_201709, m7_Gini_201709, m8_Gini_201709, m9_Gini_201709, m10_Gini_201709, 
                        m11_Gini_201709, m12_Gini_201709, m13_Gini_201709, m14_Gini_201709, m15_Gini_201709, m16_Gini_201709, m17_Gini_201709)


# Accuraccy

#models_accuracy_201709 <- c(m1_accuracy_201709, m2_accuracy_201709, m3_accuracy_201709, m4_accuracy_201709, m5_accuracy_201709, m6_accuracy_201709, m7_accuracy_201709, m8_accuracy_201709, m9_accuracy_201709, 
#                            m10_accuracy_201709, m11_accuracy_201709, m12_accuracy_201709, m13_accuracy_201709, m14_accuracy_201709, m15_accuracy_201709, m16_accuracy_201709, m17_accuracy_201709, 
#                            m18_accuracy_201709, m19_accuracy_201709, m20_accuracy_201709, m21_accuracy_201709)


# Juntando todo
model_performance_metric_201709 <- as.data.frame(cbind(models_201709, models_AUC_201709, models_KS_201709, models_Gini_201709))

model_performance_metric_201709 <- model_performance_metric_201709[order(models_Gini_201709, decreasing = T),] 


# Colnames 
colnames(model_performance_metric_201709) <- c("Modelos_201709", "AUC_201709", "KS_201709", "Gini_201709")

# Display Performance Reports
kable(model_performance_metric_201709, caption ="Comparación de modelos - OOS - 201709")


ruta_modelos <- "D:/edwin/Modelo scoring cobranza"

modelos_cob <- ls()



save(model_rl_1,model_rl_3_tun,model_rl_4,model_rl_5_tun,model_6_ad_1,model_7_ad_tun_1,
     model_8_ad_2,model_9_ad_tun_2, model_10_ad_1, model_11_ad_tun_1, model_12_ad_2,model_13_ad_tun_1,
     model_14_rf_1,model_15_RF_tun_1, model_16_rf_2,model_17_rf_tun_2,model_performance_metric,
     model_performance_metric_201706,model_performance_metric_201707, model_performance_metric_201708, model_performance_metric_201709, file = paste0(ruta_modelos,"/Modelo_cobranza_v2_parte1.rda"))



performance_total <- cbind(model_performance_metric, model_performance_metric_201706, model_performance_metric_201707, model_performance_metric_201708, model_performance_metric_201709)

write.csv(performance_total, paste0(ruta_modelos,"/Challenge_Cobranza_parte1_v2.csv"), row.names = F)









##########################################################################################################
#------------------------------------ INICIO PRIMER INTENTO (PASADO) ------------------------------------#
##########################################################################################################
#

##############################################
### CATEGORIZACION
##############################################

#1) var numerica

cat_var_num

colSums(is.na(df_train)==T)


df_train$Gtia_Soles_cat_f <- factor(ifelse(df_train$Gtia_Soles_cat == 0,1,2),levels = c(1,2),labels =  c("No","Si"))



df_train$Patrimonio_cat <- factor(ifelse(df_train$Patrimonio < 417639.00,1,2),levels = c(1,2),labels =  c("<417639.00",">= 417639.00"))


df_train$ddatrMSF_ponderada_cat <- factor(ifelse(is.na(df_train$ddatrMSF_ponderada)==T,1,
                                                 ifelse(df_train$ddatrMSF_ponderada <= 1,2,
                                                        ifelse(df_train$ddatrMSF_ponderada <= 4,3,
                                                               ifelse(df_train$ddatrMSF_ponderada <= 7,4,5)))),levels = c(1,2,3,4,5),labels =  c("NA","[0-1]","<1-4]","<4-7]",">7"))



df_train$ddatrMSF_ponderada_cat <- factor(ifelse(is.na(df_train$ddatrMSF_ponderada)==T,1,
                                                 ifelse(df_train$ddatrMSF_ponderada <= 0.71,2,
                                                        ifelse(df_train$ddatrMSF_ponderada <= 3.58,3,
                                                               ifelse(df_train$ddatrMSF_ponderada <= 6.62,4,5)))),levels = c(1,2,3,4,5),labels =  c("NA","[0-0.71]","[0.72-3.58]","[3.59-6.62]",">6.63"))

df_train$ddvnrMSF_ponderada_cat <- factor(ifelse(is.na(df_train$ddvnrMSF_ponderada)==T,1,
                                                 ifelse(df_train$ddvnrMSF_ponderada <= 276,2, 3)),levels = c(1,2,3),labels =  c("NA" ,"<=276",">276"))


df_train$PNor1MSF_mean_cat <- factor(ifelse(df_train$PNor1MSF_mean <= 78,1,
                                            ifelse(df_train$PNor1MSF_mean <= 97,2,3)),levels = c(1,2,3),labels =  c("[0-78]","<78-97]",">97"))

df_train$MaxDmB_15_cat <- factor(ifelse(df_train$MaxDmB_15 <= 0,1,2),levels = c(1,2),labels = c("No","Si"))



#2) categoricas



#RANG_INGRESO 
#9417 
#FLAG_LIMA_PROVINCIA 
#3386 
#EDAD 
#5326 
#ANTIGUEDAD 
#1762 


levels(df_train$DesUbicacionLocal)


df_train$DesUbicacionLocal_cat <- factor(ifelse(is.na(df_train$DesUbicacionLocal) == T, 1, 
                                                ifelse(df_train$DesUbicacionLocal == "Vivienda" & df_train$DesUbicacionLocal == "Campo Ferial", 2, 3)),levels = c(1,2,3),labels =  c("NA","Vivienda - Campo Ferial","Otros"))



#quitamos ubicaion del local
df_train <- df_train[,c(1:3,11:16)]

###################################################
#  DATA OOT
###################################################



data_oot <- data_oot[,c(13:20,2:8,10,9,21:55)]

data_oot <- data_oot[,c(1:17,18,20,22,19,21,23,24,26,28,30,32,34,36,38,40,42,44,46,25,27,29,31,33,35,37,39,41,43,45,47,48:52)]


data_oot <- filter(data_oot, is.na(CntE1MSF)==F)


NA_data <- as.data.frame(colSums(is.na(data_oot)))
colnames(NA_data) <- "Cantidad_NA"


data_oot$flag_pago <- factor(data_oot$flag_pago, levels= c(0,1))



#Observaremos la cantidad de % que hay de pago o no pago
round(prop.table(table(data_oot$flag_pago))*100,2)
#0-> 41.23% y  1 -> 58.77%

#########################################################
# 2. ANÁLISIS BIVARIADO
#########################################################

# A) CANTIDAD DE ENTIDADES REPORTADAS EN EL SF


ggplot() + geom_bar(aes(y = ..count..,x =as.factor(CntE1MSF),fill = flag_pago),data=data_oot, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


ggplot() + geom_bar(aes(y = ..count..,x =as.factor(CntE2MSF),fill = flag_pago),data=data_oot, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(CntE3MSF),fill = flag_pago),data=data_oot, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

featurePlot(x=data_oot[,18:20],y=data_oot$flag_pago,plot="box",auto.key = list(columns = 3))

#Posibles valores outliers >= 8 

#Se obtienen los mismos gráficos, así que da lo mismo
#Me quedaré con el último mes como variable 


# B) Promedio % Normal en el SF - t-3



data_oot$PNor1MSF_mean= apply(data_oot[,21:23],1,FUN=function(x) mean(x))


# C) Días de atraso en el sistema financiero (he sacado un ponderado de los últimos 12 meses)

data_oot$ddatrMSF_ponderada <-data.matrix(data_oot[,24:35])%*%matrix(1:12,12,1)*(1/78)


sum(is.na(data_oot$ddatrMSF_ponderada)==T)


# D) Deuda directa vencida en el SF (ultimos 3 meses) - ponderada

data_oot$ddvnrMSF_ponderada <-data.matrix(data_oot[,36:38])%*%matrix(1:3,3,1)*(1/6)


sum(is.na(data_oot$ddvnrMSF_ponderada)==T)

# E) Posee Garantía

#data_desarrollo$Gtia_Soles_cat <- ifelse(data_desarrollo$Gtia_Soles > 0, "Si", "No")


data_oot$Gtia_Soles_cat <- ifelse(data_oot$Gtia_Soles > 0, 1, 0)


# F) table(data_desarrollo$MaxDmB) - Máximo día de mora mayor a 15 días o no

hist(data_oot$MaxDmB)

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(MaxDmB),fill = flag_pago),data=data_oot, position = position_stack()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


#data_desarrollo$MaxDmB_15 <- ifelse(data_desarrollo$MaxDmB >= 15, "Si", "No")

data_oot$MaxDmB_15 <- ifelse(data_oot$MaxDmB >= 15, 1, 0)


# G) Ubicación del Local - por ahora no lo toco

data_oot$DesUbicacionLocal <- as.factor(data_oot$DesUbicacionLocal) 

# H) Sector Económico - tampoco lo toco

table(data_oot$Sect_Apli)

data_oot$Sect_Apli <- ifelse(data_oot$Sect_Apli == "COMERCIO", "Comercio", 
                             ifelse(data_oot$Sect_Apli == "PRODUCCION", "Produccion", 
                                    ifelse(data_oot$Sect_Apli == "SERVICIO","Servicio", data_oot$Sect_Apli)))





df_oot_f<- data_oot[,names(data_oot) %in% list(c("CIF_KEY","CODMES","flag_pago",as.vector(t(list(df_varperf$var)[[1]])[1,]) ))[[1]]]

##############################################
### CATEGORIZACION
##############################################

#1) var numerica

colSums(is.na(df_oot_f)==T)


df_oot_f$Gtia_Soles_cat_f <- factor(ifelse(df_oot_f$Gtia_Soles_cat == 0,1,2),levels = c(1,2),labels =  c("No","Si"))



df_oot_f$Patrimonio_cat <- factor(ifelse(df_oot_f$Patrimonio < 417639.00,1,2),levels = c(1,2),labels =  c("<417639.00",">= 417639.00"))


df_oot_f$ddatrMSF_ponderada_cat <- factor(ifelse(is.na(df_oot_f$ddatrMSF_ponderada)==T,1,
                                                 ifelse(df_oot_f$ddatrMSF_ponderada <= 1,2,
                                                        ifelse(df_oot_f$ddatrMSF_ponderada <= 4,3,
                                                               ifelse(df_oot_f$ddatrMSF_ponderada <= 7,4,5)))),levels = c(1,2,3,4,5),labels =  c("NA","[0-1]","<1-4]","<4-7]",">7"))



df_oot_f$ddatrMSF_ponderada_cat <- factor(ifelse(is.na(df_oot_f$ddatrMSF_ponderada)==T,1,
                                                 ifelse(df_oot_f$ddatrMSF_ponderada <= 0.71,2,
                                                        ifelse(df_oot_f$ddatrMSF_ponderada <= 3.58,3,
                                                               ifelse(df_oot_f$ddatrMSF_ponderada <= 6.62,4,5)))),levels = c(1,2,3,4,5),labels =  c("NA","[0-0.71]","[0.72-3.58]","[3.59-6.62]",">6.63"))

df_oot_f$ddvnrMSF_ponderada_cat <- factor(ifelse(is.na(df_oot_f$ddvnrMSF_ponderada)==T,1,
                                                 ifelse(df_oot_f$ddvnrMSF_ponderada <= 276,2, 3)),levels = c(1,2,3),labels =  c("NA","<=276",">276"))


df_oot_f$PNor1MSF_mean_cat <- factor(ifelse(df_oot_f$PNor1MSF_mean <= 78,1,
                                            ifelse(df_oot_f$PNor1MSF_mean <= 97,2,3)),levels = c(1,2,3),labels =  c("[0-78]","<78-97]",">97"))

df_oot_f$MaxDmB_15_cat <- factor(ifelse(df_oot_f$MaxDmB_15 <= 0,1,2),levels = c(1,2),labels = c("No","Si"))



#2) categoricas



#RANG_INGRESO 
#9417 
#FLAG_LIMA_PROVINCIA 
#3386 
#EDAD 
#5326 
#ANTIGUEDAD 
#1762 


levels(df_oot_f$DesUbicacionLocal)


df_oot_f$DesUbicacionLocal_cat <- factor(ifelse(is.na(df_oot_f$DesUbicacionLocal) == T, 1, 
                                                ifelse(df_oot_f$DesUbicacionLocal == "Vivienda" & df_oot_f$DesUbicacionLocal == "Campo Ferial", 2, 3)),levels = c(1,2,3),labels =  c("NA","Vivienda - Campo Ferial","Otros"))



df_oot_f <- df_oot_f[,c(1:3,11:16)]


######################################################################################################
#----------------------------------  FIN DE LA PRIMERA CATEGORIZACION ------------------------------#
######################################################################################################


#***************************************************************************************************






















